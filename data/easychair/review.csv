review#,submission#,review_assignment#,reviewer name,field#,review_comments,overall evaluation - score (ignore),overall evaluation score,subreviewer info (ignore),subreviewer info (ignore),subreviewer info (ignore),subreviewer info (ignore),Date of review submission,Time of review submission,recommendation for best paper
1,2,48,Juxxxx Bruxxxx,1,(OVERALL EVALUATION) Incorrect format and unrelated content,"Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2/13/2018,3:04,no
2,2,63,Vitxxxx Casxxxx,2,"(OVERALL EVALUATION) The topic is interesting, and undoubtedly touches upon very important economic aspects of the world-wide coffee trade. However, I believe that it would be of interest to a (very) small group of JCDL attendees. I would suggest to present this topic to a more appropriate event.","Overall evaluation: -1
Reviewer's confidence: 2
Recommend for best paper: no",-1,,,,,2/15/2018,12:05,no
3,3,48,Juxxxx Bruxxxx,1,"(OVERALL EVALUATION) This poster analyzes responses from town hall-style meetings regarding smart communities and the implications for digital libraries. I found the methods to be confusing; the author appears to have listened to meetings and derived the opinions of the participants in the meetings and the associated implications on digital libraries in smart communities. Coupled with the double blind submission, the poorly defined methods and results leave enough doubt to prevent this from being accepted.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/13/2018,3:13,no
4,3,63,Vitxxxx Casxxxx,2,"(OVERALL EVALUATION) Omni-comprehensive and multi-disciplinary infrastructures and their relationship with digital libraries are indeed actual topics, but in this brief abstract the considerations provided are at such a high level that they are not able to provide further insight or actual suggestions useful for those topics. Also ?€?human centered?€? seem to be more buzz-words rather than actual requirements or features. 
If this contribution will be accepted, it might be worth to better present and discuss the concept of the library as a ?€?digital octopus?€?, as this is one aspect of the actual (hot) discussions whether research data should be part of the library or rather part of the ?€?digital laboratory?€? (i.e. the infrastructure) supporting the research activities.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/15/2018,12:06,no
6,4,326,Jacxxxx Saxx,1,"(OVERALL EVALUATION) The topics of the paper is clearly appropriate for the JCDL conference.  The authors presents a study of the effectiveness of four learning schemes used to predict the quality of a citation (binary:  important vs. marginal) in an corps of scientific papers (ACL corpus on computational linguistics). The experiment is based on a set of 450 citations (not so big).  

In a second part, the authors propose two new learning strategies (one based on SVM and random forest, the second on a deep learning architecture).  The experiment shows that a better performance (precision, Recall, F1) can be achieved by the two new proposed schemes.  A statistical test is missing to confirm this findings.    

minor points
Who are the experts (Page 3, Section 3).
Is the citation collection available? (Page 3, Section 3)
Section 4.1. not fully clear:  ""Each feature is divided into four categories""  Each feature or the whole feature set?
Section ""4 Results ..."" must be ""5. Results ..""
In Fig 2 and 3 :  add a space before ""(area =""
Page 7:  ""by using the-fold"" -> ""by using the three-fold""
""6 Conclusion""  -> ""7. Conclusion""
In the conclusion, the support for your learning scheme is a single collection... maybe another scientific domain can have a different citation pattern..

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This is an interesting paper.  A few problems, but nothing that cannot be fixed in a final version.  
I don't fully understand why you reject it... but that's life...","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/23/2018,16:06,no
7,4,90,Yixx Dxx,2,"(OVERALL EVALUATION) The current paper explores the rhetorical context of citations in scholarly big data. I provide several suggestions and hope the authors can consider: More sentences should be provided for better elaborating the motivation of this paper. This paper needs a thorough round of English proof reading. It also needs more methodological implications in the Conclusion section. Moreover, potential pros and cons of the algorithms in the 4.1-4.4 sections should be detailed more.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,Yi,Bu,buyi@iu.edu,353,2/8/2018,16:14,no
8,4,421,Bxx Y,3,"(OVERALL EVALUATION) This paper proposed a new classifier for citation purpose classification by consolidating features from four prior models, and also a LSTM model. Overall the paper does not demonstrate significant intellectual contribution for the following reasons:

First, consolidating prior models is incremental work, unless some insights were provided regarding the strength and weakness of each model, and how the new model addresses these issues. Unfortunately no such analysis was provided. 

Second, the use of LSTM model cannot be justified because LSTM takes sequence input, but the 64 features do not form sequence. The parameters like 52 input units do not match with the 64 features. More details are needed to explain the implementation of the LSTM. Because the data set is rather small with only hundreds of examples, a 5-layer LSTM model is likely overfitting.  

Other issues:

The authors did extensive literature review on related work. It would be more helpful if the classification tasks can be specifically described along with performance comparison in that different studies used different numbers of categories and their definitions vary as well.

The research method description could also include more rationale for critical choices. For example, in 3.1. why used three different tools (OpenNLP, StanfordParser and Factorie) for pre-processing? How are features like ""Author uses tools/algorithm of cited article"" extracted?","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/17/2018,5:36,no
10,4,8,Ghxxxx Abxxxx,4,"(OVERALL EVALUATION) the authors apply the Extra-Trees classifier to extract 29 best new features and create a new model that they use to apply Random Forest and Support Vector Machine to classify reference as important or not important. The new model improves on the state of the art by 11.25 according to the experimental results.
The paper is technically sound, the approach is described in details and the results are reasonable. My understanding is that the paper contributed two things:
1- identified new features that will help with enhancing model accuracy
2- compared different models using two different supervised learning approaches (SVM's and RF)

The related work section is a little long for a technical paper, however, it provided a nice survey of the previous work. I wish it was shorter and the extra space was used to better explain the paper contribution and add more tables such as the confusion matrix which I like to look at since I can get more information about the model performance. 

Using the LSTM was not justified especially since LSTM is not suitable for this kind of data. The only justification given is that it is a new popular approach which is not a good justification.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/24/2018,7:36,no
12,5,326,Jacxxxx Saxx,1,"(OVERALL EVALUATION) Interesting paper.  The authors have analyzed 75 papers on experiments / presentation / descriptions of deploying cloud services for libraries. The main results focus on data, patrons, library staff, IT infrastructure, cloud services, costs, and policies & contracts. Besides these main aspects, the authors underline that the librarians must understand the IT processes.  In addition, the authors mention clearly the problems related to the legal aspects, the risks, and the importance for librarians to support the new web services. 

In a second part, the paper presents seven main recommendations to help librarians in selecting / developing a set of cloud services (the service scope, managing the time, the costs, the quality, the human resource, the communication and the risks).   

The style and writing of the paper is clear and easy to follow.  The organisation is good and examples are used to help the reader to understand the underlying problems / issues / possible solutions. 

This paper might generate questions and discussions during the conference.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: yes",3,,,,,3/8/2018,21:18,no
14,5,411,Zhxxx Xx,2,"(OVERALL EVALUATION) The paper summarizes challenges reported in papers related to library cloud adaptions, then offers some general guidelines to address these challenges. While the topic is relevant and of interest to JCDL audiences, the paper does not go deep enough to reveal much new information not already well known. The methods section lacks details, making it difficult to evaluate the validity of the results. The recommendations section makes all identified challenges a project management issue, which lacks support from the literature and experience. A few other issues:

1. Figure 1 is too small. The clustering and the linking could be a major contribution of the paper but are not sufficiently addressed in the paper.
2. It is not clear how many of the identfied challenges are library specific. It feels like that they are in general limitations of any SaaS. These issues are being addressed by the industry, e.g., via govcloud, better tooled management interfaces, and better cost estimation, etc. It?€?s not clear if the challenges remain unresolved or have been alleviated by these new developments.
3. Are library journals the best place to mine these challenges? It may be the case that libraries lag behind the IT industry in cloud adoptions such that library SaaS is implemented poorer than industry average and then library personnel is poorer prepared for the transition? Or the quality of some papers can be lacking? For example, the UWHS example cited in the paper drawing from a single failure of adopting free cloud-based videoconferencing its conclusion that ?€?UWHS is less likely to rely on any free, cloud application for any ?€?critical project again?€??€?. This clearly ignores the fact that a significant number of universities have already outsourced their email services to free google or Microsoft cloud services.
4. The focus on SaaS, in particular ILS, does not take into considerations of many other library cloud adoptions. Many institutional repository software, including Dspace and fedora, have developed cloud images and are widely used.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Could still be accepted as a poster.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2/17/2018,0:33,no
15,5,301,Bexx Plxx,3,"(OVERALL EVALUATION) The manuscript addresses the role of cloud computing in library services.  Is there a net gain to incorporating cloud services? What are they? What are the potential problems?   While cloud computing as a formalized concept has been in the lexicon of IT for over a decade, with recent growing awareness of data sensitivities and vulnerabilities, the topic remains timely.  The objective of the manuscript is strong:  illuminate the benefits and concerns to incorporating cloud computing in library services, and offer recommendations.  

The manner in which the objective is achieved is deeply weak however. A fundamental weakness is an ill defined concept of cloud computing.  For instance, the authors imply that email is a cloud service. It is not, and shows confusion over the difference between distributed local area services provided by an institution and commercial cloud providers.  The authors bring some clarity to the question by naming Software As A Service (SAAS) in the abstract, but this is only mentioned in the abstract and the SAAS concept is then not carried through the body of the work.   The manuscript could be improved by carrying the concept of SAAS through the manuscript, and within that, carefully distinguishing local area services (such as those supported by the institution), private cloud services, and public cloud services.   Use a similar conceptual frame to identify library services too - those are similarly not well defined.   

The two major topics of the manuscript: a challenges of cloud computing (Sections 3, 4) and a recommendation section (5). The two topics are disjoint, and appear to have been glued together given that they do not support one another, do not flow from one to the other, and have opposing assumptions.  The challenges (Sections 3, 4) appears to be a student literature survey; it is written in a halting, disconnected style that speaks to a master's student summarizing papers and hooking them together in a barely present narrative.  This incoherence between topics and a glaring lack of technical knowledge on the part of the authors results in rather absurd inferred conclusions (i.e., 4.1 para 3): Since anyone can set up a cloud account, it is advisable to not share patron data over the web.   (4.1 para 7): Data loss is a glaring threat because (Amazon) hosts can crash. The reviewer refrains from attempting to explain why these are absurd conclusions, and instead suggests that the authors can improve the manuscript by vetting the conclusions with someone with deeper technical knowledge than the authors appear to have. 

The authors raise good questions about the location, rights, and protections of their major data assets.   The institution as a stakeholder has data resource management policies that affect the decision to move core data assets to a commercial cloud.  The manuscript would be strengthened by a focus on data assets of the libraries. The reviewer recommends this focus.  

For this or any topic to be effective in the context of this manuscript, however, the challenges need to be crisply identified in a more coherent narrative in Sec 4, and then addressed in an integrated fashion in Sec 5.   This requires bringing together the highly disparate Sec's 4 and 5, but that is required anyway for the manuscript to approach being in a form that contributes to a reader's knowledge.  

In summary, while the manuscript is far too weak for inclusion in this venue, the authors are encouraged to continue to work on the topic especially in handling the data assets of a library. It is recommended that the authors additionally engage a technical data scientist who can help crystallize the true challenges of moving to a commercial cloud.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The paper is heavily flawed;  it's two relatively unconnected topics glued together.  The first topic shows glaring weakness of knowledge in computing, resulting in absurd conclusions.  The second topic does not reflect any conclusions from the first topic; it is completely standalone.  I will strongly argue against this manuscript being accepted in the form that it is in.  Perhaps next year we'll see a better version.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2/18/2018,16:26,no
17,6,2,Trxxx Aaxxxx,1,"(OVERALL EVALUATION) This short paper presents a mobile device application that supports the exploration of the physical collection in a library building by recommending relevant subject areas in other areas. Log file analysis is used to answer some specific research questions such as what trends can be identified for this kind of browsing.

I find this research is interesting and relevant for the conference, but although the experiment is somewhat easy to understand, the presentation is unfocused and not yet at the level that is required for a conference publication. 

In the abstract and the beginning of the paper, the paper claims to present the development and evaluation of the topic space recommendation model. I do not find that this model is presented properly anywhere in the paper, and do not see any references to other papers presenting it. It is partially described in the background section, but difficult to identify what this model actually does. In the conclusion, the name of the app that implements this module is mentioned but it is unclear if this app module is the same as the model initially indicated as the topic of the paper. 

The introduction of the paper (as well as the abstract) lacks the contextualization and motivation for this experiment. I also find it inappropriate to include authors contention and personal position, as it only weakens the contribution.  In general, the paper lacks a good description with examples on how the system works and what the actual contribution is.

A main contribution of this paper seems to be the log analysis that is used to answer 2 research questions. The first RQ is well formulated, but the second is hard to figure out and the author can improve the contribution from this research by more carefully designing a set of well-defined research questions. 

The graphs do not render well on my print, and I suspect that this also will be a problem in the final print. Even in the pdf on screen, the images do not render well because of low resolution. 

The analysis in the finding seems to be focused on what recommendations that have been made - or followed by end users. Given that the log is covering 2 years, I find the number of users and records recommended surprisingly low and indicates a mobile app that is rather infrequently used? It is somewhat unclear if they at all investigated how successful the recommendations where. Phrases like ""checkout records"" and ""strong enough for circulation"" may make sense for a librarian but is not a precise way of describing the limitations of the research. The recommendations are described as having a ""long tail power law distribution"" but the research would have been much more interesting if the authors succeeded in explaining this phenomena in terms of user experience. All in all, I find it hard to figure out what the actual findings are.

The paper has a good list of publications, although the formatting needs to be improved.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/23/2018,8:53,no
19,6,138,Danxxxx Gxx,2,"(OVERALL EVALUATION) In the digital era, the huge number of books gets harder and harder to manage over time. In order to solve this problem, the author of this paper presents a new application in order to develop and evaluate the topic space recommendation model. 
Also, this study presents some interesting results about how this application increases awareness and access to library services. 
The structure of this paper is clear and  well proportioned. Also, the method is presented in detail, giving enough information about the methods used. 
However, a few comments should be taken into account by the author:
- Figure 1 should be placed in the paper near where it is first discussed, rather than at the end of the Introduction, if possible.
- Proofreading is necessary to correct some minor errors (pg. 1 ?€?to scholarly inquiry?€?, ?€?has been renewed interest on?€? ?€? the use of the article is recommended; ?€?a collections?€?).
The paper describe how to develop and evaluate the topic space recommendation model as an alternative to the personalization algorithm. Anyway, in order to develop this model by using a mobile application, this research has a good practical implication.
With suggested minor improvements along these lines, this paper can make a good case for digital library development.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,3/8/2018,15:29,no
20,6,427,Zhxxxx Zhxx,3,"(OVERALL EVALUATION) This paper did an analysis of how library users navigate different categories of books by using a mobile app. To be more specific, the authors analyzed the log of 18 unique mobile app users over 2 years, and analyzed how's the recommendation quality after users have scanned some book barcodes. The conclusions are that: (1) some book classes have more subsequent recommendations than others. (2) the outlier analysis shows how topic expansions are related with some attributes, and resulted in long-tail phenomenon. 

My general feeling about this paper is borderline. One major issue that I'm concerned most is that there are too few users involved in this study (i.e. only 18 unique users). This might cause high variance in the subsequent analysis. Moreover, I strongly suggest the authors combine more closely Sec. 4 and Sec. 5 to show how the log analysis of the apps and Fig.2&3 lead to the conclusions in Sec.5. In its current writing, my feeling is that Sec.5 is very loosely connected with Sec.4. 

My past experience on Internet-related analysis papers is that, a study like this shall be done on at least hundreds of users, and use statistics or data analysis tools to reveal some key metrics (e.g. number of expanded topics after starting item, the plot of long-tail distribution, some more quantitative analysis) which lead to the claimed conclusions. But I do acknowledge this may not apply to library science, and it's possibly wrong in this paper's case, so please use your discretion to ignore this paragraph.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/16/2018,19:35,no
22,7,49,Juxxxx Fx,1,"(OVERALL EVALUATION) This paper presents a method of generating a representative document with a canonical timeline of events that are cataloged on Twitter by users sharing URIs of articles. In other words, the authors are using sharing activity on social media to generate a timeline for events being discussed at high volumes.

The authors use hashtags, n-grams, and their associated shared URIs to generate topical indexes, and use a retweet and sharing count to identify popular topics under the assumption that popular means the event should be cataloged. 

The approaches to identifying topical events is very intriguing. However, the fine-grained intent of the authors is unclear. one sentence in particular in the introduction (""While one could argue that editorial content and wikipedia pages contain the best information, many other perspectives are left behind due to attention, bias, and human scalability."") suggests that this approach is designed to incorporate multiple biases or viewpoints. However, the approach used by the authors has the potential to introduce bias. One can imagine that tweets using hashtags regarding #benghazi or #clintonemails would center around content vastly different than #trumptaxreturns or #mexicowall. Rather, timelines regarding #rogueone would be much more comprehensive. In other words, I didn't see any evaluations regarding the completeness of the timelines using this selection method to discredit any bias introduced into the resulting documents. I would have preferred to see examples of bias being removed or countered to ease my discomfort with the potential for this phenomenon to occur.

My concerns about bias regardless of popularity of content is amplified in the evaluation section. The authors aim to generate a timeline of events, but only use one wikipedia article (out of 9!) that has a comparable timeline. When comparing against the wikipedia articles, the inferred timelines have a very broad set of precision and recall measures (e.g., 0.07 vs 0.80) based on the topics. It seems difficult to draw conclusions about algorithmic effectiveness given this breadth.

Further, the authors use humans to identify defects in the constructed timelines. This seems to excuse low recall of events in the timeline. In other words, their evaluation identifies the number of irrelevant events in a timeline; if the recall of the timeline is low, the algorithm may perform better. However, f-measure is more indicative of the algorithm's success in this case. I would have preferred to see a canonical reference timeline (potentially generated by human experts) and the precision *and* recall and associated defects measured. 

Figures 6 and 7 were not fully understandable; what is ""T"" and ""W"" on the X-axis?

In summary, this paper is extremely interesting, but the evaluation and test dataset evaluation falls short.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I hate to be the ""non-committal reviewer"", but I truly am on the proverbial ""fence"" about this paper. I see a high value in the work (it's really neat that they are working on automatic story generation given the shutdown of storify), but think there are moderately concerning shortcomings with the evaluation. I look forward to my peers' input and will revise my review if others' have compelling arguments that I have not considered for either acceptance or rejection.

UPDATE -- given my peers' comfort with this paper and my reverence for both of their high levels of expertise in the area, I have changed my review to ""Accept"";","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/19/2018,17:54,no
23,7,18,Haxxx Alxxxx,2,"(OVERALL EVALUATION) This paper proposes an interesting idea for automatically generating a story in the form of a wiki using tweets and news articles. It introduces the concept of social pseudo relevance feedback and social query expansion. 

The paper offers a clear explanation of the methods used. 

Here are some questions and suggestions: 

- Figure 1 is mentioned in the text on page 1. However, the figure does not appear until page 3. The figure should be moved to page 2 where it would be most helpful to readers.
- Page 2: Change ?€?Sharif et al.?€? to ?€?Sharifi et al.?€?
- Page 2: ?€?Integer LP approach.?€? Please state what LP stands for.
- Page 2: A shortcoming of the paper is that it does not offer an in-depth discussion of related work. For example, ?€?Other work includes linking online news and social media [28]?€? and ?€?generating event storylines from microblogs [15]?€? are just the titles of the cited articles. This is not sufficient to explain anything about these studies, the literature in general, or how the current paper extends, differs from, or contributes to the research landscape. 
-Moreover, how does the current paper differ from the following related work that was not cited: 
- Hua, T., Zhang, X., Wang, W., Lu, C. T., & Ramakrishnan, N. (2016, October). Automatical Storyline Generation with Help from Twitter. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management (pp. 2383-2388). ACM. 
- Zhou, D., Xu, H., & He, Y. (2015). An Unsupervised Bayesian Modelling Approach for Storyline Detection on News Articles. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (pp. 1943-1948).
- Wang, Z., Shou, L., Chen, K., Chen, G., & Mehrotra, S. (2015). On summarization and timeline generation for evolutionary tweet streams. IEEE Transactions on Knowledge and Data Engineering, 27(5), 1301-1315.
- Page 8: Section 6.2. How many articles were collected?
- Pages 8-9: In Figures 6, 7, and 8. What do the x-axis and y-axis represent?
- Page 8: Section 6.2. The authors compare their timeline with references from Wikipedia given that many articles on Wikipedia don?€?t have a timeline. It would be interesting to compare the proposed timeline with a Wikipedia timeline. Several events on Wikipedia do have a timeline, however: https://en.wikipedia.org/w/index.php?search=Timeline+&title=Special:Search&fulltext=1&searchToken=3cc4yhepmcrukrd4z9jne76m1.  
- It is not clear what percentage of links are from spammers or advertisers.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/16/2018,0:55,no
25,7,25,Yaxxxx Alnxxxxx,3,"(OVERALL EVALUATION) The paper presents a framework that automatically generates a timeline for an event or an evolution of a story from the online stream of social media. The product of this research is similar to a wiki page in a couple of aspects: table of content, story evolution or timeline, references, related work. As the authors mentioned, a Wikipedia page usually is more than this; it usually has a history of the events, causes, and consequences.  The authors performed three evaluation methods on the page they generate an offline evaluation, a Wikipedia evaluation, and a diversity evaluation.   


The paper is well written and the methodology was clear. However, some parts of the evaluations and the results need more clarification. For example, section 6.1 that explains the offline evaluation doesn?€?t show enough details about table 3, especially the length. I couldn?€?t get exactly how the evaluation was done, who are the workers who evaluated D2 and what are their background, etc.?","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/16/2018,19:56,no
27,8,327,Jacxxxx Saxx,1,"(OVERALL EVALUATION) The objective of this paper is to present a multi-disciplinary methodological framework.
The current version of the paper is a description of a experimental design of a digital libraries (cultural heritage) for a Inuit community (North of Canada).  The author must clearly justify the choice and explain some of the used term (e.g., multi method) and how this can be integrated into a framework.  I'm expecting reading the advantages and limitations of the proposed new methodical considerations.  But many interesting questions can be found from this experiment (what are the specific cultural heritage objects that must be preserved? in which form and why?, etc.).

The writing is clear and organisation of the paper is good.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Very limited interest, and the title does not correspond to the content","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/19/2018,15:27,no
29,8,400,Mixx Wrxxx,2,"(OVERALL EVALUATION) The paper outlines the methodological framework used to develop a community digital library (Digital Library North - DLN)  for an indigenous community in the north of Canada. The introduction, context and related work sections provide decent background. The core is section 4 which outlines the framework key elements; environmental scan, formative usability study, surveys , community leader engagement, information audit, community workshops and photography, What has already been reported in other references are the development of the model for the environmental scan and results of a community survey.

In the introduction and in the beginning of the conclusion, the author (project team) asserts that to develop a community DL for indigenous communities, you need a diverse methodological framework, and that is what the paper lays as the framework for the DLN.

The conclusions, however,  jump the reader from a review of the framework (with, what I'd expect as limited evaluation of its parts), to asserting three key lessons for developing a cultural digital heritage library without presenting or referring to specific results analysis - e.g. the final conclusion is the ?€?DLN framework allows for a deeper and a more accurate perspective of how to develop community DLs ?€? for indigenous and aboriginal communities?€?. It seems a leap from the framework discussion and note of some initial results to these assertions for what is a work in progress - I can understand these key issues, but it seems to get to assertion based on evaluation would be future work (or work that?€?s been done, but not reported yet).

That said, I?€?m intrigued to hear more of the project, and I think this project is definitely within the audience interest of JCDL. As this short paper only outlines the framework of community engagement, I hope we could look forward to reports on the environmental scan and how well it functions for a DL, more analysis on the usability data (and the comment of ?€?less formal ways to collect usability data?€? would be interesting to develop), and what was learned from the leader engagement as a long paper. The information audit appears to have identified quite a large corpus of material, and it would seem an archival digitization task is in the offing.

Section 4.3 - end of section states survey data were analyzed and presented in another paper, but that paper is not in the references.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/19/2018,14:55,no
30,8,287,Daxxx Nixxxx,3,"(OVERALL EVALUATION) This paper describes methods used to engage with a community for the construction of heritage collections.

The paper is well-written and it clearly describes the methods chosen for the project. The review of related work is an appropriate size for a short paper but there is no clear linkage between this work and the framework in Section 4. Figure 2 just seems to be the union of all methods previously mentioned. The methods are described well but in a more narrative format than in a critical reflection - which is what I would hope for.

At the end of Section 3 there is mention of another paper but no citation. This is odd unless it is also submitted to the conference.

The ?€?key lessons?€? listed in the Conclusion don?€?t appear to be be specifically based on the experiences of this project: can they just be restated as ?€?we used the methods we had selected earlier?€?? There doesn?€?t appear to anything specific that links the methods chosen and the community-based collection development: I don?€?t see a clear contribution beyond reporting project activity.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/17/2018,3:22,no
31,9,370,Praxxxx Terxxxxx,1,"(OVERALL EVALUATION) The paper provides a method for combining knowledge captured from practitioners and from documents into semantically meaningful concepts. 
  The paper utilizes the lettrines labelled by historians utilizing relevance to propogate labels across the database. The motivation behind the paper and a brief
  background into challenges and related work is provided. The relationship between keyword visual representation, concepts in the context of system design, the learning process, 
  the algorithm are discussed. Experimental results for a set of 910 lettrines across different letters, patterns, background and sizes from Virtual Humanistic Libraries
  was used to study propogation. Results are discussed and future work is identified.

  The paper is innovative and applicable to digital libraries and image analysis. There are minor issues with clarity which should be addressed. Particularly
  the last line of the first paragraph of the Introduction, spacing through out the introduction (manually_done, a a_conclusion). Acronyms (CIBR - Content based IR ?) need
  expansion, spelling for words such as precized (3.2) needs correction.","Overall evaluation: 1
Reviewer's confidence: 2
Recommend for best paper: no",1,,,,,2/14/2018,3:22,no
32,9,62,Vitxxxx Casxxxx,2,"(OVERALL EVALUATION) Lettrines are the big decorated letters that often appears as the first letter of a word in ancient books. They are very useful for historians to distinguish the works, the printers and the date of printing. The paper presents a system for the interactive propagation of annotations made by a historian to other lettrines in a data base of lettrines?€? images. 
The methodology is well described and the results seem quite promising. However, the topic is very specialized and would be of interest to a small minority of attendees. Also from the description it does not appear that the described methodology could be used in other applications. The contribution might be more appropriate if presented as a poster.","Overall evaluation: 0
Reviewer's confidence: 2
Recommend for best paper: no",0,,,,,2/16/2018,23:53,no
33,9,13,Marxxxxxxx Agxxx,3,"(OVERALL EVALUATION) The authors in the abstract specify that the goal of the paper is: ""... we propose an approach to interactively propagate annotations representing the historians?€? knowledge on a database of lettrines images manually populated by historians (with annotations). ""
The concept of annotation is then central to the work, but in the related works section they do not refer to seminal works of the extensive bibliography on annotations, digital annotations and systems to support the creation and management of annotations. Why? Are the authors aware of all the work done in the sector?
Much important activity has been carried out within the DELOS network of excellence and the Open Annotation Collaboration.
Subsequently, many results of interest were published in the proceedings of the JCDL, ECDL, TPDL, and ACM DogEng conferences, and in the relevant scientific journals such as, for example, ACM TOIS and IJDL.

The presentation of what has been done by the authors is presented at very different levels of study:
- The authors use keywords that are typical of the field of information retrieval and that refer to general concepts, but the authors never contextualise in depth each topic with respect to what actually done. 
- Instead, section 3.3 shows a ""learning algorithm"" for a model. This algorithm refers to a specific implementation, so is more low level. It would be useful to the reader to understand what the authors propose, if the model the algorithm refers to was presented. But the presentation of the model is not introduced and explained in the paper.

In section 4.1 the process of indexing of the documents is illustrated in a simplistic way, as if the authors did not really know the methods of indexing the text that have always been based on the knowledge of the research results of George Kingsley Zipf.

Probably the authors were influenced by the choice to present their results through a short paper, so they oscillate between a presentation of general concepts in a generic way, and then with some attempt to provide details of some aspects that would be of interest. It also seems that the paper was written hastily, because there are many errors in writing: repetitive words, words that lack a letter, unnecessary white space, etc.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Unfortunately in the current presentation the work can not be accepted.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2/18/2018,16:23,no
34,9,30,Daxxx Baixxxxxx,4,"(OVERALL EVALUATION) This submission details an adaptive pattern recognition technique the authors have devised to support scholars without a programming background to study visual artifacts within documents, such as lettrines.  Indeed, lettrines is the focus of this article, although the authors point out that -- given the adaptive nature of the approach which starts with no a priori knowledge -- the approach is also applicable to other visual artifacts.

The paper is reasonably structured, however the number of grammatical errors and spelling mistakes makes some parts hard to follow.  Any spell checker would flag ?€?achine?€? ?€?manuallydone?€? and ?€?precized?€? as not recognized words: the first looks like it was meant to be ?€?machine?€?, the second ?€?manually done?€?, but I could not work out what the latter was meant to be.  

The paper uses the term ?€?semantic concepts?€? numerous times without defining (or giving a reference to) what is meant by this.  This is more problematic than the grammatical and typing errors in being able to follow the work that is being described.  I wonder if all that is being described in this case are manually assigned text labels? It is another detail that could be addressed with some editing work, but continues the trend of a lack of care and attention to detail.  

The description at the very end of Section 2 is rather nebulous as to what it means.  I wonder, for example, if the fundamentals of the Gamera software suite by the DDMAL group at the McGill University (Canada) (which incidentally would count as prior work) doesn?€?t meet the requirements set out by the authors.

In terms of the technical work, it was troubling to read that their processing of the image maps it to be a gray-scale image when color is given as one of the four principal elements to lettrines that are studied (?€?the letter, the color of the letter, the pattern and the background?€?). The use of 3x3 cells also seems somewhat arbitrary, and not justified.  It begs the question what would happen to the accuracy of the technique reported if lettrines varied considerably in size, or if the digitized images the scholar is interested in comes from disparate sources where scan resolution is not uniformly controlled.  This then links to the wider context of just how exactly the document recognition system being described -- which requires it to be under the control of the scholar -- would operate in practice in a digital library -- this is not addressed in the paper, but is clearly an important issue given the conference topic.

The reported test set size is not very large (910 lettrine images covering the 26 letters of the alphabet), which was then further split in two to produce training and testing data.  I found it unusual that precision results were given without being accompanied with recall rates.

These issues make the paper too problematic in its current form for me to recommend it be accepted as a short paper to JCDL.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/20/2018,9:39,no
36,10,326,Jacxxxx Saxx,1,"(OVERALL EVALUATION) This paper focuses on the problem of identifying temporal street names (a street name with a date reference) in various languages: to automatically determine whether a street name is a temporal street name.  The paper is well structured and clearly written.   The description of the proposed system is well presented.  Various analyses are provided: Are some dates more frequent than others? Are some months more frequently used than others? ...  The precision achieved by the system is rather high (97%), and 62% when providing an explanation. 

The main drawback of this paper is the topic.  It is very specific and not directly connected with DL.   

Minor comments.
Beginning of Section 2.1:  The duration and set type are not explained.
Last paragrph in Section 2.5.  We could have a street and avenue with the same date, but both are distinct streets (the same in German with Strasse, Weg, Gasse).
Beginning of Section 5.1.  It seems that the street names appearing in the test set were already used when generating the system.  Need to confirm (or not) this point.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Very specific topic, and if you have room, why not.  The paper is clearly written with an evaluation","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,3/8/2018,23:31,no
37,10,163,Fexxx Haxxxx,2,"(OVERALL EVALUATION) The paper proposes an analysis process that automatically extracts ""temporal"" street names from OSM. Afterward, the process seeks for each extracted street name Wikipedia pages that explain the cause or origin of the respective street name. Afterward, the authors conduct a manual analysis of the street names and explanation.

While the paper presents an interesting idea, it is unclear to me how this would be of interest to the JCDL community, mainly for two reasons First, the computer science contribution is rather low, since the described process simply combined well-established NLP methods to extract street name and find relevant Wikipedia pages. Second, the analysis of temporal streets (Sec 4) is likely more of interest to the social sciences or geography that to a computer science audience. Moreover, the usecase of the research project is not sufficiently described by the authors: whom would the proposed project and results be beneficial to?

In Section 5, the authors evaluate the two processing steps of their automated analysis: extraction and seeking of Wikipedia pages to explain temporal street names. As the authors note in Sec 1 and Sec 7, comparing the workflow with other methods is difficult or even not possible, since no approach so far has aimed to address this issue. While the evaluation of both processing steps is technically sound, a comparison with a baseline method would be great, specifically for the second step, which links Wikipedia pages to extracted street names. Also, it is unclear how many annotators participated in the study (Sec 5.2), only one or were those the same as mentioned in Sec 5.1. In both Sections, the authors should give information on the background of the participants. Even more important, they should caluclate the ICR and only accept the results if the ICR is sufficiently high between all involved participants.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,1/31/2018,14:02,no
38,10,57,P??xxxx Caxxx,3,"(OVERALL EVALUATION) This paper presents a study on street names mentioning dates. The authors show how the data is obtained and perform a study on the distribution of dates in several countries.

The paper is very appropriate for this venue. Although, technically it is not very original, it does show novel exploratory results. The method is well described and is potentially useful to many different areas.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/16/2018,15:05,no
39,12,341,Giaxxxxxx Silxxxx,1,"(OVERALL EVALUATION) The paper presents a quite interesting insight into the training text used to determine word embeddings. On the other hand, the paper could present better motivations for the work and clearer take-home messages. 
In general, the paper does not read very well and it is not always easy to follow. The presentation of the work could be largely improved in clarity and organization in order to make it more readable and comprehensible. Moreover, there are some problems with the evaluation which uses a ""quality"" measure not better specified and do not report any statistical study of significance which is required for such analyses. 


More in details:
The first sentence is not grounded in the literature: ""Word embedding approaches like Word2Vec [21] or Glove [25] are powerful tools facilitating be??tter search results and data analysis in digital libraries."" Word embeddings are used in many contexts especially in NLP; in IR they are used within neural networks in particular, but they are more means to represent documents in order to employ neural networks models for search rather than retrieval methods themselves. Moreover, within DLs I do not know if they have been employed to improve search results or to analyse data; it could be, but references are needed or this sentence should be revised accordingly.
As a consequence, the very motivation for this work is not well grounded in the DL area. 

The description of word embeddings is not very clear and quite cumbersome. I mean, I know how word embeddings are determined but I had some troubles understanding section 2.1.1 An example would have better served the purpose. 

Figure 1 in Section 3.2 is baffling. What is average quality? How is ""quality"" defined? Usually, the effectiveness of a model is evaluated by using proper metrics (e.g. DCG or AP or ...) and each metric has a proper meaning measuring a different angle of the model. Is quality based on how close the predicted embedding is with the ""correct"" one? 

Quality is not defined also afterward. This affects all the results. I do not know what I am looking at. 
Anyway, are the differences between the similarity measures of statistic significance? 

The comparison with the Google dataset is rather speculative and this is comprehensible since the underlying text is not available.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,1/26/2018,9:50,no
41,12,330,Chrxxxxx Sexxxx,2,"(OVERALL EVALUATION) The paper investigates the influence of corpus fragmentation on the quality of text embeddings using standard evaluation sets. 
This topic is highly interesting, and obtained results would help researchers in choosing good parameters for corpus construction and model training. Word embeddings for encoding text semantics is of interest to the community. 
The chosen methodology is generally sound, the paper nicely written and the steps are understandable. However, I do have some question about details/choices and are not convinced about some conclusions the authors made. Especially, the conclusion of ""3-grams are best"" and the deviation from the notion of window size of Mikolovs paper might cause confusion and hinder correct uptake of results from the community. 
Publishing source code and data sets is definitely a plus and increases reproducibility of the study.
 
Detailed comments can be found below.


Background
==========
- Formula (1): I think this is a simplification. I would add a parameter \theta for model-specific hyperparameters (e.g. negative sampling in word2ve). Among the hyperparameters are then d, epoch_nr, win). C and dict_size are parameters for the training data and not the model itself.
Also the fragmentation (k in k-gram) would be such a parameter.
- 2.1.2. I was wondering why the authors chose CBOW. since already in Mikolov's original paper [citation 21 in the paper] skip-gram outperforms cbow.
- Footnote 2: For me, it is not clear how you treat sentences. For sentence ""A b c d. E"" and 3-grams would you generate abc, bcd or anything else? An Example would have helped.
- Section 2.3. 
 - It seems that you use a different notion of window size and context than word2vec. A window size of 2 would generate 4 context words in word2vec/cbow. For a 5-gram a b c d e, the training example generated would be a b d e -> c (only the middle word is predicted). In you example you 1) consider a different notion of window size and 2) also create training examples a b c d -> e.
First, I would like to know the reasoning for these choices and second it has to be made clear in the paper that this differs from word2vec notion of window-size, otherwise readers might misinterpret the results. Your window size of 2 is Mikolov's window size of 1.

Experiment Setup
=================
- Figure 1 interpretation: It seems from figure 1 that - other than described in the text - the model is best at 7 epochs, and after getting worse, accuracy seems to increase again after 10 epochs. So, I am not convinced of your choice of 5 epochs.
- I wondered for the analogy tasks how you counted test cases for which a, b, and/or c were not part of the vocabulary. Did this happen?
- It would be helpful for result interpretation to report the values achieved in other studies on the different corpora. Just to see, whether your corpus+model+fragmentation-methods are way off or very close to what has been achieved elsewhere.


Experiment Questions
====================
- experiment question 1: 
 - ""..size of any n-gram corpus increases exponentially with large n"". I am not convinced of that. The number of tokens is nearly the same. Of we move a sliding window over a text of size s, we get s-n+1 tokens (if we ignore sentence boundaries). The corpus size is then the number of distinct tokens. As I agree, that the number of bigrams is larger than the number of unigrams, I am not convinced that this relation also holds for 10-grams and 11-grams, for instance. Seeing, that we stop to generate the n-gram at sentence boundaries. (**)
 - Justification of the question: I think this question needs to be limited to corpora of specific sizes. For a small corpus, but large n, we will have a lot of n-grams with a very low match count. Thus, the question (and your answers) need to be constrained to ""sufficiently large corpora"" (as you also argue in section 3.1.)
- experiment question 2:
 - the same argument for the size of the corpus as above holds here

Experiment Results
==================
- Section 5.2.1. n-gram size, referring to table 5:
 - I would disagree with the interpretation that 3-gram is the best. The total loss for 2-grams compared to 3-grams is 50% (17% to 6.7%). From 3-grams to 5-grams we again half the loss (6.7% to 2.3%). For the analogy tasks, results will even be better than full-text with 5-grams.  Only the difference from 5-grams to 8-grams is nearly neglect-able on average. Thus I would go for 5-grams. However, if a second parameter (e.g. the corpus size and thereby the training efficiency) needs to be considered, this choice might be different (see also my other comment marked with **)
  - I would use a similar argument w.r.t results in table 6.
- Section 5.3. 
  - It would be helpful to get a more detailed knowledge why min-count is so important. Which examples (or how many) could not be solved in the analogy task because a word from the test set was not part of the training set due to this threshold? Could you give examples?
  - Further, it would be interesting to see the different final corpus sizes with a specific miscount parameter (how many n-grams will be excluded?)
- Section 5.3.3.
  - what is an existing match count compared to the actual value for the match count? Do you mean theoretical value an actual value? Like x-axis are all natural numbers between 1 and max-match-count and y-axis is the observed frequency? Please clarify, to help interpret figure 2



Language/Format/Structure
=========================
- It took me some time to relate the results in Table 1-4 to the benchmark set described in section 3.3. It would be helpful to have the names you used for the columns of the table in section 3.3 (some are there but not all), and also have an additional heading in the tables saying which are similarity and which are analogy test sets. Further, it would also have been helpful to read the evaluation metric (Spearman vs. accuracy) in the table (either in the caption or in the column heading. 
- The same (heading, aggregation) applies for tables 7 and 8.
- I suggest to aggregate Tables 1-4 were aggregated, having an additional column/indicator for the respective window size. This would allow to better compare the results w.r.t. window sizes.
- above table 5: ""as explained Example 3"" -> as explained in Example
- Section 5.2.: You already introduced the parameter \emp{win} as window size. Please use the same parameter in section 5.2. (instead of $j$).
- Table 5: I was confused that the bold values (largest) ones actually encode the second-to-worst values. This does not make sense to me (apart that in the interpretation that this is the method you would chose, but I would disagree). Please reconsider the coding. 
- Table 10: please include the distance measure (cosine) in the table header
- Figure 2 should be placed on the page before, at least before table 10
- Figures 1 and 2 have some artefacts in print-out. Could you include a vector graphics version instead?","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/9/2018,13:28,no
42,12,31,Daxxx Baxxx,3,"(OVERALL EVALUATION) This paper presents work analyzing the comparative accuracy of word embeddings trained on Google ngram-style corpora (i.e., as counts of ngrams rather than full, continuous text) under a several different experimental designs (varying the minimum count threshold and essentially the ngram/window size).  While most work using word embeddings tends to either work directly with embedding pre-trained on another continuous corpus (e.g., word2vec, Glove), some important work does use embeddings trained on Google ngrams (e.g., Hamilton et al. 2014), so it's a useful exercise to examine how the embedding quality might degrade as a function of reducing the amount of information in aggregating it.

Strengths:

-- Overall this is a nice experimental design, and I appreciate the in-depth explanation of the causes behind the degration in quality for the different factors.

-- The use of two corpora (Wikipedia, 1B word dataset of news) is great, and it's heartening to know the results are similar between the two, which speaks to the robustness of the results

-- I appreciate the clear recommendations (e.g., setting a minimum count threshold no less than 1/1,000,000).

Weaknesses.

-- I have strong concerns that the results presented here are not due to the factors examined (window size, min count), but rather to hyperparameter choices in word2vec (or other artefacts of the learning algorithm) such as the learning rate or the order in which the data is presented.  This is most salient in the example from table 1; with a window size of 1, word2vec trained on full, continuous text sees exactly the same information as every model with a fragmentation level > 2 (i.e., wiki_3_1, wiki_5_1 , wiki_8_1) -- only (as the authors point out), half as many times as any fragmented model.  If a full model and the wiki_3_1 model were initialized at exactly the same place and the wiki_3_1 was run for half as many iterations as the full model, so that it observed exactly the same *amount* of training data, would we not expect to see exactly the same representations in both models? 

-- This may be what section 5.4 is getting at, but since word2vec is trained using SGD, the order in which it is presented information matters for the embeddings that are learned.  Is it possible that the deficient behavior is observed here with the fragmented models because it's essentially taking a steps that's twice as big for each update (compared to the full model), since it's seeing exactly the same data twice?

-- Some of the results show the fragmented models performing worse than the full model, which I suspect is a result of just statistical error.  Can you present confidence intervals for the results (using the bootstrap, for example)?

-- I think some of the citation practices here could be improved; for example, instead of saying ""the famous example"" of man/women = king/queen, just cite Mikolov 2013; the citation provided for sentiment analysis on Twitter (Brody and Diakopoulos) doesn't actually use word embeddings at all.  

Minor

-- The initial discussion of fragmentation is a little confusing; why are 2grams more fragmented than 5grams?  I think this is conflating fragmentation with the minimum count parameter.

-- I disagree with the rhetoric in section 1 that one can come to ""general conclusions"" about the quality of embeddings with intrinsic evaluation.  Those metrics assess fitness for those specific tasks, but not a ""general"" fitness across a wide range of tasks.

-- in 2.1.2., yes word2vec and Glove have been shown to share general properties, but it's too strong to state ""that there are no fundamental theoretical differences"" between them.

-- I'm quite confused by what figure 2/section 5.3.3 is meant to communicate.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2/16/2018,23:57,no
43,13,369,Praxxxx Terxxxxx,1,"(OVERALL EVALUATION) This paper introduces a data extraction system for acquiring affiliation information from web pages across domains incorporation conditional token probabilities, enriched using structural features (DOM) of HTML documents. A good background for methods used in extraction of entities and challenges in extraction of entities using a single methodology across domains is provided. The idea underlying the paper is unique, and is appropriate to digital libraries.

    There are however serious concerns in the discussion of the methodology, and evaluation of the methodology. When discussing the mathematical foundations of how the conditional probabilities are calculated the paper seems to make use of the equality operator, when an approximation operator should be utilized, since the
LHS of the equation is no longer equal to the RHS when simplifications are applied to one side of the equation. The evaluation of the paper does not state if the same set was used for building the conditional token probabilities and for testing. Assuming that the same set was utilized, the paper does not discuss the fit of the model (over fitting is possible when the size of sample is small (total of 11782 entities) even with cross validation). It would be useful to have any of the methods discussed in related work be evaluated on the same set for comparison.

    There are minor spelling and grammatical errors (Introduction Line 2: Tipically, Introduction Paragraph 4: To acknowledge (estimate?)","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2/13/2018,6:36,no
46,13,149,Swxxxx Gotxxxxx,2,"(OVERALL EVALUATION) 1. An interesting task as current NERs are not such elaborative in terms of languages and html content as targeted in this paper. The paper focus on extracting names from faculty websites. The traditional NER taggers are more focused on the grammatical structure and are not suitable for faculty websites. The authors should emphasize on this. 
2. The paper uses a methodology of the textual content and structural content to achieve the precision on the task. 	
3. This is a useful problem and can be applied in many education application. The authors should also talk about how this can be useful in the education environment applications. 
4. The current models fail in such tasks. So authors should provide a comparison experiments of using standard NER techniques on this tasks. 
5. More details about labeling approach, and statistics of the languages and faculty counts will be useful. It is unclear from the paper the details of the dataset for training and testing. Also, it would be good to provide some details of how the model performed on various languages. On which languages it performed better vs which languages it failed? What is the analysis?

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I have seen that standard NERs models have many limitations and had to be tweeked for problems like this. The authors used text and structural features for better accuracy and on multiple languages. I would say that more details on the experiments should be sufficient to accept this paper.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2/24/2018,0:43,no
48,13,324,Haxx Salxxxxxxx,3,"(OVERALL EVALUATION) In this work, the authors present one of the problems of web data extraction, namely entity name extraction of authors in faculty directories with the purpose of enriching public bibliographic databases. They propose a statistical approach that combines textual and structural features of HTML web pages which produced high precision and recall upon testing it against a dataset they created of +11000 researchers.

The flow of the paper is good, with high clarity and organization. The authors covered the prior contributions extensively and with breadth. They also devote a good amount of the paper to explaining the problem, and their approach and methodology to solve it. I was most impressed with the dataset that they collected and manually labelled. I hope the authors will publish it as well as this work so that the scientific community could benefit from it. Finally the experiments were sufficient and well documented. All in all this is a good balanced paper in my opinion. My only suggestion is that it would have been much more impactful if they ran their experiments on the same testing dataset but using a couple of the other prior approaches from their related works section. Their 0.95 F-score is good but would have been more impactful if it was compared with the other methods' results.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: yes",2,,,,,3/9/2018,0:18,no
49,14,48,Juxxxx Bruxxxx,1,(OVERALL EVALUATION) No paper submitted.,"Overall evaluation: -3
Reviewer's confidence: 1
Recommend for best paper: no",-3,,,,,2/12/2018,10:34,no
50,14,63,Vitxxxx Casxxxx,2,(OVERALL EVALUATION) The few lines of abstract provided seem to suggest that the topics of the poster will be the digitization of donor files (to be used in the future for sociological research) and the use of SobekCM (an open source system for digital libraries) to create collections of historical materials. Both topics are not new and might not be of interest to the attendees of JCDL 2018.,"Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/15/2018,12:07,no
52,14,339,Sanxxxxx Sixxx,3,(OVERALL EVALUATION) No document available for review,"Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/16/2018,5:50,no
53,15,389,Kaxxx Verxxxx,1,"(OVERALL EVALUATION) This paper proposes a method for ontology fusion that is based on a set of heuristics for handling local ontological relationships that exist for a node/entity in each of two ontologies, and prioritising specific relationships during the fusion process. Overall I found the paper difficult to follow and the intuitions underlying the approach not well justified.

The authors talk about ""binary similarity"" but this entire concept seems poorly articulated to me. This is simply ""strict matching"" rather than ""similarity"", is it not? Binary: either something matches or it doesn't. There is no ""similarity"".

Methodology:
The authors present their method in a collection of semi-formal definitions and rules. However, these aren't entirely formalized and do not clearly express constraints. Consider Rule II: if there are no nodes in O_1 that match o_j, then o_j is added to O_1. In that case o_j is inserted as a child of o_i -- which o_i given that o_j matches none of the nodes? Why is it safe to assume that o_i has the same type as this node o_j that doesn't match anything in O_1? This does not appear to be well-defined.

In general several of the definitions and rules seem to be defined in terms of the example in Figure 1 but it is not entirely clear how it can be applied in general.

The algorithm pseudocode is likewise not general (rather it is defined in terms of the layers in the example; does the notion of layers generalize?). Furthermore it contains errors:, n, I, etc. are defined to be integers but the pseudocode refers to null (implying a set) and in any case n is never updated and I is incremented as a counter, not an item removed from a set.

Why are all relations other than subordination lumped together as ""Correlation""? Why is this okay to do?

3.2.2 refers to stability of results in terms of ""mapping size""/large datasets. I don't see how this follows from the results in Figure 3 -- is this inferred from contrasting Figure 3 with Figure 2? In that case, putting the results in the same graph would make it more clear. This could be tested more systematically by considering different-sized subsets of the larger ontologies. Further, it is important to explore what is the impact of having many entities that cannot be directly mapped between two ontologies, i.e. for which Sim(o_i,o_j)=0? I suppose the reason for many other algorithms allowing for (partial) similarity rather than binary matching (Rule I isn't about similarity at all -- it requires strict matching) is to allow more nodes/entities to be aligned. What is the intuition that supports hardening this requirement? This very likely explains the higher precision -- strict matching would result in many false negatives (nodes unmatched when they should be).

It is not entirely clear what the P/R/F numbers mean -- correctness of nodes, relations, both? What exactly is a TP?

How can the authors claim 100% accuracy in the Conclusion?

Quality of writing:
The language in this paper is hard to follow in places; word choice and grammar are both not fully fluent. Some of these are easy fixes (""on the base of"" -> ""on the basis of""), others are a little more complicated (""the algorithm of main traverse procedure"" -> ""an algorithm for traversal of ontologies"") and others are just unclear (""timeless efficiency""). These are all examples from the abstract, but the main text suffers from this as well (""All entities ... are operated by layered traversal"" -- how can ""entities"" be ""operated""?) What are ""deformation methods""? I'm not sure what ""the missing open access of experiments environment"" means precisely. What is a ""loop algorithm"" and why does it lead to lower recall? Not following the logic.

The authors also mention that ontology fusion is distinct from ontology mapping/alignment in 3.1.1 but they do not elaborate on what this means. In what way are these tasks different?

References:
The referencing has some gaps. First, a sentence on the use of lexical semantic similarity _in ontology fusion_ cites a paper which is only about lexical similarity, not about ontology fusion (i.e. not really supporting the point that ""most studies"" focus on this but rather using the reference to indirectly define lexical semantic similarity). In fact there are several references which only consider semantic similarity but don't seem to contribute to the issue of ontology fusion. At the same time, the notion of lexical semantic similarity is not directly defined.

The authors might be interested in:
Joslyn, Cliff, Patrick Paulson, and Karin Verspoor. ""Exploiting term relations for semantic hierarchy construction."" Semantic Computing, 2008 IEEE International Conference on. IEEE, 2008.
Gessler, Damian DG, Cliff Joslyn, and Karin Verspoor. ""A posteriori ontology engineering for data-driven science."" Data Intensive Science (2013).","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,1/29/2018,7:21,no
54,15,97,Antxxxx Doxxx,3,"(OVERALL EVALUATION) This paper introduces BS Onto, a system for ontology fusion that relies on binary similarity (that is, BS). The motivation of this contribution is to tackle the issue of the performance of ontology fusion.
  Experiments show that BSOnto can perform in line with most existing systems (that is, slightly below the state of the art), but is computed faster.
  Relevance is a potential concern. A real use case analysis is lacking, and it is neither clear in the paper why ontology fusion is an important issue, nor why efficiency of ontology fusion is especially important to the JCDL community. This seems to be an offline process, for which efficiency is not so much of a concern.
  Soundness is another concern. The paper introduces challenges in terms of space- and time-complexity, but the core complexity of the algorithms is not discussed. This is only addressed by run time observations.
  Presentation: The paper is essentially well-written although a small number of formulations could be improved thanks to careful proof-reading.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/17/2018,7:27,no
55,15,345,Daxxx Smxx,4,"(OVERALL EVALUATION) The authors describe a method for ontology fusion and evaluate it on tasks from the OAEI Instance Matching track. The proposed method (BSOnto) is shown to have higher precision, and slightly lower recall, than several baselines. This seems like a good, focused short paper contribution; however, the writing is at times very obscure. For example, the abstract refers to ""main traverse procedure"" and ""timeless efficiency"". The last paragraph of the introduction seems to be saying that it's very hard to evaluate or compare systems at all, but that doesn't seem right given the straightforward evaluation. The mapping between ontology tasks in the Evaluation section (#3) is unclear. The runtime comparisons could analyze the significance of the results, given the much lower variance of the proposed system. All in all, a little work on clarifying this paper's contributions would be useful.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/20/2018,14:03,no
56,16,247,Gaxx Marxxxxxxx,1,"(OVERALL EVALUATION) This paper presents a strategy for name disambiguation (only the homonym problem) that does not require supervised learning and thus may be less costly to apply.  Rather than presenting the method as solution and comparing it to other solutions, the novelty in the paper is its focus on the different features (sources of evidence) and how results evolve over iterations of the clustering algorithm used.  The eight features are those typically used in similar work and the one key issue is defining a measure of similarity that drives whether the algorithm merges clusters or not.  Assessing the veracity of this measure with different selected combinations of features and investigating a limiting factor for convergence are the two issues of substance in the paper.  The evaluation and results are presented in a set of figures that demand careful reading and viewing on the part of the reader but demonstrate how results vary not only depending on what features are included but also how the clustering process proceeds over time.  It would be helpful to the reader to do a bit more explanation of each figure in the text and put them closer to text in the final version.  The finding that co-author and author references are helpful under different clustering constraints (similarity, limiting factor) confirms common sense about disambiguating authors with the same tokenized name.  
Overall, this is a useful paper because it demonstrates a method that could be applied in different use cases, for example, those where precision is valued over recall, or vice versa.  In effect, digital librarians could tune the similarity and limiting parameters to the needs of their users and collection.  Additionally, the costs of implementation could be considered in implementing the unsupervised approach with tunable parameters.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/8/2018,20:06,no
57,16,65,Lilxxxx Caxxx,2,"(OVERALL EVALUATION) The paper presents an alternative approach to name disambiguation.  The theme of the proposed approach is to reduce the complexity of the process.  The approach is tested on Web of Science data in order to have good name sets for testing, using the author-id of WoS. 

The authors describe a good selection of relevant work, acknowledging that some approaches achieve good results.  Their focus is on reducing the complexity of the approach.  No direct comparison of the results of their approach to other approaches is available.  Instead, the authors rely on specific results of their approach over a variety of sizes of data.

The paper is clearly written and the results documented in a large set of graphs showing performance.  

The authors several times use the word ""intend"" instead of ""intent""
The frequent use of w.r.t is a bit distracting.  No other abbreviations are used and this comes up rather often.
In the final paragraph of Section 4.2, there is an instance of ""his"" that should be ""this.""

Figure references are not always in order of the figure numbers.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/15/2018,20:01,no
58,16,142,Bexx Gxx,3,"(OVERALL EVALUATION) The paper addresses the well-researched problem of author name homonymy. Various approaches have been proposed to address this problem such as analyzing co-author networks.

The author presents a very simple, yet novel probabilistic similarity measure that delivers state-of-the-art results although being conceptionally simple.

What I like:
- interesting idea
- well presented
- good analysis and data visualization
- good discussion of results
- figures in the appendix
- the surprisingly good results

What could be improved:
- It would be great if the code and the data would be made available to allow reproducibility. This way others can compare their own results with the results achieved. E.g. using https://dataverse.org/","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/16/2018,21:22,no
60,16,386,Anxxx Vexxxx,4,"(OVERALL EVALUATION) This paper should be of considerable interest to the JCDL community and I was compelled by the claims made for this work: a simple, easy to understand and effective author-name disambiguation.  

Although the overall method is pretty clear and offers a promising strategy to tackle this problem, its claims to greater simplicity and greater accuracy than other methods are not solidly substantiated by evidence. Other claims such as the discussion of quality limits for clustering moves in section 3.6 sounds like the author(s) are convincing themselves that their formula for ""l"" just works and that the reader should just take it on faith that, for example ""the exact values [for alpha and beta] are not particularly important - only the order of magnitude"" and the Implementation details section is sparse.  The discussion of the experiments in section 4.4 also leaves the impression that the authors have not had much time to systematically analyze the result to understand why their choices of alpha, beta and epsilon are effective. The experimental maximization of these parameters may be evident to the authors from an examination of the results graphs but it would be helpful to more clearly guide the reader through that reasoning.

In short - I think this approach to the problem is very promising and I trust that the author(s) are onto an effective strategy. For this conference, I would have preferred that this paper might summarized as a short paper (without all the graphs etc.) or else that the claims of effectiveness and simplicity be better substantiated, even with just a table of F1 scores comparing the agglomerative clustering approach with other approaches, even if they were arrived at on different corpora. 

Below are a few suggestions for improvement for the next version of this paper.

P. 1 Col 2. ""We note that when disambiguating a name {\it name}, we do not need to consider any other names {\it name'} (in) R""

I am not sure (a) I understand what that means or (b) why this is worthy of note.  Is it because other approaches (e.g. ones that use pair-wise comparisons of names) do not consider each name one at a time?

Since the authors of this paper seem to put some weight on their (simpler) approach.

P.2 Col 1. ""She shows that the extend of ambiguity has a direct influence on the scientific performance"" - typo with ""extend""

P.2 Col 1. ""They show among others that the top-ranked researchers are so high up the ranking due to a lack of author name disambiguation."" 

Suggest a rephrasing ""They show, among other things, that one reason that the top-ranked
researchers are so high up the ranking is due to the lack of author name disambiguation.""

P.2 contains four occurrences of the phrase ""very good results"" or ""very good results"" to describe other researchers work but with no quantitative measures to back it up (except for one reference to an F1 measure).

Suggest an comparison of F1 measures of various methods (if available), e.g. in the discussion section - to substantiate the ""very good"" / ""better"" claims

P.2 Col 2. ""Depending on supervision and what they call ?€?rules?€? renders their method rather complicated"".

What aspect of supervision makes their method complicated?  The fact that it *depends* on supervision or that their supervisory training method depends on 'rules'... I am willing to accept that the method is complicated but it isn't clear why.

P.2. Col 2. ""there is exactly one document d(x) that this mention appears on."" Suggest:
""there is exactly one document d(x) in which this mention appears.""

P.3. Col 1. ""All categories assigned to d(x)""... by whom? Elsevier's subject categories? It isn't clear from the description of Fcat(X) whether or not the authors of this article decided on the categories.  Same comment about keywords (although it seems likely that these are they keyword chosen by the articles' authors.

P.3. Col 1. ""All names given as authors of all documents d' referenced by d"".  Does this mean the same thing as ""All the names of the authors in the list of references in d"" or does it mean ""All the names of the authors in the list of references in d that are also authors in the collection.""?  In other words, are you considering author names for works referenced by d but which (the works) are not also in the collection you were studying?

P.3. Col 1. The sentence structure for Feature 8 is unclear.

Section 3.2 describing the Agglomerative clustering algorithm seems could be clarified to substantiate the claim in section 2 that ""blocking"" is being used. If, as the authors state in 3.2 ""the initial state where each mention x is in its own cluster C = {x}"" and ""Then, pairs (C,C') of clusters are merged"", surely this amounts to pairwise comparisons of {x} / {x'} in the limiting case where each cluster is a singleton of one.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I didn't want to say it point-blank to the author(s) but it seems to me that there is a lot of ""hand-waving"" in this paper.  It also reads like ""notes to myself about why I did what I did"" rather than a research paper.","Overall evaluation: -1
Reviewer's confidence: 2
Recommend for best paper: no",-1,,,,,2/18/2018,18:35,no
61,17,376,Ricxxxx Toxxx,1,"(OVERALL EVALUATION) This paper introduces research initiatives related to the proposal and use of a framework for the validation of conformance checkers. Three dimensions are considered in the framework: correctness, usability, and usefulness.

Motivation and objectives are started clearly. Furthermore, the evaluation framework is sound. Its phases and components are in general explained properly and their use is illustrated in such a way that may serve as a guide for others in the community interested in performing similar evaluation protocols. 
Experimental results validate the use of the proposed framework and shared learned lessons are of wide interest.

Presentation needs to be improved, although. Some issues include:

1.	It is not clear how many tools compose the conformance checker. An architectural overview of the system should be included in the paper.
2.	It is not clear the rationale for the media types considered in the study.  Part of the discussion present in reference [10] should be brought to this paper.
3.	A more detailed overview regarding the profiles of participants could be incorporated with the paper. For example, it is not clear how familiar they are with similar tools (or technologies) to those used in the usability studies.
4.	There is no clear discussion about used classifiers in the correctness evaluation phase.
5.	Tables 2 and 3 should be replaced by graphs (e.g., boxplots).

Some minor issues include:

6.	Some overview about achieved results could be included in the abstract.
7.	Provide dates for the last access to the listed links.
8.	The use of quotes in the description of classes should be revised.
9.	It is not clear what authors mean by ?€?Section 2.2 IDs?€?. Does this section refer to the PDF reference?","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/10/2018,18:49,no
62,17,356,Shxxxx Sugxxxx,2,"(OVERALL EVALUATION) This paper presents an experimental evaluation under the project named PREFORMA to evaluate conformance checkers for preservation of digital resources in three aspects - correctness, usability and usefulness. This paper describes the evaluation methodology in each of these aspects and shows the evaluation results followed by discussions. 

The project presented in this paper is interesting and practical. The methodology taken in this project seems reasonable and sound as a practical project. However, unfortunately, this paper is weak as a scholarly paper because it lacks descriptions about the innovative features of the evaluation methodology and/or results obtained from the project. 

Proof-reading by a third person is recommended for this paper because there are unclear descriptions/sentences.
- Correctness, Usability, Usefulness need definitions as criteria for evaluation
- Unclear English phrases, e.g., singular and plural forms of ""the conformance checker(s)""
- N in the sentence ""where N is the total number..."" below formula (3) is not used in none of the formulas
- It would be better to include richer related works

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper is acceptable as a poster.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/17/2018,16:00,no
63,17,202,Jaxx Kaxx,3,"(OVERALL EVALUATION) The paper investigates ""conformance checkers"" for three file formats.

There are some key strengths:

- The general topic of quality control of file ingestion at scale, is of key importance to the practical application of advanced DL solutions in heritage and memory institutions.

- The attention to the professional aspects, rather than the pure technical aspects, is refreshing and long overdue -- these tend to be the key barrier to realworld application.

- The proposal is a very simple and straightforward approach with limited novelty -- which is a key strength as this will be crucial for practical uptake.

There are some limitations:

- The results are interesting, and promising, but a more crisp cost/benefit analysis would be welcome.

- A broader discussion of the embedding in (current) curatorial practices would be welcome.

- The paper at times reads too much like an EU project report, and less as a scientific paper on the topic -- although this is perhaps a matter of style and preferences.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Interesting -- and important new aspect to discuss at JCDL -- but more a project report than having earthshattering results or insights.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2/23/2018,11:43,no
64,17,415,Seuxxxxx Yxx,4,"(OVERALL EVALUATION) * Strengths
	- Good amount of details provided for a conformance checker developed from the project PREFORMA
	- Opensource tool and training/test corpus publicly shared
	- Thorough evaluation of the prototype tool involving both users and experts

* Notes
	- Figures 3 and 4 unnecessary (taking up too much space without much information, font in these figures is too small to read)","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/24/2018,16:11,no
65,18,55,Klexxxx B??xx,1,"(OVERALL EVALUATION) IMHO, the contribution of this paper is incremental/marginal and does not warrant publication at this conference. It is too specifically tailored to one paper, which is somewhat extended and compared against.

More specifically:
- ""uncovered by the inventory"" -- What does this mean?
- ""we take another random sample ... on the set taken by the authors"" -- Why do you say this here?
- ""... in improving the precision of sense detection."" -- Upto here, everything sounds incremental, and the contribution has not become clear yet.
- ""... of novel word sense detection."" -- The text until here (the previous paragraph in particular) are too verbose.
- ""In particular, if a target word qualifies ...of the two time points."" -- I do not sufficiently understand this.
- ""Manual evaluation of the results ... by the original method."" -- Why are there no comparisons with other approaches, such as the ones from Kulkarni et al. or Hamilton et al.?
- ""The proposed method can therefore ... of novel word sense detection."" -- This is not precise enough. What exactly can be combined with what?
- As mentioned, related work is mentioned, but the relationship remains unclear, in terms of performance in particular. There need to be experimental comparisons.
- ""their bigram distribution"" -- What is it? This submitted paper should be self-contained.
- lexicographer's mutual information -- What is it? This submitted paper should be self-contained.
- ""we are concerned with only 'birth' cases for our study"" -- Why?
- Hierarchical Dirichlet Process -- What is it? This submitted paper should be self-contained. 
- ""and are expressed by the top-N words ... probability"" -- An example would be helpful (if this plays a role). All in all, this description is not understandable. The relationship to the new method proposed later is not sufficiently clear to me.
- ""The authors treated the ... in a sense repository."" -- I do not understand this.

English:
- as per the users' needs
- with 'attractive personality' related sense
- Attempt has also been made
- detect novel sense of a word","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,1/29/2018,8:24,no
66,18,350,Mxx Sxx,2,"(OVERALL EVALUATION) The present paper proposes a new technique based on network features to improve the precision of new word sense detection. The paper is particularly interested in detecting continuous changes of word meanings over time.
Although the paper provides important research problems and the interesting apporach to word sense detection, the paper suffers from the following several issues.


Major problems
The authors claim that ""In this paper, we showed how complex network theory can help improving the performance of otherwise challenging task of novel sense detection."" However, I am not convinced how the present paper address complex network theory to tackle the word sense detection problem. If the authors want to make such strong argument, they need to provide engough evidence for it.


I am confused about the following sentence. What do you refer to by ""In this work"" Mitra et al. or your work? It seems to me that you refer to your work. But it is vert unclear what you exactly refer to.
""In this work, authors consider multiple time points and not only detect new senses (i.e., ?€?birth?€?), but also identify cases where (i) two senses become indistinguishable (?€?join?€?), or (ii) one sense splits into multiple senses, or (iii) a sense falls out of the vocabulary (?€?death?€?).""

Tense of verb must be consistent. I found that there are a number of places that tense of verb is inconsistent.

The authors mentioned that ""We perform the evaluations manually and each of the candidate word is judged by 3 evaluators."" But they did not mention about the agreement rate among three evaluators. Since the author used the manual evaluation, it is critical to show the agreement of judgements among evaluators.

Minor problems
like machine translation, semantic search, disambiguation, Q&A etc. --> like machine translation, semantic search, disambiguation, Q&A, etc.

by Mitra et al. -> by Mitra et al. [27]","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/14/2018,17:14,no
67,18,176,Anxxxx Hixx,3,"(OVERALL EVALUATION) The authors introduce their NLP approach to disambiguation and language dynamics, in particular the detection of new meanings and word meaning shifts. 

The paper has a very strong focus on the baseline an evaluation, with the novelty of the approach being merely sketched. Unfortunately, significant related work on disambiguation is missing from the non-NLP field, such as ""An open-source toolkit for mining Wikipedia"", ""Improving access to large-scale Digital libraries through Semantic-enhanced Search and Disambiguation"", as well as work on Gerbil- the General Entity Annotation Benchmark Framework, to name just a few. 

The evaluation is described in detail, especially the detection of novel senses and the detection of known shifts. However, the authors describe their results in detail (for some concrete examples) but do not compare with any other approaches (beyond their two baseline algorithms), which makes evaluating the impact of their results difficult. We suggest shortening abstract and introduction, as well as Section 3, to make room for more details on comparison to other approaches in their experiments.  

The paper appears as if mere lip-service is paid to Digital Libraries by mentioning them just once in the introduction. Clearer discussion in how this research would benefit digital libraries (beyond generic 'large data') could make the paper more relevant to the JCDL audience.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/18/2018,19:00,no
68,20,74,Hunxxxxxxx Cxx,1,"(OVERALL EVALUATION) This workshop proposes to gather the studies on the knowledge discovery from digital libraries.  My main concern of this proposal is that such a scope is probably too large for a workshop and very likely to be highly overlapped with the main conference. The proposed topics of interest include very broad topics (e.g., artificial intelligence, big data analytics, information retrieval, etc.); some of which I believe even larger than the topics of interest for the main conference.  As a result, it is perhaps inappropriate to include this proposal as a workshop for JCDL.

The organizers are suggested to list some prospective program committee members to show that the organizers are well prepared.","Overall evaluation: -1
Reviewer's confidence: 4",-1,,,,,1/24/2018,2:47,no
69,20,352,Laxxx Soxxxx,2,"(OVERALL EVALUATION) The workshop aims at promoting knowledge discovery for the digital library, which might be a potential research topic. However, I think the topic of knowledge discovery is too broad. Organizers should at least mention examples of possible tasks willing to be solved by knowledge discovery technics. What are the downstream tasks and the long-term objectives in the field? I have the feeling that any work feeling with data mining/machine learning/big data/?€? might fit with the topic. In my sense, the ????topics of interest???? item might be more particularly addressed to research challenges.

Otherwise, important dates and submission details sound good. Organizers have publications in related fields but nothing is said about their experience in organizing workshops.","Overall evaluation: -1
Reviewer's confidence: 3",-1,,,,,1/27/2018,14:18,no
70,20,402,Dxx W,3,"(OVERALL EVALUATION) Knowledge discovering is related to digital libraries but not quite new. The topics listed in the proposal seem a bit decentralized and covers everything including big data analytics, artificial intelligence, information retrieval, etc. As a workshop, I think it should be focused on a specific field. In addition, the proposal is simple, not considering some detailed factors like the acceptance rate, the audience, how to call for papers, etc. It even does not tell the brief bio about organizers. I think it should be further carefully planned.","Overall evaluation: 0
Reviewer's confidence: 4",0,,,,,1/28/2018,18:44,no
72,21,20,Suxxx Alxxx,1,"(OVERALL EVALUATION) relevance to JCDL: This study explores an important topic -- what metric companies can use as a guide for strategic decisions about research investments. Linking research and outcomes is important for all sectors however a clearer linkage of the relevancy to JCDL is needed. 

 novelty/originality: The approach used by the researchers -- linking a comprehensive review of patents with revenue generation as reported in the Fortune 500 rankings is sound and extends work undertaken in the past. It does offer a novel approach adding the concept of temporal buckets.

 methodology: The methodology is well thought out and has checks in place to achieve rigor. The study works with a substantial dataset (2.6 million full txt articles and 93 million patent citations) and identifies rigorous processes including a full suite of preprocessing steps.

 style/quality of writing: This paper is very well written both in terms of its organization and style.  The argument is clearly stated following a logical progression that provides the reader with scaffolding for understanding the impetus for the work, the design of the research, the experimental setup and the results.  In addition the writing is easy to read with terms being defined and concepts being described in easy-to-understand language. 

evaluation: the study results are interesting and well presented. The possible explanations offer interesting analyses of the results and insights that are useful. The addition of case studies offers further insight and suggests the authors were working to triangulate results which adds further credibility to their work.

 replicability: The description of the study  appears to be thorough and explicit which suggests it would be replicable. For example the description of bucket construction, which is a unique piece of this study, appears to be sufficiently detailed for replication. I use the word ""appears"" since my own experience is that until replication is tried it is not possible to imagine every condition that may diminish replicability.

 adequacy of references: References are acceptable for this paper which wisely uses much of its specified space on this study after providing an adequate lit review to set context","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/24/2018,20:07,no
73,21,238,Jonxxxxx Lexxx,2,"(OVERALL EVALUATION) This paper details an interesting study into ranking trends of Fortune 500 in terms of revenue and innovation (measured via patents). The authors propose temporal buckets for aggregation and analysis. The authors present several breakdowns and analyses for sets/buckets of Fortune 500 companies. Analysis of content found within patent data also provides an interesting source of network science based insights into the interworkings of Fortune 500 companies. The concept of temporal ranked shifts itself is an interesting and relevant topic to retrieval. The paper itself is well written. 

However, this paper does not propose new insights, approaches, techniques, or theory that are revelant to the JCDL community at large. While this paper presents interesting insights on several aspects of their dataset and case studies, the proposed analysis appears more ancedotal than exhaustive. The references are suitable for the topic but indicate that an enterprise, economics, or data mining conference would be more suitable for this submission. It is not clear where this paper could possibly fit within the JCDL agenda.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2/16/2018,3:52,no
74,21,288,Jeaxxxxxx Ogxx,3,"(OVERALL EVALUATION) This paper proposes a  study concerning the relationship between innovation and revenue generation for several Fortune 500 companies over a period of time. This kind of study is quite important since it permits to have a view on the amount to be invested for future research, according to the correlation between scientific production and economic performance. This study relies on two important parameters to evaluate research outputs which are respectively the quantity and quality of scientific papers and patents. 


Relying on a robust study of the patent citation dataset available in the Reed Technology Index collected from the US Patent Office, authors highlight some relations between parameters such as number of (i) patent applications, (ii) patent grants, (iii) patent citations and Fortune 500 ranks of companies. 
This analysis is crossed with a temporal analysis showing the trends over the time, and try to highlight causal explanation concerning the influence of these parameters.. Two use cases of industry giants illustrating?€?fierce technology competition and its effect on overall ranks is also discussed through a nice graph based analysis


The paper is very well written are argued, the data are perfectly presented, and the experimental conditions are correctly discussed?€?. However, the paper  raises several questions and comments that follow ?€?

First of all, the analysis proposes a study concerning several Fortune 500 companies, but doesn?€?t really make any disctinction between the nature of their activities while these nature could strongly infuuence the results of the analysis?€? For instance, the production industry may behave differently from the service industry. 
Regarding this aspect, the classification doesn?€?t really distinguish the enterprises issuing of the digital economy, while their business model is very different from ?€?historical?€? industry, especially in terms of patent management?€? These points should be discussed in a final version of the paper, in order to objectively assess the situation..
Another point deal with the classification in 3 buckets that conditions many conclusions of the paper. Even if this classification permits to structure the reasoning process, it would be interesting to add some comments about the influence of this classification on the global conclusions of the paper?€? What would have happened if a company had been classified in another bucket ? what would have been the conclusions if 4 or 5 buckets had been used?€?In reality, what is the impact of the choice of this classification in 3 buckets
Another more theoretical point point which raises a question is the correlation tool, that is used in many places in the paper. The correlation is an excellent analysis tool for some parts of the criteria which are studied in the paper, but I think that the paper could be enhanced by using more ?€?powerful?€? mathematical tools for such studies. This point is particularily true for the temporal analysis (trends in the time) for which some inter-correlation tools (coming from signal processing) or distances between distributions could also be interesting (Kulback distance or word mover?€?s distance for instance)?€? In order to cross more parameters, some techniques such as Principal component analysis could also be very useful ?€?
In the 6th part, concerning the characteristic of temporal rank shift, the classification in 4 categories is discussable. It is discussable because it impacts the conclusion. Even if we can consider that the authors made a choice (which is ?€?their choice?€?), the way the classification is done could be improved?€? The thresholds which are retained for the classification (80%) and the distribution around these 80% (+/- 10 or +/-2 std deviation rely on theoretical assumptions which have nont been proved (normal distribution ? ) 
Concerning th e 7th part, which is very interesting, the construction of the graph should be more clearly explained ?€? The edges are explained, but nothing about the node is given?€? Are the nodes attributed ? 
Without these information it is quite difficult for the reader to extract the meaning of the graph. 
Furthermore the drawing of the figure 13 (graphs) permits to highlight some kind of ?€?patterns?€? in the graph.. It would nbe interesting to try to interpretate these patterns, as well explaining a little bit the global shape of the observed graph.

Besides these comments this paper is very interesting and shows how the crossing of different disciplines can help to analyze a societal question.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I hesitaed between weak accept and borderline. The topic is interesting. The way the problem is addressed is correct, but could be improved with more powerful theoretical tools. However, it is a nice transdisciplinary subject.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/16/2018,18:50,no
76,21,99,Jx Stexxxx,4,"(OVERALL EVALUATION) This is an interesting bibliometric analysis of the relationships among large corporations and patent production and use. As such, it definitely has many merits and has an audience that would appreciate it. However, after three readings of the paper, I cannot find myself making a case that this paper is in scope for JCDL. This paper is better suited for a informetrics conference or a business conference innovation conference.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2/18/2018,2:25,no
77,22,49,Juxxxx Fx,1,"(OVERALL EVALUATION) This paper uses health rumor data to evaluate users' trust in the material being rumored.

Several red-flags exist with this submission:
-- Wrong format (JCDL uses ACM/IEEE)
-- Submitted as double blind (JCDL uses single blind)
-- Poor use of space (3/4 of page 10 are white space)
-- No ties to JCDL topics (digital libraries, archiving, institutional knowledge)

Beyond this, the authors also utilize a flawed methodology by only using verified rumors to measure user trust rather than compare user trust in verified rumors vs non-rumors. Further, the authors rely heavily on the work by Zhang (as cited), but make no effort to compare their results or differentiate their work from the work of Zhang. It seems that explicit supporting or rejecting the proposed hypotheses would allow the authors to either support Zhang's findings (i.e., health rumors follow the same trust patterns as non-health rumors) or demonstrate that health rumors have special properties that make them different from non-health rumors.

This paper unfortunately is not suitable on any of its topic, presentation, or methods to be accepted to JCDL2018.","Overall evaluation: -3
Reviewer's confidence: 3
Recommend for best paper: no",-3,,,,,2/5/2018,19:40,no
80,22,10,Maxx Agxx,2,"(OVERALL EVALUATION) The research topic is relevant to JCDL and literature is well reviewed. 

Research hypotheses look reasonable and interesting, but the survey design doesn?€?t look fit very well for the research questions. The fact that all of the 30 participants are graduate students (22 master degree holder and 8 PhD candidate, and 14 participants major in social science) means that they are likely to have higher information literacy than the general public and to be critical to the rumors, thus they are unlikely to share general attitude toward online health rumors, as the authors briefly mentioned. Therefore, a direct comparison between precedent studies is difficult. A sugges for further research: making a similar survey with younger students with presumably lower information literacy and comparing the result with that of the present study would give insight into a role of information literacy education. 

The selection of the rumors used in the survey requires a more detailed explanation, especially because the rumors database (reference 33) is not accessible. Authors used such a broad word ?€?health?€? as a keyword and?€?randomly?€? selected rumors ?€?which include the proposed rumor presentation (picture, verification or hyperlink)?€?, but a criteria for selection, for example, the number of comments/retweets, classification of dread rumors and wish rumors (which are common classification for online health rumors), or categories of the health topic (such as dentists, beauty care, child care, diseases of aging people, etc.) could have improved the survey plan and been useful for data analysis. 

A little work for the presentation of the article would be required; tables shouldn?€?t divided into different pages/columns (tables 1 and 2), figures in table 3 should be right-aligned, the caption for the figure 3 should be beneath the figure rather than in the next column, and contents of sections 3.1 and 3.2 are partially redundant.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I changed my evaluation from ""1"" to ""-1"", though I didn't virtually change my comments (just added one phrase). Re-consideration of the problems of the paper which other reviews had pointed out, with which I agree, and I myself had wrote in my reivew made me change my evaluation.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/22/2018,0:49,no
81,22,335,Frxxx Shxxxx,3,"(OVERALL EVALUATION) The study of what components make rumors trusted is important and particularly so with respect to health information.  The authors appear to use the term rumor to be misinformation although the term rumor could also be used to refer to correct information from a non-direct source.

While the study has numerous issues (small sample size of microblog posts, inappropriate study population), the results of the paper substantiate/reinforce prior results.   The authors acknowledge the issues with the study in the limitations section -- which is refreshing, but this seems really to be a decent pilot study for a more rigorous examination of the issues using a more appropriate study population and a larger set of microblog posts and/or variations of the same microblog post with/without the features being explored.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/18/2018,19:11,no
82,22,390,Jenxxxxxx Wxx,4,"(OVERALL EVALUATION) This paper investigates the effect of rumor presentation on user trust 
in online health rumor.

In general, the papaer is clearly written.
However, in addition to the ANOVA test of the presence of pictures, verification, and hyperlinks,
there's no proposed method in distinguishing between rumors.

The contribution of the paper is not clear.

Also, the sample size in the experiment is too small with 30 students in the survey,
and 10 students in the interview.
The data size of 24 rumors is also not acceptable.
The experimental results are not convincing.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/19/2018,3:28,no
83,23,375,Domxxxxx Tkxxxx,1,"(OVERALL EVALUATION) The paper is about semantic modelling, which is an important topic. The paper, however, is very chaotic and difficult to follow. The state of the art, the novel contributions, and the boundaries between the two are not clearly stated. It is therefore difficult to judge the novelty or relevance.

The paper is very abstract and lacks examples or diagrams that would help the reader to understand it. A lot of names appear without any definition or reference.

In addition, I think the abstract does not match the paper itself. For example, the authors write in the abstract: ""We explore the similarity of our approach to object-oriented analysis and modeling."" I don't see such a comparison in the paper.

For this reasons, I believe it should be rejected.","Overall evaluation: -2
Reviewer's confidence: 2
Recommend for best paper: no",-2,,,,,2/12/2018,15:07,no
85,23,60,Joxx Hx,3,"(OVERALL EVALUATION) The paper describes a mapping between ontological concepts and the object-oriented model. The goal of the authors is to provide rich descriptions of real life objects, based on the Human Activities and Infrastructures Foundry of ontologies. There is a good intellectual work behind the paper, that follows previous works of the authors. However, the paper includes several inconsistencies from the object-oriented perspective that need to be pointed out.

First of all, the paper is confusing and merges concepts of very different levels of abstraction without providing a clear rationale. 
The goal of the work is very ambitious since the authors try to build structured models of complex historical situations. They propose to use the conceptualization made in the HAIF, which seems to be a good option. However, at the same time, they put the focus on how its concepts map to Object-oriented concepts, making assumptions that are not exact. For instance, the authors claim that the goal of encapsulation in object-oriented programming is pairing of objects with methods, which is too simplistic and out of focus. Encapsulation is a programming technique used to ensure information hiding as a way of reducing coupling between modules, and later classes, in classical programming languages. Pairing objects woth methods would be a mean to ensure encapsulation, but not a goal on itself.

Quallities are clearly properties of objects that, surprisingly, are not mapped to attributes in object-orientes languages. Relational qualities, in turn, could be seen as associations (in UML terminology). Finally, there is another mistake when the authors associate UML with BPMN. There are completely different notations. In fact, UML has a much wider scope than BPMN, to the point that the so-called Activity Diagram in UML covers the same domain as BPMN, namely process modelling.

There is extensive literature on formal and object oriented modelling languages proposed in the early 1990s that explored the view of objects as observable processes. H. D. Erich, A. Sernadas, G. Saake and others developed families of languages that covered this view). Also, Yair Wand published a famous paper where he defined a direct mappingbetween the Mario BUnge?€?s ontology and the OO model:

An ontological model of an information system
Y Wand, R Weber - IEEE transactions on software engineering, 1990

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The paper is confusing, with notorius inconsistencies regarding the Object oriented model.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/15/2018,9:42,no
86,23,141,Cx Lxx,4,"(OVERALL EVALUATION) Based on previous work that proposed a Human Activities and Infrastructures Foundry (HAIF), this paper considers issues in the implementation of that Foundry. Previously they proposed a basic formal ontology BFO to describe their foundry.  This paper explore additions to the foundry ontologies to support formal modeling on Korean ceramic water droppers.

They then investigate two museum objects to identify some of the differences between descriptions of Universals and Particulars in the BFO, and explore the possibility of using their techniques to supplement traditional metadata.

There are several issues not discussed.

There were no actual examples given of the derived ontologies nor was there any discussion of the size of the ontology and how it would scale.

There is no discussion of how such an ontology would be evaluated.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/16/2018,17:06,no
88,24,148,Sujxxxx Dxx,1,"(OVERALL EVALUATION) The proposed topic is of interest to the JCDL community and as the authors point out, the number of images being made available online and part of digital library collections is increasing.
Consequently, it is good to have a venue where people working on related topics and listed problems can gather to discuss status of research, practical aspects, and commercial solutions in these areas.

However, I am unable to judge if the listed authors were involved in similar workshop organization and have enough background and expertise on the topics mentioned in the proposal (based on their homepages) to be able to review and judge papers in the area.
The call for workshop specifically asks for the following information which is missing in the proposal.

??         identification of the expected audience and expected number of attendees
??         contact and biographical information about the organizers
??         if a workshop or closely related workshop has been held previously, information about the earlier sessions should be provided dates, locations, outcomes, attendance, etc.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The workshop sounds interesting and will be of value to the JCDL community.
However, I could not figure out the proposers' background and expertise on these specific topics and if they have organized similar workshops previously.","Overall evaluation: 0
Reviewer's confidence: 4",0,,,,,1/25/2018,5:34,no
89,24,381,Supxxxxxx Tuxxx,2,"(OVERALL EVALUATION) This workshop aims to bring together researchers and practitioners who deal with digital libraries of images and image retrieval. Image fields have played a big role in digital libraries recently, it would be nice to have a workshop that keep participants abreast of challenges and solutions in handling image collections.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,1/26/2018,13:36,no
90,24,352,Laxxx Soxxxx,3,"(OVERALL EVALUATION) The workshop addresses a very interesting and relevant topic dealing with image collection. Recent advances in deep learning have increased the need for large-scale image collections, opening several challenges in terms of creation, organization, access, and use. 
The workshop format (one day) is accurate and allows enough time for discussion. The possible topics are interesting. 

Suggestion: I would add another one related to the creation/use of multi-modal collections (e.g., text/image) since both evidence sources are complementary and there are several contributions in terms of multi-modal retrieval, captioning, ....","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,1/27/2018,14:08,no
91,25,168,Daxxxx H,1,"(OVERALL EVALUATION) The paper proposed an entity relationship model and a set of visualization schemes.
Node-link graph, force-oriented layout and bubble map and etc. are used in the schemes.
The entity model include three entity sets: the main-entity set, the child-entity set and the secondary-entity set.
Two examples of applying the visualization scheme have been presented.

Strength and contributions:
An entity relationship model is proposed and a set of visualization schemes are proposed.
The model has high universality and enables quick search of related entities.
Detailed requirement, rendering method and exhibition schemes have been discussed.
Two visualization systems have been made for two datasets to show the applicability of the scheme.

Weakness and questions:
The novelty of the proposed scheme is limited or at least not well illustrated.
When first mention some algorithm or terms, it?€?s better to add the corresponding citations. For example, when force-oriented layout is mentioned in introduction, citations should be added. Otherwise it is not easy to follow.
In Chinese Medicine, how about two prescriptions with same medicine but different amount? They must be the same prescription?
In related work or introduction, there should be more clearer discussion on how this work differ from previous work.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/11/2018,20:09,no
92,25,1,Trxxx Aaxxxx,2,"(OVERALL EVALUATION) This full paper presents a query/resultset model and visualization scheme for data that consist of entities of multiple types that are interrelated. The initial contribution is a generalized model of main entity, with related child entities and secondary entity(ies), which is applied in the construction of search results and the visualization of the results. The method and visualization are explored two different use cases, which serve as a proof of concept, but otherwise there is no evaluation to validate the generic nature, applicability or usefulness of the solution. 

The contribution is relevant for the conference, but there are different elements of this paper that can be improved. I find that the maturity of the research is a bit below what we should expect for a full paper. 

I find the proposed generalized model interesting, but the paper lacks proper evidence that this is a common pattern. In knowledge bases with multiple entities, there are potentially many different entities that can serve as the main entity - depending on the user??s interest. This sort of implies to me, that such a model is more related to the presentation of results. The two use cases that utilizes this model, may at best serve as an indication that this is a reasonable generic design assumption. The paper has an emphasis on the visualizations, but only presents the solution and a major critique to the paper is the lack of evaluation of the results in terms of usability. A user study - or other form of evaluation - will significantly increase the value of this.

The paper is reasonably well structured, with relevant figures and illustrations. The language generally needs to be improved, and the numbered lists should have been properly formatted as lists. The space before the citations is systematically left out, but the list of references is relevant. In general, I find this contribution interesting and relevant, but will encourage the users to broaden the scope and maybe contextualize and relate to the field of entity search. An more systematic evaluation of either the usability of the  visualization or the general applicability of the proposed model, is needed.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/14/2018,11:34,no
93,25,6,Alxxx Abduxxxxxxx,3,"(OVERALL EVALUATION) This paper presents a visualization scheme for displaying multi-entity relationship using the similarity correlation among the entities. The scheme was then demonstrated through the use of two datasets: a Chinese herbal medicine prescription dataset and a paper dataset.

This paper is interesting as it provides users with the capability of visualizing the multi-entity relationship within a search result. Within the visualizations, users are able to explore the similarity between the entities. However, the colors that are used in graphics make the visualizations hard to read. I would suggest the use of either ColorCAT (http://colorcat.org/) or ColorBrewer (http://colorbrewer2.org/) in the justification of the color usage. Although there is a section on the sample analysis, it would be nice to have a user evaluation to demonstrate the usability of the system (besides the authors of the paper). Perhaps a case study with domain experts, e.g., a pharmacist using the Chinese herbal medicine prescription dataset completing some defined tasks. I can understand the usage of the bubble graph in providing a more detailed class classification chart, but I was wondering whether the paper can also provide an explanation how it plans to address the issue of JND in using a bubble graph.

Some minor comments:
- Missing references:
* NEREx: Named-Entity Relationship Exploration in Multi-Party Conversations (https://dx.doi.org/10.1111/cgf.13181)
* WebVOWL: Web-based Visualization of Ontologies (https://link.springer.com/chapter/10.1007%2F978-3-319-17966-7_21)
- Some typos:
* Section 3.2: and and association -> and association
* Reference to the Chinese herbal medicine system database","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/14/2018,14:17,no
94,26,181,Nixx Hoxxxx,1,"(OVERALL EVALUATION) I think this paper has merit for a number of reasons, although some modification to its methodology would yield more rich results. It is relevant to the JCDL since it highlights practical and social issues related to digital libraries through its investigation of the scholarly literature about digital scholarship centers in academic libraries worldwide, with particular focus on China. They explained that China is in an early, exploratory stage regarding digital scholarship, so this preliminary study is novel and produces the beginnings of a framework for establishing these centers and services in their country.

I thought the methodology of searching academic databases and conducting content analysis on the results was well-intentioned, however they only searched Web of Science, ScienceDirect and Emerald (plus some professional reports) for English-language sources and three Chinese sources. If they intend to learn about digital scholarship services offered by university libraries, the academic literature will likely only provide evidence from other overview studies or individual institutions that have happened to publish about their services, which might not be representative of what's actually offered. Many digital scholarship centers offer information about their services on their websites, in blogs, on social media, etc. and further study of these sources might yield more reliable results about what institutions are actually offering. 

That being said, the authors did a good job analyzing what results they did get, and I liked their strategy of grouping digital scholarship services into broader categories for the purpose of constructing a framework. Groupings like this are interesting and needed within digital scholarship in order to better organize the wide array of services associated with the term. If they find additional services offered through more data/literature collection, they might consider revising or expanding them. The writing style was clear and understandable with a few small stylistic and grammatical errors that a quick copyedit could fix.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/9/2018,17:05,no
95,26,39,Toxxxx Blxxx,2,"(OVERALL EVALUATION) The paper provides an overview of digital scholarship activities in order to support further development in China. While it largely is an overview of existing literature, it could be interesting to the conference to understand some recent developments there. It is a detailed literature review but lacks a strong theoretical framework to bring together the various elements they identify as digital scholarship. It would have been nice to see some critical evaluation whether the hype around DS is justified.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I would think this works better as poster - if accepted.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/14/2018,13:15,no
97,26,397,Nicxxxxx Woxx,3,"(OVERALL EVALUATION) The authors are reporting on a national initiative to establish a theoretical framework for developing digital scholarship services within Chinese university libraries. The initiative itself could be relevant to JCDL audiences; however, the content of this paper is mostly a preliminary literature review. There are distracting grammatical errors and the discussion is fairly short. Table 1 wastes space that could be used for more detailed discussion. There is nothing novel or original in the conclusion. There is little regarding the unique or interesting factors affecting providing digital scholarship services in China. As the authors note, further empirical work needs to be done to support their framework.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/15/2018,19:41,no
98,27,307,Anixxxx Prxxx,1,"(OVERALL EVALUATION) Summary of the review: 


Relevance to JCDL: The given work explicitly deals with the problem of domain specific linguistic variations in a big collection documents. 
The work present a cross-collection topic model combined with automatic domain term and phrase selection. The model achieves this by introducing different distributions (parameter) governing by the entropy for the collection-specific and collection-independent words.  


Novelty/Originality: The main novelty is treating domain specific text as something being generated from a distribution for common terms and another distribution for domain dependent terms (per domain) and training for the distributions jointly to maximise entropy. 

Assessment/Evaluation/Comparison: The proposed model is compared in details with the closest cross collection topics model. The authors perform explicit evaluation on perplexity and topic coherence as well as implicit evaluation on document classification to distinguish the words in two groups as mentioned. 


Style/quality of writing: The writing is good and overall the work is a pleasure to read. Different portions of the paper like motivation, background, technique etc are well weighted. 


Replicability: The experiments are generally replicable. The dataset is public and the experimental details are adequate to replicate the setup.   


Adequacy of references: The references are adequate from a focused task paper point of view.


General Queries:
1) recently there has been a lot of interest in using word embedding for compensating for the lack of semantic information in statistical LDA model? How do you think it compares with [1] (or many similar works)? Basically can word embedding compensate for facts that topic and apparatus mean the same thing (and hence maybe separate distributions are not needed)  

2) How does this model compare with hLDA where we can have hierarchy for domains and topics?


Minor:
line 29/1: ?€?Upto 4% better perplexity?€? use lower instead of better? since other two comparisons are higher and all three are better in some way.

line 22/2:Linguistic contrasts, such as domain-specific vocabulary, complicate topic modeling. Maybe the example paragraph follow directly after this line.


References:

[1] Das, Rajarshi, Manzil Zaheer, and Chris Dyer. ""Gaussian lda for topic models with word embeddings."" Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Vol. 1. 2015.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/15/2018,19:51,no
99,27,107,Andxxxxx Ferxxxx,2,"(OVERALL EVALUATION) In the paper, the authors propose combine ccLDA with a measure of termhood for modeling multiple domain-specific text collections, tokenizing by phrase segmentation. They propose a novel topic model that splits the vocabulary into collection-specific and collection-independent words, according to entropy-based termhood measure.

They uses the entropy of hapax legonema for determining the threshold value to split the vocabulary into domain-specific and -independent words.

The paper is well written and organized, the references are suitable.

The paper is original and relevant to JCDL.

The authors compare their proposal with ccLDA under accuracy, topic coherence and perplexity measures. 

Please, put the equation of termhood in the text body.

At the link provided in the paper, I was unable to access the code of the work. This makes it difficult to replicate the work results. 

In the Datasets section, you must discuss how the articles from English, French and German Wikipedia are reduced to the minimum number of words.

In the Document Classification section, it is not clear what the values presented on Table 3 are. Are they average values from 10-fold cross validation?

A bracket is missing in the first equation at the Perplexity section.","Overall evaluation: 3
Reviewer's confidence: 3
Recommend for best paper: yes",3,,,,,2/16/2018,18:35,no
100,27,140,Cx Lxx,3,"(OVERALL EVALUATION) The authors present a cross-collection topic model combined with automatic domain term extraction and phrase segmentation. Their model distinguishes collection-specfic and collection-independent words based on information entropy and reveals commonalities and differences of multiple text collections. They evaluate their model on patents, scientific papers, newspaper articles, forum posts, and Wikipedia articles. In comparison with state-of-the-art cross-collection topic modeling, their model achieves higher topic coherence, better perplexity, and higher document classification accuracy. They claim there model results in very clear topic representations.

These are interesting results but on small data sets that are very disparate. There are several issues.

How will the models scale? Not well I suppose.

How does one measure ""clear-cut topic representations."" Just by looking? This claim is over stated and not evident from the tables.

It would have been more interesting to compare fields within computer science or within science. The areas compared are somewhat disjoint.

There are many missing relevant citations; here are a few.

Jo, Yookyung, John E. Hopcroft, and Carl Lagoze. ""The web of topics: discovering the topology of topic evolution in a corpus."" Proceedings of the 20th international conference on World wide web. ACM, 2011.

Du, Lan, Wray Lindsay Buntine, and Huidong Jin. ""Sequential latent dirichlet allocation: Discover underlying topic structures within a document."" Data Mining (ICDM), 2010 IEEE 10th International Conference on. IEEE, 2010.

He, Qi, et al. ""Detecting topic evolution in scientific literature: how can citations help?."" Proceedings of the 18th ACM conference on Information and knowledge management. ACM, 2009.

Wang, Xiaolong, Chengxiang Zhai, and Dan Roth. ""Understanding evolution of research themes: a probabilistic generative model for citations."" Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2013.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/18/2018,15:17,no
101,28,399,Mixx Wrxxx,1,"(OVERALL EVALUATION) Paper describes investigation of some possible approaches to ranking papers by citation. Common method is through number of citations. Here, they do some sentiment analysis of the sentence in which the citation occurs, determine if it?€?s a ?€?positive?€?, ?€?neutral?€? or ?€?negative?€? citation, and use that to modify the citation count.  They use some NLP approaches to develop the sentiment - they have developed a classifier to give the positive, neutral (objective) and negative.

They then use this to compare rankings by two general approaches; one based on citation count, and a second based on pagerank-like method that weights citation edges.  In each case, they run over a corpus from the computational linguistics literature (ACL anthology network) twice, once with just the count, or pagerank, the second incorporating sentiment. Results in each pair indicate changes in rank for a given paper. 

I have no problem with the basic method used to get a sentiment measure, and then define the ranking approaches and do a comparison, but what?€?s the value of this beyond a nice NLP and ranking exercise?

Adding in sentiment has changed the ranking, but what does that mean? In each example case, the papers moved down the ranking when sentiment was included (but then didn?€?t some go up then too?) Let?€?s say I have a paper, could I think that neutral citations are in some ways just ?€?dotting the i?€?s?€? of recognizing the literature (e.g. when referencing JCDL literature, something this paper doesn?€?t do by the way), positive cites are those building on, or using, what my paper has suggested, and negative could well be others looking to differentiate their work.  Hence, a negative sentiment could be an indicator of really new work in the field.

I think to make this a more solid contribution to thinking about how to develop automated citation analysis tools that would help others find relevant material in the field, then more thought into what the data means is needed - something the authors begin to hint at in the future work section. At the core of this is to figure out what those sentiment measures can actually tell us.  This is definitely dangling an interesting prospect in thinking of bibliometrics or scientometrics.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The core experiment is actually quite nice and would have been better as a short paper as it?€?s really not yet formed in how their ranking approaches utlizing sentiment would help a researcher in the field. Mainly as they haven?€?t really figured out what their sentiment measures actually mean.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/10/2018,22:07,no
103,28,413,Wexxxx X,2,"(OVERALL EVALUATION) This paper presented an investigation of introducing sentiment of a citation into the ranking of scientific papers. The paper first investigated methods of assessing a sentimental score for each citation sentences. Then the authors proposed two basic ranking schema and compared ranking results with/without considering sentiment score. Finally, the impact of the sentiment influence in the index is closely inspected with one paper as an example. 

The paper is generally well written with experiments towards sentiment analysis accuracy and differences of ranking results. But the overall contribution and conclusion are not clearly presented due to the list of concerns listed as follows. 

- The sentiment analysis accuracy is about 80.61% which doesn't seem very high. The author also did not compare their results with other approaches. As an active research field, many open source packages and pre-trained models are available to support sentiment analysis. The author should consider adding comparisons of their proposed method with some established one, such as Stanford coreNLP for sentiment analysis.  This can help readers better understand their accuracy results and contributions. 

-  The author trained and tuned based on a smaller curated dataset (corpus 1) and applied it to a larger corpus. As shown in table 1, the distribution of positive/negative instances is quite different between two corpora. Additional explanation and analysis may be needed. One potential reason is due to the bias introduced by oversampling of minority classes. 

- The comparison results of different ranking results may need more discussion. What is the goal of such comparisons?  It is not clear what conclusion authors trying to draw besides the four indexes are different. On the other hand, how the sentiment scores are introduced to build indices are not clearly presented. Different weighting schema used may change the ranking results greatly. 

- The analysis of example paper also seems inconclusive. Introducing the sentiment score displayed different impacts in different indexing schema. The research questions 3 need further studies.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/25/2018,11:29,no
104,28,258,Phixxxx Mxx,3,"(OVERALL EVALUATION) The paper introduces different ranking options for scientific papers. The novelty is taking sentiments of citation contexts/instances into account.

The introduction in well written and motivates the project of utilizing citation instances. I would recommend to use citation context as a term. This seems to be the more established term. 

In related work the authors miss the research around citation contexts a bit. There are many relevant papers. They also miss to mention some relevant workshops and shared tasks. But this is not a huge problem.

The problem starts with the research questions which are not well formulated and clear. Esp. RQ 1 and 3. 

The corpora are fine and adequate.

Chapter 6 is detailed but I miss the connection to the RQs.

The ranking indexes should have proper names not just numbers.
The interpretation of the evaluation of the indexes is not well explained and discussed.

Citation contexts/instances can play a major role in DL and can improve IR, but I can?€?t see how the approach in this paper can improve the current situation in DLs.

I like the general idea a lot but the set-up of the paper (RQs, evaluation and discussion of the results) is not proper enough.

Perhaps the paper can be downsized to a short paper. Effectively it is not a complete full paper (10pages) yet.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Perhaps the paper can be downsized to a short paper. Effectively it is not a complete full paper (10pages) yet.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/16/2018,11:51,no
105,28,421,Bxx Y,4,"(OVERALL EVALUATION) This manuscript aims for incorporating citation sentiment into ranking index of scientific publications. While this is an interesting idea, the work described in this manuscript seems in the early stage of research, which is not yet ready for publication due to a number of methodological issues.

First, citation sentiment does not indicate target. A positive sentiment could mean good or bad to a cited paper in a comparison like ""[1] is better than [2]"". Some research is needed to investigate how to identify the target of sentiment.

Second, the details of the citation sentiment classification were not reported. Also, since the goal is to rank scientific publications, how would the classification accuracy affect the ranking?

Third, some caution is needed when using the SMOTE method for oversampling language data. Different from image data, which can be easily altered slightly to generate a new example, language data cannot be easily altered without distorting the semantic meaning. There have been some creative approaches for augmenting language data, e.g. Zhang et al. (2015) 

Zhang, X., Zhao, J., & LeCun, Y. (2015). Character-level convolutional networks for text classification. In Advances in neural information processing systems (pp. 649-657).

Lastly, the evaluation strategy is confusing. Several ranking index methods were compared based on the similarity between the ranking result, and then a particular paper was picked to investigate the influence of citation sentiment on the ranking. Neither step provides strong evidence for the validity of the ranking methods.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/17/2018,3:33,no
106,29,417,Xixx Yxx,1,"(OVERALL EVALUATION) This paper proposed to solve multi-label document classification based on titles only, since the availability of titles is much higher than that of full-text, therefore we could have more training data for a machine learning model. This idea is innovative and has its value in practical usage.

However, there are several problems in the comparison results. Multiple factors (training set size, models, feature representations) are entangled, making the comparisons less convincing.

First, the author claimed that MLP is better than CNN and LSTM in certain cases. However, MLP used the TFIDF representation of *full* text and CNN and LSTM methods use the pre-trained embeddings only for the *first* k words. This is an unfair comparison since first k words are a biased subsampling of the full-text (e.g. on average full-text has 2.5k-6.7k words but the author only used first 250 words). MLP method can utilized all information in the full-text.

Second, the author used different hyper-parameters (e.g. # hidden units) for full-text and titles, and the model for titles has more #hidden units (therefore larger capacities). For comparison purpose, the author should use the same hyper-parameters. Only in that case can we conclude that the benefits are coming from the more available titles, not the model itself. I guess the author's rationale was trying to get the best performance. However this doesn't control the factors.

The claim that deep learning models outperform traditional methods when training set is larger that 65k is not rigorous. This is not what [44] suggested. [44] suggested that generally speaking there is a dichotomy, but the exact number should be case-by-case.

The author claimed that there are very few papers on classification with large-scale label spaces. However nearly all neural machine translation models are dealing with large-scale label spaces.

A citation to AdaGrad is missing.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/16/2018,18:21,no
107,29,31,Daxxx Baxxx,2,"(OVERALL EVALUATION) This paper presents work investigating the performance of several neural models (CNN, LSTM, MLP) at the task of multi-label subject classification for two publication datasets (EconBiz and PubMed), where the primary question is assessing the relative performance of models trained only on the full text of the articles compared to models that have increasing access to additional training data in the form of article titles.  This work is very clearly described, well-grounded in important literature on neural approaches to text classification and presents an interesting, focused contribution.  A few comments:

-- The main contribution of this paper is presenting experimental evidence of the relative performance of training on just full text compared to full text + many more titles.  To trust these results, we need to be sure that all models are given their best chance of succeeding (in terms of hyperparameter optimization etc.).  Section 4.3 notes how hyperparameter choices were made for full text and titles separately (which is great!) but I'm unclear concerning the discussion throughout of different architectures chosen for different models/training sets (e.g., one-layer MLP with 2K hidden size presumably for just full text, and a different model for full text + titles?  PubMed titles has its own model?), and it would be helpful for the reader to understand exactly where those design choices were made.  Additionally, while the use of deep-ish neural networks is motivated here by the size of the *titles*, that assumption is not applicable for the smaller full-text dataset.  Including models that are designed to give the full text their best fighting chance would be more convincing than being hamstrung by the constraint of a neural model.

-- What exactly is a ""sample-based F1"" (and how is it different from standard F1)?

-- More details on exactly how hyperparameter optimization was carried out would also be useful (e.g., held-out dev data)?

-- The results in table 2 have enough variability that it would be helpful to quantify your uncertain about the performance with confidence intervals.

-- It's not clear to me exactly how the MLP baseline is different from other MLP whose results are reported here.  From the description in 3.2, it seems to changing the activation function in the base MLP to a ReLU and adding bigrams as features?","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/17/2018,1:03,no
108,29,174,Draxxxxxx Herxxxxxxx,3,"(OVERALL EVALUATION) This paper looks at subject classification of scholarly documents. It compares the performance of classification models trained on full-text vs solely on titles. The motivation for exploring title-based classification is the wide availability of titles for training (since full-text is freely and easily available only for open access documents). Given the amount of available training data, the paper looks at whether increasing the number of training examples when using titles can achieve comparable performance to full-text.

For this task, the authors explore deep learning, which has been shown to work especially well and significantly better than traditional models when large amounts of training data are available. Furthermore, the authors look at this task as the case of extreme multi-label classification as both datasets contain thousands of labels. Specifically, the authors compare the performance of multi-layer perceptron (MLP), convolutional neural network (CNN) and recurrent neural network (RNN) for subject classification of papers from the biomedical database PubMed and from the economy and business database EconBiz. 

This is both a well written paper and an interesting experiment. The authors show that given enough training samples, title based classification can reach a comparative or even better performance than full-text based classification. This is a useful finding which shows large databases of publication titles such as PubMed can be used to create models for automated subject classification. I also think it is interesting the MLP and the LSTM performed so well (compared to the CNN).

I only have a few minor questions. When describing the MLP model, the authors state only 25k most common unigrams (and additional 25k bigrams in case of the second MLP model) were used. How did the authors determine 25 thousand unigrams to be sufficient for the task?

I would appreciate if the authors could talk in more detail about their chosen evaluation metric (F1 score). Was it calculated as mean F score per label or in a different way?

Page 7, beginning of discussion (I think this may be a typo) ?€? the authors state the performance of the best performing title model trained on all available EconBiz titles is nearly 10% better than the best performing full-text model, however, table 2 and the previous text say this improvement is less than 5%. Maybe a typo?

I would also be interested in how long did the models train for each training set.

Overall this research seems well done and within a scope relevant to the conference.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/17/2018,3:45,no
109,31,168,Daxxxx H,1,"(OVERALL EVALUATION) Generally I think it's good paper. Especially for the way it propose to generate synthetic data. 

Strength:
1. A novel way of generating labeled data. It?€?s really useful for data-driven approaches. Compared with several previous way of generating labeled page data, the one proposed here is more ?€?general?€?. Both latex and xml documents are considered. According to the evaluation, the generated data has high precision and recall. It could also be used for generating other elements too.

2. Learning based model could be improved by feeding more data. I think it is the good direction to go and the performance showed it outperforms rule-based method in PubMed dataset.

3. The paper is clearly written and easy to follow.


Weakness&Question:
1. Overfeat is not state-of-the-art framework any more. Have u tried Faster-RCNN or SSD? The caption detection problem might be solved by using a better architecture. For example, change the different anchor setting in Faster-RCNN.

2. why the dataset is called distantly supervised dataset? Because it?€?s synthetic?

3. How to deal with sub-figure? Figure that contains several sub-figures seems not easy to deal with a pure visual model.

4. There are two recent paper ?€?Learning to Extract Semantic Structure From Documents Using Multimodal Fully Convolutional Neural Networks?€? and ?€?Multi-scale, Mutli-task FCN for semantic page segmentation and table detection?€?. It?€?s good to see some discussion in the related work.

5. More ablation study could be done. For e.g, changing the CNN architecture.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/4/2018,17:07,no
110,31,374,Domxxxxx Tkxxxx,2,"(OVERALL EVALUATION) The paper describes a method for generating a ""gold standard"" dataset useful for extracting figures from scientific papers, as well as a neural networks-based algorithm for this task. In general, this is an important task and very relevant to digital libraries. The dataset the authors published will definitely be useful for the future research. The research described in the paper is novel. The paper is well written and easy to understand. In my opinion, it is a good contribution to JCDL.

The biggest issue I see is that the authors work mostly with images of the documents' pages, for example, they detect figures in the images of the pages using neural networks. At the same time for many many years scientific papers have been published as born-digital PDFs, which already contain the images along with the direct information about their location on the pages, so there is no need to ""guess"" this using machine learning. I wonder why the author do not extract this information from the PDF content and use it for figure extraction. I would strongly suggest the authors to at least comment on this issue in the paper.

Of course, this issue is not relevant for tables or image/table captions, which are present as fragments of the PDF text stream and are not easily distinguishable from other fragments.

The authors also use the PDF text stream for some tasks, such as finding image captions and table regions. The problem here is that the order of the text in the PDF stream in general is meaningless and has nothing to do with the order of the text on the page. So for example, it is entirely possible that the image caption text is split and fragments are present in different locations within the PDF stream. Standard way to analyze PDF text is to extract text fragments from the stream along with their coordinates, and concatenate them using only the coordinates. Libraries such as PDFBox or iText can be helpful for this.

Minor comments:

1. Several times the authors write the phrase ""labels for figure extraction"", or similar. In fact, it appears in the second sentence of the abstract. It is not obvious what are those ""labels"". I suggest to provide more detailed description and/or examples, ideally the first time it is mentioned.

2. Please specify for all the tasks what is their input (images/PDF/...). It is not clear.

3. There are some interesting differences in the distributions in Figure 2. Is this due to the different research areas of the papers, or the described method for building the dataset?

4. Please use labels for axes in figures (eg. figure 2).

5. Evaluation: it would be nice to see the results of PDFFigures/DeepFigures separately for figures and their captions (table 3). Also, have the authors evaluated matching figures with the captions, that is, how accurate is the described distance-based approach?

6. In line 956 the authors write: ""Additionally, our data generation approaches could be extended to other information about papers such as title, authors, and sections; while systems for extracting this information have traditionally relied on purely textual features"". This sentence is not true; for many years now the state of the art metadata extraction systems (GROBID, CERMINE, PDFX, Team-Beam and others) use extensively visual/location/formatting/size features to locate various information within the documents.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/12/2018,14:51,no
111,31,417,Xixx Yxx,3,"(OVERALL EVALUATION) This paper proposed using deep object detection model from computer vision community to extract figure and tables in PDF. They leverage the auxiliary data provided in two large web collections of scientific documents (arXiv and PubMed) to locate  gures with no human intervention. The large-scale annotations will be released and the performance is on par with a rule-based method on CS-Large and significantly better on PubMed.

The novelty is limited, since the similar object detection model is widely used in computer vision. Also, the author should cite several papers that use deep learning models (detection models and segmentation models) to extract figures/tables, such as [1,2.3].

[1] Chen et al. Page segmentation of historical document images with convolutional autoencoders. ICDAR2015.

[2] Yang et al. Learning to Extract Semantic Structure From Documents Using Multimodal Fully Convolutional Neural Networks. CVPR2017.

[3] He et al. Multi-Scale Multi-Task FCN for Semantic Page Segmentation and Table Detection. ICDAR2017.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/16/2018,18:44,no
112,32,331,Chrxxxxx Sexxxx,1,"(OVERALL EVALUATION) This paper investigates how to rank formulas hypothesising that different formulas within one (Wikipedia) are not of the same importance. 
The work is interesting for the community and well motivated. 
Compared to previous work the authors propose an extended features set (including graph-based features) and evaluate the contribution of each feature type on the final performance. 
While the work is generally sound and mostly convincing, the paper lacks details for fully understanding the approach. The same lack of details hinders the judgement of the validity of the authors' evaluation. Both does not allow to reproduce the experiments (which would be feasible if the source code would have been published). Further, the authors refer to their most related work (which is from the same group [1]), but decide to generate a new data set for evaluation, and I am not sure why (is the data set easier?).  Related work needs to be rewritten.
See detailed comments below.

[1]https://dl.acm.org/citation.cfm?id=2756918

DETAILED COMMENTS

Related Work
- What does "" use the manual run to derive insights for refining the query formula to improve the low precision"" mean? For me, this does not summarize the work of Schubotz et al.
- ""through LDA and doc2vector method"" - I suppose ""doc2vec"" is meant here. 
- W.r.t. the work most similar to the paper, Wang et al., it would help to be clear which features they employed, to judge how large the extension was. 

Method
- ""citations (hyperlinks) which exists in the description of the formula"" -> I am not clear how you identify the description of the formula. For some examples (as in Figure 2) it might be obvious, but what about the formulas in page https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms for example?
- 3.2.2. details about how the word embedding model was generated are missing (e.g. stopwords removed?, hierachical sampling used?)
- formula-based features: how is the length of a formula calculated? e.g. x^2 is it of length 2 or 1?, similarly, is the numerical position counted in characters?, is position feature binary (i suppose so)?
- inner-article features: 
  - semantic relevance to article ""Then we respectively calculate the semantic relevance among the title, lead paragraph of article and textual descriptions of the formula as two metrics"" - I am little confused what is compared to what? 
  - semantic relevance to keywords: here tf-idf+cosine is used. Why? Or why did you chose a different similarity as before (similarity to article)?
- inter-article features:
  - as I read in 3.2.1. the graph is constructed with wikipedia pages as nodes. I suppose ""in-formula"" is the in-degree of the corresponding node in the graph? But what about formulas that do not have an own wikipedia article? As https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms? Or do you omit those?  The same question is open for me for the feature ""out-formula""

Evaluation
- Details about how the arcticles are selected are missing. This is crucial, since it would allow to judge the complexity of the task (and maybe answer the question about formulas in articles like https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms I mentioned before)
- Figure 3 - how many articles do have exactly 1 formula? Ranking would be easy for that subset of the data set.
- If you use the average values of the annotated importance (which is a ordinal value between 0 and 4), how do you deal with e.g. with an average of 2.3? Is this rounded?
- Table 4: Value ""0.0.93""?
- Table 4: score > 0, P5, w/o For value 0.9347 is greater than the (bold printed) value in the ""All"" column ""0.938""
- The most related work stems from the same group, so I am wondering why the evaluation was not done on the data set used before? 

Language, form, structure
- ""previous work has achieved well performance"" -> good performance
- ""just play the supportive roles"" -> omit the ""the""
- Omit heading 3.1. 
- Figure 2 is not readable in print-out. There is some horizontal space left, with some rearrangement the figure should fit without adding vertical space.
- RankSVM[], ListNet[], same for 3.3.2 ""surrounding text[]"" -> spaces are missing 
- footnote 2 -> url no longer valid, should it be https://sourceforge.net/p/lemur/wiki/RankLib/?
- case-sensitivity in sec 3.3.1. and 3.3.2: inconsistency in feature naming, e.g. ""Semantic Relevance to article"" vs. ""Semantic relevance to Keywords""
- ""The articles of Wikipedia is well-structured"" - singular/plural
- Table 2: round values to the same precison 
- ""they are only consider"" -> they only consider","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/7/2018,11:07,no
113,32,19,Haxxx Alxxxx,2,"(OVERALL EVALUATION) - This paper addresses the idea of ranking formulae within an article based on the relative importance of each. Using articles and formulae from Wikipedia, the authors build a formula citation graph and use a word-embedding model to store the relations between the formulae and articles (11 features). Their ranking achieved better results than other baseline methods have.  
- With only 332 articles that present 2,384 formulae, the dataset is not large.
- Six undergraduates labeled the formulae based on their importance (0?€?4), and the average was used. However, it is unclear as to whether the undergraduates had any issues in regard to understanding any of the articles or formulae. Further, the authors do not state whether the ratings were distributed or skewed. 
- The problem is not well motivated. The fundamental question of how ranking formulae will benefit researchers is not clear. 
- There are situations where in order to assert or stress a point, an author can explain concepts using an example, and in this paper the authors do exactly this using Bayes?€?s Theorem. However, the authors do not state how the model would behave or how to rank the formula.
- It is not clear how the paper would deal with similar formulae that use different symbols. 
- Additionally, most related papers refer to at least one common formula. However, it is not clear how this paper would deal with such cases. 
- Page 2: Footnote #2 https://sourceforge.net/lemur/wiki/RankLib returns the following statement: ?€?Whoops, we can't find that page.?€?
- Page 3: The description of the ?€?link?€? feature is not very clear. 
- Page 4: The following incorrect citation is given: ?€?[14] Kushal Singla. 2016. A Document Retrieval System for Math Queries. In NTCIR.?€? The citation should read as follows: ?€?Thanda, Abhinav, et al. ?€??€? 
- Some related works that are not cited: 
- Sun, B., Tan, Q., Mitra, P., & Giles, C. L. (2007, May). Extraction and search of chemical formulae in text documents on the web. In Proceedings of the 16th international conference on World Wide Web (pp. 251-260). ACM.
- Kacem, A., Belaid, A., & Ahmed, M. B. (1999, September). EXTRAFOR: automatic EXTRAction of mathematical FORmulas. In Document Analysis and Recognition, 1999. ICDAR'99. Proceedings of the Fifth International Conference on (pp. 527-530). IEEE.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/16/2018,1:18,no
114,32,313,Andxxxx Raxxx,3,"(OVERALL EVALUATION) The paper presents an approach to rank equations within an article with respect to their assumed importance, with the importance being computed via factors such as citations to an equation, level of detail, etc.

The paper is generally well written, providing a convincing story line and solid analyses. One limiting factor might be the reliance on Wikipedia articles both for the establishment of the citation graph as well as for the source article to be ranked. However, this seems to be rather a practicality of parsing equations rather than a fundamental issue.
Concerning the study, the authors report on the gender of the annotators ?€? is this information relevant in any way? More interesting in this context would have been to report the inter-indexer consistency to determine how solid any ranking numbers obtained are.
Minor aspect: figure 1 has printing problems, resulting in a massive black box obfuscating the figure.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/17/2018,10:26,no
115,35,272,Anixxxx Mukxxxxx,1,"(OVERALL EVALUATION) This paper  presents an analysis of what scholars ask about, what features the questions and answers convey, and what socio-emotional reactions feature during the interaction on ResearchGate Q&A?
One of the strong point of this study is annotation of 371 questions and 7530 answers extracted from RG's Q&A into several categories mentioned in the paper. 

The paper is fairly well written and the methods are fairly well explained. Some of the answers for question 3 (Section 3), I did not find particularly useful or compelling. The differences across questions seems to be the most useful of the analyses.

Some issues:
1. I could not find definition of non-standard terms like ""AVE"", ""PCT"" presented in Table 4.
2. Many grammatical mistakes like ""What characteristics of different types of questions?"", ""What characteristics of the answers of different questions?"", ""Measures of Answers under Questions""etc.

I strongly suggest that the annotation be made public for future use. This will be of particular interest to the JCDL community to present some use-case of this study.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,1/27/2018,7:12,no
116,35,365,Taxxxx Suxxx,2,"(OVERALL EVALUATION) This article describes an analysis of question - answer pairs on Library and Information Science drawn for ResearchGate. The authors briefly review the rise of academic social networking sites and prior studies. The majority of the paper describes the results: characteristics of the questions being asked, time until they were first answered, nature of the answers and how these vary by type of question, etc. They then discuss the results in terms of three research questions: types of questions, types of answers by question type, and how these vary based on ""socio-emotional"" reactions. 

The strength of this paper is that it has all the pieces one would expect in a scientific paper and is reasonably well-organized and written, within the page limit constraints. More examples of what their coding framework items really mean would have been helpful, particularly for the socio-emotional analysis which seems like a misnomer relative to the data being reported. 

The weakness of the paper is that there is really no motivation for the research - who will use this information in what way? There is no contribution to theory, and no stated contribution to practice. The justification is the academic social networks exist and therefore we should study them. But, the authors provide no ""towards what end"" rationale. It is not clear what other researchers should do with the information that most questions posted on Research Gate are seeking information, rather than opinions, for instance. The key finding is that if you ask a question on Research Gate you had better be prepared to wait a long time for an answer (Table 4)!

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This could be a good POSTER.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2/12/2018,23:52,no
117,35,26,Yaxxxx Alnxxxxx,3,"(OVERALL EVALUATION) The paper is a quantitative study based on analyzing 371 questions related Library and Information Science (LIS) on Research Gate. The main questions that the authors investigated were: What kinds of questions do scholars ask on RG Q&A on the topic of LIS?, What characteristics of the answers of different questions?, and What socio-emotional reactions do scholars express during the interaction?

The findings of the paper are interesting and draw a good picture for the Q&A in RG in LIS. I couldn't see why these findings are useful or what is next? 

These are some suggestions/comments:
* The methodology of collecting the data is not clear. How the categorization of questions was done?
* Bales?€? interaction process analysis (IPA) - the authors mentioned the method they used in identifying the socio-emotional reactions of the answers and questions, but they didn?€?t clarify the process. Was it the process of identifying the reactions manual or there was any type of extraction? 
* The first paragraph of section 3.2 is not clear.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I would recommend to re-submit it as a poster.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2/16/2018,19:15,no
118,36,80,Faxxx Crexxxx,1,"(OVERALL EVALUATION) It is an interesting paper presenting a methodology that combines the analysis of content read by searchers while performing some predefined controversial search task. While it does not manage to find an answer to its original research 
question, it still describes an interesting way to combine different kind of data when running IIR studies. Overall, it needs some rewriting and refocusing and its current form is not ready to be published but it is a promising piece of research and authors should be encouraged to continue and frame their contribution in the definition of innovative  methodologies.
Cons: 
- students as surrogate searchers cause a serious bias to study
- tasks are not real and thus motivation is an issue too
- authors are aware of the limitation caused by ""the sample and information seeking tasks information"" but they should go further and argue that they could not come to any conclusion in respect to their research questions. Instead they should describe in more details their methodology and how it can be used in IIR studies.
- lack of definitions: some very crucial concepts are left without proper description or even reference, e.g., Comprehensive Model of Information Seeking or the Big Five Model, even if authors expect these to be well known stil they should at elate provide a literature reference
- English needs revision

Pro:
- the use of a combined method of content and statistical analysis provides some very interesting insights
- pilot study paving the way for its redesign","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,Monica,Landoni,monica.landoni@usi.ch,317,2/14/2018,17:14,no
119,36,5,Juxx Abxx,2,"(OVERALL EVALUATION) The paper is not in scope of JCDL. While JCDL does include papers on information seeking and information behavior they are always tied to other relevant topics related to digital libraries. There is no evidence of this in the paper. It presents a study on variables that affect cognitive dissonance within biased information seeking. The variables explored include: gender, personality type, and health info literacy. There is no evident application of the conceptual model or limited findings to digital libraries or other systems.

Further, the writing is problematic and suffers from ESL issues that affect readability at times. The tables are not useful and do not present the findings in a readable manner. The patterns exhibited by the participants (S and O) are not explained in the narrative but appear as one line under the table. The authors, while saying they measured the effect of gender, do not present findings on this variable.

To make this paper of publishable quality for JCDL Proceedings would require significant revision by the authors.

I would reject/strongly reject this paper based on these observations.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2/15/2018,17:19,no
120,36,183,Xixx H,3,"(OVERALL EVALUATION) This paper presents a study exploring how individual differences could possibly influence cognitive dissonance reduction in biased information seeking. The topic is interesting and relevant to JCDL. The authors did a good job in explaining what biased information seeking was, and grounding the study on the theory of cognitive dissonance. 

The use of eye tracking is relatively novel in information seeking studies, which shows good potential of this study.  However, this paper in its current form lacks many technical details and justifications, and thus may not be suitable for publication.

Although biased information seeking is an interesting and important question, it was not very clear why the two patterns (S and O) were important or why the research question was important. In other words, what are the implications of the findings? Would the findings help improve information seeking or information system design? 

The statistics of the S and P patterns shown in Table 2 were not defined, leaving readers wonder what those numbers represent. The ANOVA test also lacked explanation: what were the different groups being tested with regard to each variable? For gender variable, as there were only two genders, shouldn't t-test be used instead of ANOVA? 

In summary, the study is interesting and potentially publishable, but this paper is not sufficient to publish in its current form. The authors are encouraged to modify the paper and resubmit to JCDL next year.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/17/2018,9:01,no
121,37,59,Joxx Hx,1,"(OVERALL EVALUATION) The paper presente Microarchives, a proposal of framework for the management of complex Web resources as cohesive entities. The topic is definitely within  the scope of the Conference, and the motivation for the work is clearly stated.

Dealing with complex digital objects has been a key challenge since the early architectural models like the Kahn and Wilensky Framework. Obviously, the advances in the forms of Web content left some issues out of the framework that need to be addressed.

The initial study of the management of dynamic Web content is interesting. The different dimensions studied have not been, however, explicitly addressed in the presentation of the proposed solution.  

While the idea seems promising, the paper style is narrative, lacking technical detail that could have been provided in the extension of a full research paper. Some more formal detail could have been provided to e.g. metadata schemas, or the different services provided, which are only described with text accompanied by some screenshots. No details about the APIs  of the different components, the metadata schemas or the exchange formats are provided.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The idea of the paper is nice, as the prototype looks, too. However, there is very little technical detail included in the paper. the level of detail is rather suitable for a poster presentation.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/12/2018,18:18,no
122,37,27,Omxx Alxxx,2,"(OVERALL EVALUATION) The authors describe Microcrawler, a web application that allows users to create Micro Archives. Micro Archives are digital object representations.

The idea is very promising, and the concept is neat, with lots of practical applications. 

The paper is mostly about the implementation of the proposed approach using two use cases: blogs and software. It is not very clear how one should interpret the results of both use cases. Are there any recommendations?

On the system side, the components are described at a very high level. That said, the source is available via github.

Minor item: Figure 6 is too big.; figure 1 is unnecessary.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I'm not really an expert on this topic. From the systems side, the paper is weak (not a lot of detail). That said, they are trying to solve a good problem.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/14/2018,22:43,no
124,37,189,Nabxxxx Jaxxx,3,(OVERALL EVALUATION) The paper introduces the concept of Micrawler and how can it be beneficial to Micro Archives. It gives a good overview of the architecture. The methodology is well defined. The author talks about the opportunities and ends the paper with a nice conclusion.,"Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/20/2018,14:52,no
126,38,271,Anixxxx Mukxxxxx,1,"(OVERALL EVALUATION) I liked the overall idea of the paper. The experiments have also been done quite rigorously. However, the major problem of this approach is that the authors approximate all their quantities, using the nodes only in set C. For instance they assume that the fidelty of all the nodes in T can be approximated by the fidelty of the nodes in C. Similarly, they extrapolate the impact of the ghost vertices through the impact of the vertices C. These approximations could have serious repercussions and an exhaustive error analysis experiment is needed to support such choices for real networks. For instance, the error for various sub-cases need to be studied |T|~|C|, |T|>|C|, |T|>>|C| etc. If the error values diverge then that is fatal for the system. In case the paper is accepted, I would like to see a complete treatment of errors in the camera ready version of the paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I am giving a weak accept because the identification of the problem is good and the experimentation is exhaustive. However, the error due to their assumptions might make their system unusable. In case of acceptance, the authors should be advised to report error analysis experiments.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2/24/2018,5:42,no
127,38,55,Klexxxx B??xx,2,"(OVERALL EVALUATION) The paper targets at the difference between ideal crawling results on a 'complete' web graph and the crawling results on an, as they call it, incomplete graph. In particular, their concern is that values of so-called centrality indices may be different on incomplete graphs, and they try to quantify this difference.

I have some severe concerns regarding this submission, which -- in my opinion -- deem it unsuitable for publication at this point.
- Motivation. Their motivating example is that the NASA homepage may not be available, while other pages on this same site are. I am not convinced. Why should exactly the homepage be unavailable, while other pages from the same organization are? IMHO, a real crawler would just try to access these pages another time later.
- Undue simplifications. Your model relies on a, as you can it, simple crawling strategy ""where it can be assumed that each vertex is part of the crawl independently from all other vertices with some sampling probability p_s"". Why is this a _simple_ strategy? I totally do not see how one could implement this.
- Unclear reference point. You ""focus on the most impartial strategy, wich is vanilla BPS, but explicitely produce partial crawls by dropping x% of the vertices of the input graph"". If I know that I will compute centrality measures later, why should I pursue such a crawling strategy? Wouldn't it be more reasonable to try to identify the central nodes as early as possible, during the crawl, and to keep these? It seems to me that the problem is somewhat constructed: First you throw away nodes in a random manner. Then you observe big, 'remarkable' deviations between the centrality values in your sample and in the original graph. But I do not see why one should throw away nodes in this way in the first place. (I might have missed/misunderstood something here, but then it is the responsibility of the authors to present things in a way that does not give way to false conclusions.)

Detailed comments:
- ""exhibit deviations from their original values"" -- This is not a fact, but the hypothesis behind your work. You should formulate it like this.
- ""Vertices in our input crawls are either completely crawled ... as ghost vertices."" -- I do not really understand why we have this sentence. Isn't this completely clear?
- ""We present ranking correlations ... in the above mentioned .gov graph."" -- This is not sufficiently intuitive in my opinion.
- ""In addition the results from [32] imply ... "" -- I do not see how this sentence is related to the previous one.
-""... and can be easily computed in a distributed manner"" -- How can this function? You should provide some intuition here. -- More importantly, I do not really see what I would do next once I know this number.
- nDCG -- Please provide a reference.
- ""impact"" is mentioned in the first paragraph of Section 4, but without proper introduction. There is a definition later, but this is too late, since you are using the term earlier.
- What is a 'discordant pair'?
- You are overloading the abbreviation d(v). Once it is a characteristic of the target graph, another time (one column later) of the crawl. This should be avoided.
- ""Moreover, for synthetic target graphs ... and constant p."" -- I do not understand this.
- ""\Sum_{v.u\in N(v)} ..."" -- I do not understand this notation.
- ""The impact on any vertex ..."" -- of v, or what do you mean?
- ""assuming that each ghost vertex exerts on average an impact of Im(C) on its neighbors"" -- Why is this assumption justified? It seems to me that it is not.
- ""To understand ... acts as the seed set. -- All in all, this is not understandable.

- English:
* Especially if not crawled for ... such as Web archives.
* Complicating matters further.
* suitable sized
* a ranking deviations
* explicitely","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/19/2018,9:46,no
128,38,53,Roxxx Buxx,3,"(OVERALL EVALUATION) This paper describes a method for estimating the non-fidelity of a sampled graph in terms of the ability to rank nodes correctly using PageRank. This metric is then used to extract subgraphs from large graphs that are likely to be high fidelity.

I am concerned about the technical soundness of the paper. Some assumptions were insufficiently supported. 

- In 4.1, the authors assume that nodes are sampled independently and uniformly at random. Of course, this is not how crawls are typically constructed, which the authors well know since they model web crawls later in the paper. The assumption of independent sampling is not realistic and could have a big impact on their key result, the fidelity measure. The authors do not address the limitations implied by this assumption, even in the concluding discussion. Possibly the reason that the HAK measure works well on random graphs is because the assumption is closer to reality here, rather than anything structural about the graphs themselves. More attention needs to be paid to the impact of this assumption, or possibly a more realistic assumption can be made in its place. (For example, the probability of sampling a node could be made proportional to its degree since high degree nodes are more likely to be encountered in crawls.)

- At the end of 6.1, the authors state that they used personalized PageRank when computing the ranking deviations for their target graphs. The reason for doing this is extremely unclear. Personalized PageRank and regular PageRank are quite different and serve different purposes. Seed nodes would, of course, be more likely to be internal to the sampled networks and have more reliable PageRank values. So, it is not at all clear that the results relative to the personalized version generalize to non-personalized PR. This limitation is not mentioned in the abstract, which discusses only PageRank and makes the work seem to solve a different problem than it does.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/20/2018,22:21,no
129,39,149,Swxxxx Gotxxxxx,1,"(OVERALL EVALUATION) 1.	The paper focus is not clear and the author digressed with many objectives. This is analytics paper but lacks the motivation for analytics. The goals defined as similar to the design of the dashboard. There is nothing new in this paper
2.	The author also talk about various papers in library research but the linkage to the paper focus is missing. Is this paper relevant to recommendations or space utilization or resources (both physical and electronic) management or user profiling or user behaviour analysis or subjects/topics analysis or Technology analysis? I see everything combined in the write up but the flow is tangled and complex
3.	Overall, what is the takeaway from the paper? What is that someone can learn from this paper? How can they benefit in applying what they learn from this paper? All these points are unanswered.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2/16/2018,0:07,no
130,39,401,Dxx W,2,"(OVERALL EVALUATION) Internet of Things (IoT) infrastructure within the physical library environment is popular, which includes features for location-based recommendations. The topic of this paper is to analyze the users' requests and behaviors, which is very interesting. The results indicated that users of IoT-based recommendation are interested in a broad distribution of subjects, with a short-head distribution from this collection in American and English Literature. A long-tail finding showed a diversity of topics that are recommended to users in the library book stacks with IoT-powered recommendations.
  However, the paper can be improved in the following aspects:
  1) I suggest the literature review focus on the location-based recommendations of Internet of Things infrastructure within libraires. However, right now it includes too many topics as Information Organization Foundations and Emerging Technology, Information Seeking Research Needs, Monitoring and Evaluating Space with IoT Hardware.
  2) Before results analysis, it should introduce the data set that was used in this study and the Internet of Things infrastructure in the author's library. So that we can know how it works.
  3) The biggest problem of this paper is that most of the results analysis is descriptive statistic, like API hit by month, location distribution, subject distribution, etc, and the results are not surprising. I would like to suggest more deeper analysis methods to be used to find more interesting results. 
  4) Some suggestions and implications should be addressed to the Internet of Things infrastructure within the physical library and its location-based recommendation services.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/16/2018,16:49,no
131,39,196,Samxxxx Jayxxxxxx,3,"(OVERALL EVALUATION) This paper presents IoT infrastructure within a physical library environment for mobile, dynamic wayfinding support and recommendations for items in the library stacks based on the location-based service. The authors presents the evaluation and analysis of the user requests for recommendations based on their location and the subject area of the library stack which the user request recommendations. The results suggest the users of this particular study are interested in a broad distribution of subjects with diverse topics from the library book stacks. 

It is not clear from the section 3 how the WayFinder app is enhanced with the IoT location-based recommendations. The Figure 1 depicts some of these ideas but I believe some technical details of the architecture would greatly improve the readability. Does the WAyfinde provide recommendations based on the currently searched items and then locate the similar items and populate it based on the location? The example only describes a situation where the popular items (popular is loosely defined here!) near the user. Does the popularity is based on the searchers from other users or based on the prior searches made by the same user? Based on the statement ?€? similar needs can be analyzed to create a hybrid collaborative filtering with the content-based filtering from the app search modules, which may be relevant in providing recommendations here?€? I assume the recommendations are only based on the classification associated with stack, location.  

On the positive side, I believe the approach is well-motivated and related to the digital library. The basic idea seems natural, and thus worth for an investigation. On the other hand, I cannot judge the novelty of the paper beyond what is stated therein, namely it lies on the proposed idea of creating simple recommendations from the library stack locations and user navigation.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Seems like a similar version of the paper exist elsewhere,
https://arxiv.org/ftp/arxiv/papers/1801/1801.06552.pdf","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/16/2018,19:17,no
133,40,124,Nuxx Frxxx,1,"(OVERALL EVALUATION) This paper still requires much effort from the authors to be ready for publication. Interpreting the description of the research is extremely hard due to the immature state of the text. Thus, due to the problems with the writing, it is difficult to provide detailed comments.

The authors should think more deeply about the right terms to use to describe the concepts involved in their work, and also the English language usage, which needs considerable improvement.

The paper also does not strictly follow the template and is not well formatted in many places.

One comment on the title, is that the use of unexplained acronyms (SQA) should be avoided.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,3/9/2018,1:49,no
134,40,204,Minxxxx Kx,2,"(OVERALL EVALUATION) This paper examines the behavior of 4M+ users on the Zhihu community
question answer site.  The authors report a long-tailed distribution
of user's activity and categorize the users into several different
user types.

While such a study would be of great interest to the social media and
social community areas of JCDL, I cannot currently recommend this
paper for publication due to many problems with the current work.

First, glaringly, the format of the paper is incorrect.  This is
unfortunately, a telling sign that the authors did not bother to
invest much into their submission quality.  There are many problems
with whitespace, incorrect spacing and poor figures that give real
difficulties in the readability of the work.  Literal variable names
are undifferentiated from the normal text making it difficult to
follow.  Table 2's multi-page span is unwieldy and not helpful and
needs to be better communicated to be helpful to anyone.

Second, more importantly, methodologically the work needs improvement.
The role classification (Table 1) is simplistic, belying the
complexity of the mixed_behavior role.  The core work on page 5-6
seems to just lie with the execution of some MATLAB routines and to
report these findings.  The ""entropy method"" needs to be better
grounded and disclosed.

I would encourage the authors to work towards more sophisticated
methods for analysis, as it is clear such a dataset would be useful to
analyze and compared for user behavior differences to contrast with
other social network and CQA sites elsewhere (e.g., Baidu Zhidao,
StackExchange/Overflow, and Yahoo! Answers).

Details:

- I think the common term is community question answering (CQA) rather
than SQA.

- (Python-based) crawling is a technical detail and does not merit
  more than one or two lines of discussion in a scientific paper,
  unless there is significant changes or particularities of the work.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2/21/2018,1:00,no
136,40,314,Saxxxx Rxx,3,"(OVERALL EVALUATION) This paper analyzes the Chinese social network Zhihu. The users are classified into four types: knowledge seeker, contributor, browser and mixed behavior user. The category of a user is found deterministically depending on how many posts they write or read. Finally, the ""score"" for a user is calculated using an entropy-based method. 

The paper is readable but has many formatting issues. There are many spaces and I am certain that the ACM format is not followed here. Some information is irrelevant such as: a. ""Python's pandas object is used to group large sample data"" or ""The Python pandas object was used to separate the sample data and grouped by using the grouped"". Figure 3 should be a histogram instead of a scatter plot. Figure 4 is not at all clear: the proportion data should have been explained in other ways.  

But I have a major concern with the way the ""user performance quality assessment indicator system"" is created, which is the actual contribution of the paper. Table 2 possibly lists the features used to calculate the user performance. But it is not well explained what the ""indicator weights"" for these features are (other than the statement ""The entropy method uses the concept of information entropy to calculate the weight of the indicator system, that is, the greater the difference between each values of an index is, the smaller the entropy is, and the greater the weight of the index is"").  This should have been explained in greater detail. Also, it is unclear what the last column in table 2 is (""0.36"", page 5, last paragraph). Finally, the statement ""Due to the large number of users, we use the average score of the four categories users to represents the user's level of performance quality"" isn't very clear to me. Does this mean all user's in the same group have the same level of performance quality? The granularity is fairly shallow in that case. For figure 5, there's no X-axis or Y-axis label, so hard to understand what it represents. 

The contributions listed by the authors in the conclusion are as follows:

1. ""This paper uses Python to crawl the behavior data of more than 4 million users on Zhihu platform""
-   This is not really a research contribution. 

2. ""Classifies the users into four categories based on
different Q&A behavior: Knowledge Seeker, Knowledge Contributor, Mixed Behavior User, Browser.""
-  Knowledge seeker and knowledge contributors are well-known categories reported in prev. literature.  However, I am not sure how the authors differentiate between the browser and the inactive users since no browsing data is analyzed. More confusing is the mixed behavior user: wouldn't it be more intuitive to consider a user with more answers than questions to be ""knowledge contributor""? This classification scheme artificially boosts the performance score of one class: the mixed behavior user because from table 3 it is clear that they have higher content contribution score than even the ""knowledge contributor"" class. 

3. ""We constructed a user performance quality assessment indicator system, using entropy method to calculate the weight and find out the user's comprehensive performance quality score, we analyzed the scores based on the user-layer.""
- I consider this to be the most important contribution of the paper, but as mentioned before, it is not very well explained. Also, is the goal here to say which type of user is better than the other? That is obviously going to be the mixed behavior class because they are more active, therefore have more content contribution. 

Considering these issues I would suggest rejection.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I would suggest a rejection, or, in the best case acceptance as a short paper.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,3/9/2018,1:53,no
137,40,94,Mixxxx Dobrexxxxxxxxxxx,4,"(OVERALL EVALUATION) This paper addresses a topic which is relevant to JCDL. However it does not follow the ACM template and needs a very serious language revisions - refining the use of terms too (e.g. refining phrases like ""data crawled by Python language"" and ""This paper uses Python to crawl...""). There are some missing components from the legends of the diagrams. 

Besides that, the paper needs to introduce better how the research questions had been selected exactly - and the first one needs to be revised (they are called ""problems"" in the paper). In addition, there is a lot of context from research done in Asia, this does not represent a balanced view on the work on user assessment in other continents.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/21/2018,20:18,no
139,41,190,Nabxxxx Jaxxx,1,"(OVERALL EVALUATION) This paper is relevant to one of the JCDL topics: Personal Digital Information Management. It gives a good overview of how personal digital information management tools can be used but it lacks the comparison of the tools in depth.There are many tools, which the author listed but did not talk about how and when those could be effective. It is focused on only Evernote as digital information management tool. The abstract says the paper is about how and to what extent digital tools can be used. Although the author has described the Evernote tool in detail, the aspect of how it has made a significant improvement in data management and how it has transformed the way data is managed is not well addressed. The methodology used is case studies research which is all based on one tool, Evernote, and does not address the user?€?s experience with other tools, as suggested in the topic.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/23/2018,15:02,no
140,41,317,Brxxxx Rexx,2,"(OVERALL EVALUATION) This paper is not appropriate for JCDL because it is actually not a research paper. The author does not begin with a research problem, present a research question, or even describe which methodology was used to conduct the research. It is more a quick summary of the usefulness of PDIMT, as well as an opinion piece. 

The paper is inconsistent. It begins with a table comparing several PDIMT tools, their features, and prices; however it proceeds to only cover Evernote. Much of the paper is dedicated to describing various Evernote features, and these descriptions at times seem to have been lifted directly from the Evernote website. It quotes glowing reviews of Evernote and ends with the suspicious sentence: ""Probably it is time to pick up few PDIMT tools especially Evernote and start using it."" The paper shows no balanced or objective approaches, and at times sound like a shill for a commercial product. 

In addition to the afore-mentioned issues, this paper is poorly written. It contains many statements that are either ungrammatical or simply awkwardly phrased. Some examples:

""but they are finding difficult"" should be ""but they are finding it difficult""
""Evernote is even search for handwritten words"" should be ""Evernote even searches for handwritten words""

I do not recommend this paper for inclusion in JCDL.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2/13/2018,15:46,no
141,41,226,Clauxxxxxxx Kxx,3,"(OVERALL EVALUATION) The paper aims at fostering to use a personal management system for digital library 
search tasks, but it is a commercial for Evernote. It does not motivate 
why a user should use such a system nor does it include any research question.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2/16/2018,6:16,no
145,42,377,Ricxxxx Toxxx,1,"(OVERALL EVALUATION) This paper introduces research initiatives related to the creation of an Extract-Transform-Load (ETL) approach aiming to support the implementation of complex services (e.g., content-based search) for iconographic image digital libraries. This is an important subject, in general, not yet extensively explored by the digital library community.

One limitation of the study relies on presentation aspects. The take-home message is hard to follow. For example, the challenges in the area are discussed in the introduction but their description is not clear. Some key concepts are not introduced clearly. One example refers to ?€?encyclopedic image database?€? and ?€?deep learning?€?. Proofreading is also required (double-check, for example subject-verb agreements and word spelling).

Another issue refers to validation aspects. Both the experimental protocol and achieved results concerning the content-based image metadata extraction is not clearly described. For example, it is not clear how the ground truth was defined. The use of the IBM recognition system is also not described properly. It is not clear how the deep-learning-based solution works. As this is one of the claimed positive aspects of the study, more details should be provided. Finally, there is lack of discussion of learned lessons (e.g., good practices) that could be of interest of those handling similar collections.

Some other issues:

1.	Some overview about achieved results could be included in the abstract.
2.	Provide the meaning of ?€?SRU?€?.
3.	Provide a reference to ?€?Tesseract?€?.
4.	Some overview about the Gallica system is missing.
5.	Some overview about the BaseX database is missing.


In summary, this paper lacks organisation. Presentation is not clear and contributions are not properly discussed.
However, if authors address these presentation issues, the paper may become acceptable.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper lacks organisation. Presentation is not clear and contributions are not properly discussed.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2/22/2018,9:13,no
146,42,239,Jonxxxxx Lexxx,2,"(OVERALL EVALUATION) This submission presents an interesting approach in the domain of image retrieval, as applied to historical image collections. The paper serves as an interesting case study that demonstrates the applicability of several very well known, existing deep learning techniques to CBIR. 

relevance to JCDL: This paper provides little to no new theoretical contributions to the JCDL community. However, it demonstrates a generally useful integration of deep learning with CBIR.

novelty/originality: The submission makes use of existing techniques and APIs, especially in the deep learning domain. While the techniques do not appear to be novel, their application in this case study and type of collection is of interest to JCDL. 

methodology: The approach utilizes typical CBIR techniques (segmentation, extracting metadata, extracting content, etc.)

assessment/evaluation/comparison: The application of recognition techniques (from the deep learning community) is interesting, as is the limitations of applying these approaches (which are largely trained on contemporary collections) to historical collections. 

style/quality of writing: The submission is passable, but requires another pass through the document for editoral corrections.
	e.g., punctuation: The system returns pairs of estimated ?€?class/confidence?€?

replicability: Certainly, the directness of the practical approach, minimal new theoretical contributions, the listed APIs/software, and listed collections led to much better than typical replicability. 

adequacy of references: The CBIR and DL papers provide reasonable support for the approach.","Overall evaluation: 1
Reviewer's confidence: 2
Recommend for best paper: no",1,,,,,2/16/2018,6:53,no
147,42,418,Xixx Yxx,3,"(OVERALL EVALUATION) The author proposed a way to unify access to all illustrations in an encyclopedic digital collection. In order to do this, the author combine visual features from existing API (IBM Visual Perception) and text to improve the performance. Evaluation on a test set is conducted and the mixed MD produced best performance.

The novelty of the method is limited, since the author combine existing vision APIs.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/16/2018,19:42,no
148,42,173,Mauxxxx Henxxxxx,4,"(OVERALL EVALUATION) This short paper is certainly within the interests of the JCDL community and is well written, albeit with some punctuation errors.

While it does not add a great deal of new theoretical knowledge, it is an interesting demonstration of the challenges and potential solutions of deep learning of image description and application.

I would have liked some details of possible new work the authors will do on the combining of metadata housed in documentary silos, as noted in their introduction - perhaps this could be the future of a long paper.

A minor point, acronyms need to be spelt out in first use, e.g. GAGA and CBIR","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/23/2018,23:10,no
149,43,180,Nixx Hoxxxx,1,"(OVERALL EVALUATION) This paper addresses practical issues with digital libraries related to perceptions of Linked Data and the Semantic Web among survey respondents in libraries, archives, and museums worldwide. Linked Data and the Semantic Web are still emerging technologies gaining adoption in the digital library community and this study captures professionals' desires to participate in Linked Data efforts at their institutions but regularly encounter barriers due to lack of usable tools, infrastructure, and training.

This paper used a survey to collect data- it would be nice to attach that survey to this paper so I could see what was asked precisely. A footnote on page 3 indicates it would be ""made publicaly [sic] available' in the Trinity College institutional repository but I can't find it in there. The approach for gathering this information was commonly used and two similar linked data survey studies were described in the literature review.

I thought the study would benefit from more specific findings and more information about tools that the study investigated for working with linked data. I also would have appreciated more detail about how people use tools to create, share, and consume linked data to complement their findings with what people would like a future, hypothetical tool to do. The tool rating results from the CSUQ were a bit general and it would be nice to know more precisely how tools fell short when working with Linked Data rather than an overall usability score. Moreover, since this paper seeks to establish how library information professionals could create linked data for the semantic web to facilitate information sharing, it might benefit from describing how these professionals currently share information about their digital and physical collections using other services (OAI-PMH, etc.), the tools used for that, and how linked data would build upon more recent efforts to share and aggregate this metadata.

The style of writing is adequate, however a few assorted stylistic and typographical errors suggests that additional copyediting would help it. The conference is in Fort Worth, TX, not ""Forth Worth"". These things can be easily changed and would help give this paper additional polish. I thought the references and cited literature were adequate and I appreciate the in-depth discussion of similar studies surveying Linked Data perceptions in the digital library domain. This paper could do a little more to distinguish itself from those studies, which seem quite similar in how they are currently described.

I gave this paper a ""weak accept"" for the reasons stated above- it is a well-executed and timely study that needs a bit of copyediting and further explanation to have a greater impact on the broader digital library/archives/museum field.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/8/2018,21:01,no
151,43,61,Vitxxxx Casxxxx,2,"(OVERALL EVALUATION) The paper presents the results of a survey (based on 185 valid questionnaires) addressing Information Professionals (IPs) in the LAM sector (Libraries, Archives, Museums) to understand their knowledge, use and requirements about Linked Data (LD) and Semantic web (SW). 
The topic and the approach are not new and the results bring little additional knowledge in this field. However, the topics of LD and SW are clearly relevant, especially for the Library domain, and it might be worth to present the results of the survey as a short paper, to try and stir some discussions in the audience and renew the interest in these topics. 
Some editorial notes: 
- ""IPs are well positioned to play a leading role
in the development of the SW"" 
The acronym SW is defined half a page later
-""data stored in RDF format vie a SPARQL endpoint""
what is ""vie"" ?
- ""System Knowledge Organisation System (SKOS)""
The first S stands for Simple","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/16/2018,23:52,no
152,43,153,Juxxx Grxxxx,3,"(OVERALL EVALUATION) Authors describe a study of LAM perspectives on linked data tools and practice. Understanding barriers to engagement in linked data practice continues to be of interest to libraries, particularly as libraries begin to incorporate new LD tools such as VIVO into their workflows, and I appreciate the focus on LAMs and not just libraries. As authors note, prior survey-based research has been conducted in the area of linked data. Sharing a list of questions and response options would help to reinforce the original contributions mentioned on pg. 3.

In ?€?Related Work?€?, authors describe a similar survey study conducted by OCLC Research. While I know of OCLC and many in JCDL and library communities will be familiar with the organization, I would suggest spelling out the OCLC acronym. More citations to sections that talk about the OCLC Research study are necessary. The first ?€?Related Work?€? sentence cites both the original study and a follow up study and it isn?€?t clear in subsequent paragraphs whether authors are referencing insights gained from the original OCLC study or the one conducted by OCLC?€?s Yoshimura. In ?€?References?€? the citation for the Yoshimura article ends with what I believe is a reference to page ?€?6?€? but there are no page numbers in the D-Lib article. Should this be changed to ?€?1-6?€? or should the ?€?6?€? be removed? Authors shift in paragraph 5 of ?€?Related Work?€? to another study by LaPolla, but then it isn?€?t clear to me whether references to earlier studies in paragraphs 5-8 refer to the original OCLC Research study, Yoshimura?€?s study, or the LaPolla study.

In terms of methodology, authors interview two IPs with a lot of LD experience and working in a university library (possibly the same library), which contradicts the assertion made that the study ?€?takes into account the views of IPs, both with and without LD experience, and from a variety of LAM domains.?€? Authors also don?€?t provide a list of which institutions participated in the survey, how they were selected, and how (and why) ?€?Researchers and Academics?€? were selected to participate. Authors recognize that the goal to reach a variety of LAM institutions was not achieved; this may have been in part due to the approach to survey invitation (a methodology section which could be described in more detail).

In the conclusion, authors suggest the study points to the need for developing LD interlinking tools as a next step and that requirements were gathered, but I?€?m not sure what the requirements are (is it the list in section 5.6?) or which interlinking tools participants were asked to rate in Table 7 (is it the list in Table 5?). A reference in the conclusion to the requirements list would help, as well as including the list of questions and response options.

In terms of other changes recommended, authors should spell out the semantic web acronym SW when it first appears in the second ?€?Introduction?€? paragraph. ?€?Vie?€? in pg. 2 paragraph 2 should be changed to ?€?via?€?. In the last sentence in paragraph 5, ?€?of?€? should be changed to ?€?on?€?. The semicolons on page 5 that precede lists of responses should be changed to colons. In the ?€?Introduction?€? section, paragraph 6 should be moved to ?€?Results?€? and paragraph 7 to ?€?Conclusion?€?.

Overall, the paper is relevant to the JCDL community.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) It is difficult to determine with any great authority the originality of the paper because I am unable to compare the assertions made by the authors of the contributions to new knowledge (on pg. 3) to a list of survey questions (which could then be compared to the list of questions from previous studies). Authors also state explicitly (pg. 4, methodology) that they pull heavily from questions and answer options from earlier studies. There still may be much to learn from the new questions asked and from answers from different types of LAMs to the same or similar set of questions.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/17/2018,3:48,no
154,44,77,Timxxxx Cxx,1,"(OVERALL EVALUATION) This submission is well-written and strongly in scope for JCDL. However, especially given the long paper and poster about Quill presented at last year's JCDL, this submission is weak in terms of novelty / originality and at best a weak accept. The incremental work reported is really not enough for a new long paper. Sections 1, 2, and 3 (and multiple of the figures) largely reiterate and recap the 2017 jcdl paper and poster and do so in greater depth than necessary for a follow-up. The rest of the submission focuses on an ad hoc assessment of Quill over the last year and on changes made in response to this assessment, but the assessment reported is immature, incomplete and weak methodologically (not unrelated). While section 4 provides some new insights gleaned from observations of users wanting to use Quill to better understand 19th century negotiations, the discussion provides insufficient context about these users and the conventions or other negotiations they were seeking to analyze (the introduction mentions Utah's constitutional convention, but this use case is never mentioned again in the body of the paper). This is a problem that recurs throughout the remaining sections of the paper. Clearly the workshops were a source of inspiration for the updates made to Quill over the last year, but the observations reported from the workshop seem anecdotal and potentially random. It does not appear that user feedback was gathered in a structured manner, such that feedback could have been coded and analyzed systematically. At the very least it would be better to see the observations reported as case studies. Better yet would be if the workshops had led to formal surveys, focus groups or user studies, the results of which could then have been coded, analyzed and reported systematically using established methodologies. The overall impression is that the testing, iterative improvements, and gathering of user feedback is very much a work in progress, more suitable for reporting at this stage in a short paper. This impression is reinforced by how sparsely some of the 'enhancements' of Quill are reported, e.g, the implementation of 'quick jump' codes instead of reliance on standard URLs as a solution to a perceived weakness in using URLs for certain classroom settings.  This is reported in the paper as an assertion with no documentation as to the magnitude of problem, nor follow-up as to whether quick jumps actually solve the problem. The intuitions and changes to Quill discussed in sections 5, 6, and 7 are certainly plausible, but in a full paper for JCDL further evidence of confirmation is the expectation. A plausible, better (in my opinion) short paper could be distilled out of this submission (as a follow-up to last year's long paper and poster), but in this reviewer's opinion there may not be enough here to warrant acceptance as a full paper for this year's conference.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,2/16/2018,23:31,no
158,44,4,Juxx Abxx,2,"(OVERALL EVALUATION) The paper presents an update on the Quill project that was presented at JCDL 2017. The authors present the results of user studies they conducted during workshops with researchers and educators, potential users of the collection. However what is missing from the paper is any discussion on how the data was gathered during the workshops or how it was analyzed. This is definitely a weakness of the paper. While the authors do describe in detail the modifications made to the system to accommodate the findings from the studies and how said modifications enable users to effectively use the system, there is no connection between what they learned from the user study data and how it led to specific modifications of the system.

The paper is definitely within the scope of JCDL and will be of interest to the community. It is not necessarily a novel study nor does it present a novel approach but the findings are useful to those employing user-centered design or information representation design in development of digital libraries. The authors detail the issues that plague many digital libraries: domain expertise needs and development of metadata for this unique collections and this perhaps would make for an interesting panel presentation.

The quality of the writing is excellent with no grammatical or typographical errors to impeded the reading. The screen shots were relevant and useful to illustrate the modifications. I would suggest moving Figure 4 closer to the discussion about the figure if possible to do so without disrupting the rest of the paper.

As suggested above this paper could be re-worked as a panel about user studies and how they are used in system design in this and other digital humanities projects.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I have edited my review as requested at the program committee meeting last Friday. Please let me know if it needs further revisions.
J.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/26/2018,22:06,no
160,44,260,Roxxxx Hx,3,"(OVERALL EVALUATION) This paper is a continuation of the JCDL award winning Quill paper contribution from JCDL 2017. It focuses on the following extensions of the Quill  platform to assist researchers to collaborate with Quill on the study of a wider range of negotiated discourse material. This will further enable use of Quill in the classroom and in a wider range of academic pursuits that can utilize third-party digital archives as source material for Quill analysis. Furthermore the articles discusses community development of the Quill platform through the creation of the Negotiated Text Network.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Very effective non-technical paper that further refines the Quill framework and offers opportunity for expanded Quill community building. This should allow for improved overall functionality of the platform along with opportunities for sustainability of the application among the negotiated texts research community. I suggest we try to get them in as a poster to discuss their progress.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/21/2018,23:49,no
161,44,99,Jx Stexxxx,4,"(OVERALL EVALUATION) This is a follow-on paper from the Quill project that was honoured at JCDL 2017 in Toronto. Like the earlier paper, it is very well written and clearly presented. It also continues to represent a project very much worthy of our attention as digital library developers/researcher and digital humanities scholars. Notwithstanding my generally strongly positive appreciation of the paper, I am struggling to give the paper more than a weak accept this time. For me, the issues at play include:

1. The paper is a bit incremental (i.e., not quite enough has really evolved from last year's great paper).

2. The nominal ""user feedback"" that the team uses as its justification for design changes and system decisions is reported upon in an anecdotal manner. This anecdotal presentation makes very it difficult for this reviewer to be convinced of the connection between the user input and the implementations justified in the paper by their input. Simply put, there is not enough information about the users and their feedback given in the paper to make any kind of judgment.  

Again, I am strongly positive on the project overall. I would suggest that poster presentation (or maybe short paper) might be the better mode of presentation this year.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,2/18/2018,2:38,no
162,45,53,Roxxx Buxx,1,"(OVERALL EVALUATION) This paper describes a content-based book recommendation system using neural embedding of authorship as a data source.

I think the results are interesting and the paper is well-written. I have the following concerns that the authors should address:

1. Full-content-based book recommendation is only possible when the content is available. That implied books that are out of copyright, as is the case with Project Gutenberg texts. However, that severely limits the set of volumes that can be considered. This is the primary reason for the fact that ""only a few"" book recommenders use this information. This point should be noted in the introduction because it is a major limitation of the method -- it requires resources that are unavailable for the most part -- although the HathiTrust Digital Library is trying to get around this problem.

2. There is an implicit assumption that authors are consistent in their stylistic aims and genres. You are training the network to map all of the works of a given other to a single output. This assumption seems quite weak. Consider someone like (the late) Ursula LeGuin, who wrote essays, poetry, science fiction, fantasy, and other works. Someone who likes ""The Left Hand of Darkness"" might not be at all interested in ""The Wizard of Earthsea"". It might be true of the selective digitization within the Gutenberg Project, which is more likely to select the most popular / famous works of authors. It also means that this method will work best on authors who write only a single kind of work. Such limitations and assumptions need to be acknowledged in the paper.

3. As the authors note in Section 5, precision and recall are not normalized for the length of the user's profile. A better metric for ranking tasks is NDCG@10, which has built-in normalization and can be compared across users and across systems. I would recommend that the authors use this instead and drop the recall/precision metrics.

4. The authors include only content-based baselines in their experiments. This is good at showing the advantages of the author-oriented representation, but it does not convince this reader that the proposed system is really an improvement on the state-of-the-art. Perhaps a collaborative recommender would work better and all of this content processing is unnecessary. A collaborative method has the additional advantage of working on any book and not needing full-text access.

5. A obvious direction to take this work is to consider hybrid models in which authorship is consider one signal to be combined with others. For example, a separate classifier could be trained for identifying book genres such as those found in the NoveList metadata. Recommendation based on this data could be be combined with the authorship info. In addition, a recommendation hybrid would be a natural way to incorporate the collaborative data that this system ignores. The authors do not seem to be aware of this possibility.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/7/2018,1:41,no
164,45,291,Denxxxxx Pexxxx,2,"(OVERALL EVALUATION) The paper proposes a system that recommends books based on their authors?€? writing style. It focuses on the analysis of the textual content of books to improve their recommendations. The system transfers information learned by an authorship identification (AuthId) classifier to a book recommendation module by using a neural network.

The subject is relevant to digital libraries. The paper is well written and with a good bibliographical review.

An issue not evaluated in the work was the efficiency of the approach. Processing the whole text of books using neural networks may not be efficient, and infeasible to big data.
Is it really necessary to analyze the whole text of books to identify the authors' writing style?
Maybe less than 100,000 words are sufficient for identification. It would be interesting to add experiments with fewer words per book in order to verify this assumption.

The baselines used for comparison are general information retrieval approaches. It would be interesting also to make a comparison of the proposed approach with other specific baselines to recommend books found in the literature. This would be important to ensure that writing style recommendation is actually the best recommendation strategy.

According to Figure 2, although the proposed approach achieves better results than the baselines, the results for precision and recall are very low. Does this occur in other works of the literature? Or is it a feature of the dataset and the way the experiments were done?
Although the authors have done a good analysis of the results, they should better justify the low values.

The Conclusions section is very simple. There is plenty of room for increments.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/21/2018,14:25,no
165,45,246,Mirxxxx Mx,3,"(OVERALL EVALUATION) This paper introduces a book recommendation method based on the content of the book. The paper is very short (with a 10 page limit, it has practically just six). Overall, it is contribution is not novel enough, as explained next. 

One motivation for the work is ""It is quite suprising [sic], then, that the analysis of the textual content of books to improve their recommendations is still very limited."" Well, truly analyzing the content of a whole book in order to improve recommendations by just a fraction may not be worthy at all, given that good recommendation results are achieved with much simpler, quicker, practical methods. Likewise, a founding stone of the method seems flawed: Given the text of a book as input, the AuthId classifier predicts its author""; why does a recommendation algorithm needs to predict the book's author if such an information is usually available at the book's metadata?

The main part of the solution is an adaptation of an existing one (published at reference #28). Then, the recommendation function is actually Support Vector Regression. Therefore, the novelty of the work is very narrow. Among 39 references, there is only one to JCDL (year 2012), which makes me wonder if the audience of JCDL would be actually interested in the topic of the paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper has practically six pages (the 7th has only two references, which would fit in 6 pages by excluding unnecessary urls within the references). I think this should have been submitted to the short papers track.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/16/2018,12:28,no
166,46,86,Pexxx Daxx,1,"(OVERALL EVALUATION) This paper presents findings from a study of the role and significance of scientific meetings (e.g. conferences, workshops) in a range of physical and computer science disciplines. The study comprised analysis of a range of ?€?metadata?€? (e.g. h5-index, acceptance rates, whether the meeting has been held regularly without interruption, the extent to which a meeting?€?s geographic location by edition) about these meetings to arrive at a range of conclusions, for instance, prestigious conferences typically have a high level of continuity. The study involved collection of an impressive range of data, which have been carefully processed and analyzed. 

However, the Discussion and Conclusions are unsatisfying and underdeveloped. The main conclusions are presented as a series of bullet points, with a sentence for each. The authors have presented a very large range of data in their findings: for a paper of this length, the authors would be better advised to focus on a subset of their data so that they can develop their conclusions in depth. 

Additionally, I am not convinced that the selected measures necessarily reflect the prestige of meetings. For instance, I can think of examples of prestigious conferences in other scientific disciplines where acceptance rates are high, or where the conference is held in the same geographic venue annually (e.g. the American Geophysical Union conference is a leading geosciences conference, but has been held for over 40 years in San Francisco). It would be interesting for the authors to critically reflect on whether the selected measures (e.g. h-5 index, acceptance rate, geographic variation of venue) truly reflect the prestige or significance of a meeting.

I also believe this paper would be more appropriate for a conference or journal about scholarly communications: the topic of the paper is not about digital libraries, and the authors do not attempt to link their findings to digital libraries. 

Finally, a few other points that could be addressed in a revision:
?€?	The abstract doesn?€?t do justice to the paper, and would benefit from more specificity about the paper?€?s main findings;
?€?	A clear definition of what is meant by ?€?metadata of events?€? is needed much earlier on in the paper, for instance in the final paragraph of the Introduction;
?€?	?€?Data gathering is the process of collecting data from a variety of sources in an objective and unbiased manner.?€? (p. 2) Is this sentence really necessary?","Overall evaluation: -2
Reviewer's confidence: 2
Recommend for best paper: no",-2,,,,,2/16/2018,21:38,no
167,46,312,Andxxxx Raxxx,2,"(OVERALL EVALUATION) The paper reports on an extension of a preceding study (published in TPDL2017) to analyze publication venue metrics, adding math, physics and engineering to the computer science venues already studied. While the numbers reported and the study design are basically fine, it is unclear what the lessons learned from the numbers reported are ?€? or which of those actually come as a surprise or novel knowledge. Specifically, it is unclear what the motivation for the study is in the first place. The introduction states that the goal of the study is to ?€?gain more insights on the significance of the mentioned problems in order to ultimately devise novel means for scholarly communication.?€? Unfortunately, the remainder of the paper never touches on any of these novel means of scholarly communication, nor how they might be linked to the numbers reported. The end of the introduction finally lists two research questions (how important are events for scholarly communication, and what makes an event high-ranked in a community), but the conclusions fail to give solid answers beyond the fact that some disciplines rely more on conferences than on journals. The fact that high continuity leads to higher appreciation of an event also does not really come as a surprise. 
Having these seemingly well-known facts supported by solid numbers may, however, constitute valuable knowledge by some.
Some of the insights gained may be influenced also by the design decision (such as using an h5 index, i.e. impact over 5 years ?€? the time span and value life cycle of references is known to be highly domain-specific, with humanities having way longer citation cycles than the fast-paced computer sciences. Additionally (and maybe most surprisingly from a CS perspective) is the fact that other disciplines provide less numeric information on their publication venues, rendering many of the detailed analyses that were performed 8previously) on the CS domain inapplicable for the new domains added.
The paper might benefit from more clarity in the feature description (e.g. geographic spread is stated to be calculated based on continent, state or city ?€? would it not make sense to compute it across all three? Similarly, how is time variance represented? It is not stated in the methods part of the paper ?€? experiment descriptions seem to hint at quarterly granularity, noting that shifts between neighboring quarters should not be considered shifts at all. Why not measure variance / use days of shift around the median?
The geographic analysis states that roughly 50% of all events were held in the US. While this is obviously a true statement it ignores size aspects: the US is by far the biggest country in the context of the analysis. 

Minor aspects:
- the legend of fig 3 is way to small rendering it illegible.
- equations should be labeled

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Reporting some interesting numbers confirming intuitively well-known facts with few lessons learned.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2/17/2018,10:37,no
168,46,231,Albxxxx Laxxxx,3,"(OVERALL EVALUATION) This paper presents an interesting analysis of the characteristics of scientific events from four distinct areas: Computer Science, Physics, Engineering and Mathematics. The paper is in general well organized and written, and presents results that are of interest of the JCDL community. However, a preliminary version of this paper, addressing only Computer Science events, was presented at TPDL 2017, which is clearly acknowledged by the authors. Despite that, the additional analyses of the CS events added to this paper do not expand much the previous results reported in the TPDL 2017 paper, which means that only the overall analysis of the non-CS events (particularly Sect. 5.1) can be seen as new. However, this analysis is not deep enough and does not address several aspects that differentiate CS and non-CS events (e.g., the length of the papers).  Thus, in this reviewer?€?s opinion this paper does not present enough additional material in comparison with its TPDL version that justifies its acceptance for presentation at JCDL 2018. In view of that the authors should expand their analysis of the non-CS events and prepare a more comprehensive version of the current manuscript for being submitted to a relevant journal of the area such as the International Journal on Digital Libraries.

Positive Aspects
- The paper addresses a topic that is relevant to JCDL.
- The paper is well structured and written.
- Reported results are very interesting.

Negative Aspects
- As acknowledged by the authors, this paper is an extended version of a paper recently presented at TPDL 2017.
- Extensions added to the paper address events from other areas (Physics, Engineering and Mathematics), but are not detailed enough to reveal new relevant findings. 
- Particularly, the analysis of the non-CS events added to the paper are superficial and do not go into specific aspects that differentiate these events from the CS ones (such a kind of analysis would be a relevant contribution). 

Specific Comments

1. Some claims in Sect 1 (Introduction) should be supported by specific references, for instance when you mention questions regarding proliferation of scientific literature and reproducibility, as well as when you mention the importance of events to the CS area.

2. When mentioning OpenResearch in Sect 4.1, you comment on the use of SPARQL. As a first impression, this comment seemed out of the scope in that paragraph, but having looked at your TPDL paper it seems that some facilities for more complex queries in OpenResearch requires the use of SPARQL. Thus, this must become clear when discussing this data source.

3. Table 3 is not that relevant. Name unification in this work is not an issue, so this space could have been used to address more relevant aspects of your analysis.

4. The analysis of the non-CS events (Sect. 5.1) covers much less aspects than the analysis of the CS ones. This is quite comprehensible since obtaining data about CS events is much easier. Despite that, you should have tried to present some specific results that covered both CS and non-CS events, before going deeper into the analysis of CS events. For example, when comparing the average STJ indicator for CS events and non-CS events, nothing has been said about the fact that outside the CS area event papers are rarely cited (particularly in PHY and MATH areas) and, for this reason, their STJ indicators cannot be compared with those of CS events. Besides, you have commented on the submission and acceptance rates for CS events, but have not commented on this same issue for non-CS events. This seems a relevant issue when discussing the importance of event papers for the CS area.

5. Graphs in Fig. 3 and 6 are too small and, therefore, very difficult to read.

6. As a final comment, the title of the paper gives the wrong impression that the reported analysis equally covers all four scientific areas: CS, PHY, ENG and MATH. Since such an analysis is not possible, due to the lack of data to equally cover different aspects of these areas, the results should be reported in two distinct sections: one addressing a deeper comparative analysis of the four areas covering common issues and one presenting more specific results covering only the CS area.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper is an extended version of a paper presented at TPDL 2017 (this is clearly acknowledged by the authors). Although the TPDL 2017 paper only addressed CS events, this JCDL version does not present substantial new results with respect to the CS events, basically including a general comparison involving the non-CS events. A more elaborated version of this paper, without page limit restrictions, could be submitted to IJDL.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/17/2018,19:21,no
169,47,428,Maxx ????xxx,1,"(OVERALL EVALUATION) 'Publish or perish' is the reality we are all facing. Any study of the review process is therefore very interesting for a broad audience. 
In this submission the authors use the data from two journals in physics and compare the outcomes of two approaches in reviewing: single reviewer and multiple review process. In addition to the reviews themselves, other data sources were used, such as citation data. Several additional studies were performed, focusing on the performance of different reviewers and characteristics of reviewer groups. The results are presented in several figures.

The problem I see is that the authors are building their research on questionable assumptions. For example they do not take into account a very probable scenario that editors choose one reviewer for papers they consider of good quality as opposed to the papers they find more questionable. The authors also take the citation count as measure of quality. While citations are a proof of visibility, absolute numbers should not be compared directly. We all know that papers on very specialised topics do not receive many citations, as opposed to methodological papers, for example. Finally, even in a set of higly cited papers some will be cited more - not proving that others are of lower quality. 
There is also no need for reviews to be of comparable length or style, even if they convey similar opinions. Reviewers are invited to review papers based on their particular expertise and availability and the number of reviews does not necesarily reflect the quality of the reviewer. Reviewer groups do not have to be in complete agreement - an editor may choose reviewers with different expertise in order to receive an analysis of different aspects. The authors even assume that a reviewer group has to be compatible, similar to collaborative learning - there is absolutely no need for that; each reviewer needs to be competent, ethical and motivated on his/her own.
I am therefore not surprised that contradictory results were obtained - maybe it was the right answers to wrong questions.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/12/2018,14:14,no
171,47,399,Mixx Wrxxx,2,"(OVERALL EVALUATION) The core of the paper is reporting a technique to potentially help editors assemble a good set of reviewers for a submission based on reviewers?€? past review history. They utilize a genetic algorithm to perform analysis and present possible sets of reviewers based on use of GA?€?s in putting together teams in other fields.  They have access to the corpus of reviews of both JHEP and JSTAT, two leading journals in physics. Using these corpus, they do analysis of reviews and accept/reject results for both single reviews and multi-reviewed cases (apparently both journals do have single-review submissions), and use this to build a hypothesis for their algorithm. They use part of the corpus for training, and test over the larger corpus. The goal is to select review teams to reduce the number of discordant sets of reviewers (i.e. review ends with no resolution on disagreements) after observing what they believe are issues of some less impactful papers being accepted, and some more impactful being rejected (but then published elsewhere) as measured in long term citation count. 

I do wonder about the authors synonymous use of high quality with high impact through use of long term citation count - in my own thinking, it?€?s the issue of just what a citation count really means even though we use this as the core measure of long term impact.  In thinking this, I wonder if the approach to reducing discordance in reviews is necessarily the ultimate goal as doesn?€?t discordance at times highlight where there may be still unsettled approaches?  Yet striving to get consensus is part of reviewing, and helping with that is a useful goal.  While the paper elicited this tension for me, and it may be interesting to hear how the authors think about this, I don?€?t think it undermines their current work.

This work builds on their previous work - one referenced from the ACM conference on Information and Knowledge Management, and another, surprisingly not referenced in related work, from last year?€?s JCDL ?€?Influence of Reviewer Interaction Network on Long-term Citations: A Case Study of the Scientific Peer-Review System of the Journal of High Energy Physics?€? which looked at a variety of features that may impact acceptance/rejection, with particular attention to the reviewer network, and just focused on JHEP.  I would like to see this reflected in related work.

It would also help to have some discussion of how JHEP interacts with ArXive with respect to papers that are considered for JHEP. This may help in understanding the review process and the decisions the authors made in approaching this work. That will allow readers to contrast with other approaches.

Section 5.2, part way through paragraph (LL, MM, HH, LH, MH, LH) should read (LL, MM, HH, LH, MH, LM).

Section 6, last paragraph before section 6.1, state ?€?we argue that assigning multiple referees to a submission is similar to forming compatible referee groups?€? - that ?€?forming compatible referee groups?€? would seem to be wrong as the comparison was to a collaborative learning setting, so what was the desired collaborative learning group you?€?re comparing to?","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/19/2018,15:02,no
173,47,86,Pexxx Daxx,3,"(OVERALL EVALUATION) This paper presents findings from a study of peer-review practices in two physics journals, in particular focusing on relationships between referees?€? evaluations of articles and these articles?€? subsequent success (measured through number of citations). The study finds that accepted articles that were reviewed by a single referee were subsequently more highly-cited than accepted articles reviewed by multiple referees, and that articles where there was a higher level of discordance between referees tend to have lower citation rates. Based on this study, the paper then outlines and evaluates an approach for allocating sets of reviewers to articles to improve review processes. 

While the dataset used in the analysis is fairly large (approx. 36,000 papers), it is unclear whether the findings (based only on two journals in a highly-specialized domain of physics) are generalizable across a broad range of disciplines. 

There are a few other points that the authors could clarify in a revision:
?€?	What is meant by the term ?€?compatible referee groups?€? (p. 7 and elsewhere)?;
?€?	What basis do the authors have for assuming that referees with similar opinions of an article will tend to produce reviews of similar length (p. 4)? Surely there are many other factors that affect the length of a review?;
?€?	When a referee is classified as anomalous, how many reviews must a referee have performed to be classified with any degree of significance? Do referees, in reality, actually perform sufficient reviews such that the system of allocating referees to articles is practical?

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I've revised the review to remove the comment about the paper not being relevant, and have amended my score accordingly","Overall evaluation: -1
Reviewer's confidence: 2
Recommend for best paper: no",-1,,,,,2/24/2018,18:20,no
174,47,406,Jixx W,4,"(OVERALL EVALUATION) In this paper, the authors observe that in a review system papers with single reviewers seem to get more citations than papers that are reviewed by multiple reviewers. They then build a framework using GA to kind of predict what types of reviewers should be used. 

However, the authors do not investigate the reasons why a single author was assigned. In fact, there could be lots of legitimate reasons that the editor made the choice of assigning only 1 reviewer rather than multiple. 

Another strong assumption is to use citations as a measure of impact. This has been known to cause flaws because citations accumulate. The number of citations depends on the total number of years and the popularity of the particular field. 

But this topic is in general interesting. Balancing strengths and weaknesses, I voted it as a borderline paper.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/24/2018,20:15,no
175,48,70,Sauxxxx Chaxxxxxxx,1,"(OVERALL EVALUATION) This paper introduces an algorithm named xFactor to generate aspects out of annotated documents. Using this technique, the documents can be modeled as combination of aspects of different kind such as temporal, geographical and entity based. The aspect generation and factor functions are very novel. 

The authors, in their research and literature review, did cover a lot of breadth in the area in terms of capturing the different aspects of the problem.

From the experimentation perspective, it would have been nice to show the effectiveness of the technique described in the paper on actual document retrieval and compare it with the state of the art methods. As a reader, I would want to know whether using these techniques would provide any improvement in retrieving documents, given a query. Some experiments on that front would tell the reader where this technique stands in terms of document retrieval performance.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The paper felt very dense in terms of the complexity of the content presented. The ideas expressed were not easy to grasp in the first reading. It took me multiple readings through various sections to understand the work.

Also, as mentioned in the main review, I want to know how the techniques discussed in this paper would help in document retrieval. Though the paper does talk about retrieving aspects in annotated archives, I want to know whether I can use this technique for actual document retrieval.","Overall evaluation: 0
Reviewer's confidence: 2
Recommend for best paper: no",0,,,,,2/14/2018,4:00,no
176,48,246,Mirxxxx Mx,2,"(OVERALL EVALUATION) This paper the xFactor as an algorithm to  automatically generate semantic aspects for resolving ambiguous queries over born digital files. It also has a second contribution in the form of a testbed with more than 5000 aspects assimilated from Wikipedia (kudos to the authors for providing it). The paper uses Olympic terms as example, which is both time effective (as the Winter games are occurring right now) and timeless (as many people recognize the names and cities involved in the games). 

The problem is not novel (solving queries is necessary since DL conception) but its solution is very interesting and relevant to JCDL. The paper also provides good examples that make the methodology very clear. The solution is tested over two very different datasets: news and webpages. The ground truth was built from scratch (based on wikipedia) and will be made publicly available (as it is not replicable) once the paper is published.

As for improvements needed, I didn't understand the correctness/novelty association with precision/recall, having the latter as a ""probabilistic interpretation"" of the former. The results show precision/recall of 0.134, 0.264, etc; aren't those values way too low? There is improvement over the compared methods (sure), but still... Related work is way too long. I mean, with so much work done, shouldn't the experimental evaluation be more complete by considering more existing methods in the comparison?

Some style suggestions. Webpages are better cited as footnotes instead of references (as the latter should be for more scientific work). Also, references should all follow a pattern; most of conferences are cited in abbreviated for, so double check that all are like that (#32). Likewise, proofreading may be necessary mostly to improve word diversity (e.g., ""the user about quick facts about"", three close ""there exist/s"" in the introduction) and simplify some parts (""this is due to the fact that the factors"" -> this happens because the factors) . For Equation 5, there should be a \noindent before the following ""where""; same in the similarity computation.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/16/2018,11:55,no
177,48,277,Wolxxxxx Nexx,3,"(OVERALL EVALUATION) While the paper addresses a relevant task, it is written in an overly formalistic way, without clearly spelling out advantages and usage scenarios of the proposed approach, and without discussing possible limitations (other than providing numbers). I think the paper is still worthwile to publish, but to increase its impact, the authors should definitively add some more examples and discussion, otherwise the results achieved in this work will not be taken up by others.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,2/18/2018,19:39,no
178,48,354,Vexxxx Srixxxxxx,4,"(OVERALL EVALUATION) The idea of extracting semantic information from text documents and using them for retrieval is certainly well known. The authors attempt to add a new dimension to this approach (via 'aspects'). My concern is regarding the evaluation - one of the baselines chosen by the authors (BM25 based) to evaluate their xFactor algorithm is a fairly weak one in this context. The other baselines are based on LDA, although it is not clear why it needs to be so. Authors also don't discuss any plans for user studies, which one would expect would be vital for this particular task. There's also limited discussion in terms of results and lessons learned (for ex., why is this better than vanilla relevance feedback based approaches, beyond improvements in 'correctness' and 'novelty'). The authors would do well to provide insights along these lines as well.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The paper is methodologically sound, and provides some useful insights. If others think the authors have evaluated their methods reasonably well, I don't mind accepting this paper.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/24/2018,17:28,no
179,49,148,Sujxxxx Dxx,1,(OVERALL EVALUATION) The workshop topics are of continuing interest to the JCDL community and the workshop has been conducted previously at JCDL. The authors are well-established researchers on the proposed topics and the workshop can be expected to be successful based on previous meetings.,"Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,1/25/2018,5:37,no
180,49,194,Adxx Jaxxx,2,"(OVERALL EVALUATION) This is ongoing workshop on general topics related to web archiving and DLs.

The team is well-experienced and have run the workshop with good success for several years already. I think JCDL2018 should keep tradition of having a smaller dedicated event to web archiving, especially, giving growing interest in related topics, and I would be glad to see WADL be organized again in 2018.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,1/28/2018,10:36,no
181,52,74,Hunxxxxxxx Cxx,1,"(OVERALL EVALUATION) This workshop attempts to gather the innovative approaches to exploring and mining the medical and biomedical text information, such as the related academic literature, patient data, and health records.  I think this topic deserves to be a workshop since the medical and biomedical texts have several features that are different from other texts.  As a result, it is beneficial to discuss domain-specific research issues and design domain-specific algorithms for mining such datasets.

The prospective program committee members seem mostly specialized in computer science.  I would suggest adding a couple of experts in the biomedical domain to help identify the essential needs in this field.

The list of the topics can be improved.  Some of the listed topics seem redundant.  For example, ?€?biomedical corpus?€? looks very close to ?€?biomedical digital libraries?€? to me.  Some topics seem too large.  For example, ?€?textual big data techniques?€? seems to contain the textual techniques beyond the medical and biomedical domain.

Finally, it seems that the JCDL does not request double-blind reviews.  If this is the case, it is probably unnecessary to add such a constraint for the workshop.  I would suggest most of the policy be consistent with the main conference to prevent confusion.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,1/24/2018,2:50,no
182,52,154,Juxxx Grxxxx,2,"(OVERALL EVALUATION) The proposal builds on a previous iteration of the workshop and represents the convergence of diverse medical data perspectives. The expertise of the organizers is evident, and a list of citations to related papers would help to demonstrate depth of understanding of current issues.

All of the program topics in section 2 are exciting and the curation of health and medical data is undoubtedly a critically important issue (and one that would be of interest to the JCDL community), but I have concerns that the organizers are trying to cover too much in a half-day workshop. It would make sense to either make it a full day workshop or to focus a half-day on only the topics of highest relevance to the JCDL community. Either way, I would suggest asking prospective authors to speak to how their research/practice in the areas mentioned relates (or could apply) to advances in the development of digital libraries to support the curation of health/medical data. 

Overall, the digital library connection to the proposed workshop needs to be more explicitly stated. The objective stated in overview section 1 lacks a high degree of clarity, as the areas of emphasis do not map clearly to the program topics in section 2. The connections may be there, but points of intersection need to be noted. And more emphasis should be placed on ethics/privacy/security issues, as these are not just data management concerns. 

It would also help to include specific goals and expected outcomes of the workshop, and to reference how the 2018 workshop is informed by outcomes of the 2017 workshop. Also, what is the connection to evidence synthesis and are there ways the health science library community would benefit from workshop participation or from broader workshop outcomes?

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Dr. Edward Fox is listed as a prospective program committee member and is a colleague of mine at Virginia Tech; I do not believe this presents a conflict.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,1/26/2018,19:34,no
183,52,402,Dxx W,3,"(OVERALL EVALUATION) The topic of this workshop proposal focuses on the curation of medical data which is the hot topic and related to digital libraries. The proposal is carefully written and considers some detailed issues like the review process, the estimated submissions and accept rate, the audiences, even issues about the rule of multiple contributions with one manuscript. The most important thing of the workshop is that it has good foundation. It has been held once in 2017, therefore, it should have experiences about how to organize the workshop and it has potential submissions and attendances which can help to extend the participant of JCDL conference. Finally, the organizers are from different universities of Europe and Asia, that can help to increase the international participants of JCDL conference. The organizers are experts of medical data analysis with good research basis that can guarantee the quality of submissions.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,1/28/2018,17:53,no
184,53,429,Maxx ????xxx,1,"(OVERALL EVALUATION) This (very short) paper reports on a usability study of the Saudi Digital Library (SDL). An online questionnaire, based on Oxford Digital Library survey, was designed to capture the opinions of Saudi students studying it the US. The questions, at least as apparent from the Results and Discussion section, were rather general. Exactly half of 44 participants had used SDL and 14 have never heard of it. 
User studies are extremely important to help improve the servisec offered. Yet this study, admittedly referred to as 'initial' in the conclusion, suffers from numerous problems. First the participants were not really the intended audience of SDL, which is aimed at students of Saudi universities. Students in the US, regardless of nationality, have other resources provided for them. The sample was very small and in addition the author makes hasty conclusions such as 'perceived satisfaction was high (n=18'), 'results were easy to understand (n=15'), 'about (!) 16 users reported feeling confident using SDL in English'. The author also claims that 'users were challemged when attempting to locate Arabic language materials', yet 21 reported no opinion, 5 found it easy and 6 found it difficult. Different numbers are listed in the next paragraph, though.
Unfortunately this is a poorly designed and very superficial study.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/6/2018,10:07,no
185,53,409,Irxx Xx,2,"(OVERALL EVALUATION) The study has good aims, but fails to follow through. It is a pilot study on Saudi DL, to investigate whether the DL meets the needs of Saudi students in the U.S. 

There is clearly a gap in the research methodology. The researcher uses a questionnaire to test the satisfaction of a usability study s/he conducted, but is never clear regarding the process for the usability study. The data would be skewed as 14 participants never heard of SDL, so would have never used it. Apparently, the author did not conduct a usability test. The design is not transparent or justified and the results are fairly simplistic, descriptive results which don?€?t provide an in depth explanation of the findings. 
Descriptive data reported; however, what scale was used, Likert and what was the rating? 

Some of the results could be better presented in a table or figure. 

More references would be beneficial, specifically for justification of the research design and analysis

Unfortunately, due to the simplicity of the approach, the study is replicable. But it doesn?€?t really take a sophisticated approach. It is very basic.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2/12/2018,22:59,no
186,53,100,Jx Stexxxx,3,"(OVERALL EVALUATION) This is an little paper outlining some early work on understanding the users associated with the Saudi Digital Library. As such, this paper is definitely in scope. There are a couple of worthwhile aspects of the paper, such as mechanism for reaching out to find survey responds. Notwithstanding the good aspect the paper does have several shortcomings the might prevent it from being accepted as is. 

First, the paper is only two pages. Using the full allotment of available pages would have allowed the author to provide more analysis and discussion.

Second, and related to the first point, there is very little actual information about the findings. Most of the paper is setting up the study and then it ends with a few conclusions. 

I could see this paper being accepted as poster presentation if the author could provide some more discussion and analysis of the survey data.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Very light on content but germane to the conference.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2/18/2018,2:16,no
187,54,271,Anixxxx Mukxxxxx,1,"(OVERALL EVALUATION) In this long paper the authors compare existing graph embedding and feature engineering methods, presenting combined approach for constructing co-author recommender system formulated as link prediction problem. 

The authors have conducted a quite elaborate background research however they fail to present ANY evaluation experiments.  I found it as a half-baked paper. Therefore, without comparison against state-of-the-art baselines and elaborate discussion section, I strongly recommend rejection of this paper. For instance, how does your work compare with: https://arxiv.org/abs/1505.04560 ... I can cite many other similar papers and a thorough evaluation is seriously in the lacking for this paper.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,1/27/2018,7:08,no
189,54,358,Kazxxxxx Sugxxxx,2,"(OVERALL EVALUATION) The authors propose a method for recommending co-authors or collaborators by taking this task 
as link prediction problem. The authors plan to use machine machine learning to predict new edges 
in temporal structure of co-authorship network. 

While this work is well-motivated by the current situation in the National Research University 
Higher School of Economics (HSE) as described in Section 3.3, the authors just describe the plan 
for experiments. To make this paper publishable, the authors should propose a novel approach, 
evaluate it with a relevant measure, and then discuss the obtained results. 

(1) Novelty/Originality
- The authors try to apply just regression or classification models as described in Sec 4. 
So if the authors propose a novel approach, for example, neural network-based model, this paper 
would be intersting. 


(2) Methodology
- The authors should detail more about their proposed approach not just listing features, 
similarity scores, and symmetric binary functions. 


(3) Assessment/Evaluation/Comparison
- As pointed out above, the authors need to quantitatively evaluate their proposed approach. 
It is also important to compare their proposed approach with some state-of-the-arts. 


(4) Style/Quality of Writing
This paper has some wrong expressions as follows: 

[Sec 3.1]
- infromation on ... => in*for*mation on ... 

- Figure ?? => Figure 1
(Need to specify Figure ID) 

[Sec 3.2]
- In certain cases when the author is ... => In certain cases *where* the author is ... 

- Last name and initials => *l*ast name and initials 


(5) Replicability
- The authors plan to use classical regression or classification models as described in Sec 4. 
So if the authors clearly describe their features, it is eacy to reproduce their experiments. 


(6) References
- The authors need to survey related work that is more specific to ""identifying collaborators."" 
For example, they need to cite the following papers: 

H.-H. Chen,  L. Gou, X. Zhang, and C. Lee Giles: 
""CollabSeer: A Search Engine for Collaboration Discovery"" (JCDL2011)

H. Deng, J. Han, M. R. Lyu, and I. King: 
""Modeling and Exploiting Heterogeneous Bibliographic Networks for Expertise Ranking"" (JCDL2012)

J. Tang, S. Wu, J. Sun, and Hang Su: 
""Cross-domain Collaboration Recommendation"" (KDD2012)

S. S. Rangapuram, T. B??hler, M. Hein
""Towards realistic team formation in social networks based on densest subgraphs"" (WWW2013)

S. H. Hashemi, M. Neshati, and H. Beigy: 
""Expertise Retrieval in Bibliographic Network: A Topic Dominance Learning Approach"" (CIKM2013)

M. Y. Allaho, W.-C. Lee: 
""Increasing the Responsiveness of Recommended Expert Collaborators for Online Open Projects"" (CIKM2014)","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,3/5/2018,18:26,no
191,54,174,Draxxxxxx Herxxxxxxx,3,"(OVERALL EVALUATION) This paper presents an approach for predicting and recommending co-authors based on various network-based features. The problem is formulated as link prediction task and is tackled by combining approaches ?€? author similarity measures and more traditional graph-based recommendations with network embeddings. I think this is a very interesting problem and potentially a useful solution, however, the paper seems incomplete, which is why I cannot recommend it for acceptance. In particular, in section 4 the authors mention they measure AUC, however, the results are not presented in the paper and it seems the entire results section is missing. Unfortunately, without any results, it is hard assessing the method. I have some general comments and questions regarding the remainder of the text.

First, in the introduction, the authors seem to jump around different topics, which made the text harder to follow. In the related work section the authors mention several terms without explaining them (e.g. attribute-based vs. self-organizing networks). While these terms may be well known in graph analysis, I think it may still be worth providing a brief explanation to help the reader.

There are a number of statements in the data collection section which may need more explanation. In particular, the authors state that they ?€?manually input at the personal web-page of researcher research interest list according to RSCI categorization?€?. This sounds very time consuming. For how many records was the manual processing done? 

The authors state duplicate records were merged, how was this done? Was this done based on title match, similarity, or other criteria? 

Next, the authors state that ?€?All the missing fields were omitted during computational part of filed with median over respective category of articles and authors.?€? ?€? I?€?m not sure I understand this sentence, does it say some fields were omitted if empty and some filled with median values? It would be useful to state which fields were omitted and which were replaced with median values. 

It would also be useful to provide some statistics for the author disambiguation results to give the reader a sense of how well did the disambiguation work. 

Regarding the quality metrics, it would be helpful to state which specific metrics were imported (was it JIF, eigenfactor, or other metrics?) and again provide some basic statistics for these metrics. Finally, I would appreciate if the authors could share some overview statistics of the dataset. I assume this was partially done in figure 1, however, the figure is quite difficult to read. I thought the commas are used as decimal separators instead of colons, however, in that case, some of the values don?€?t make sense to me (e.g. I would expect the number of authors to always be a natural number).

Regarding the choice of the dataset, I think it would be particularly interesting to perform the experiment on a dataset which includes authors from different institutes rather than authors from a single institute only. I would expect the authors within the institute to more likely form connections, since there is a higher likelihood that they already know each other. From this perspective I find the choice of the dataset somewhat limiting. Forming connections across institutes and maybe even across countries is much harder and is therefore a task which may much more benefit from recommendations.

Finally, the paper would greatly benefit from proofreading.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/17/2018,6:12,no
192,56,112,Schxxxxx Fx,1,"(OVERALL EVALUATION) An overall interesting and novel piece of work and approach to detect plagiarised content using a series of image similarities algorithms and computations.  The paper is structured well and written clearly with a set of accompanying evaluations. The following are some coments/questions:

Section 3.1: VroniPlag collection ?€? As this might be new to many readers, provide more information of how the information in the collection is structured, particularly the ?€?fragments?€? among dissertations that are deemed plagiarised in some form.  How did you decide which test fragments to use for your evaluation? Were these 1:1 or 1:many instances (spread across documents) in the collection.  If applicable, how was this handled by your system? How is the ?€?original?€? content defined?

Figure 3: The outlier detection reports that a final suspicious score is calculated (Section 3.11). The combination and computation of this score can be incorporated in the Figure for clarity and completeness.

Section 4: The selection of the 15 image pairs suggest a 1:1 relationship (see earlier point).  The random 4500 images might be skew in terms of the ratios of the level of disguise between and test and random set of images although this might not be a problem in view of the large number of random images. 

In Tables 1 and 2, in addition to the 4 distance calculations, should not the final suspicious score be shown in the table? Was this score being used?  If so, how?   For cases 5 and 11, which random image came out top ?€? what are the characteristics of these images?  It also appears that (manual?) tweaking of threshold values are necessary to identify the outliers. 

What are the limitations of this work, and challenges faced in making this an automated production detection system?","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/7/2018,4:06,no
193,56,401,Dxx W,2,"(OVERALL EVALUATION) This paper proposes an adaptive image-based plagiarism detection approach suitable for analyzing a wide range of image similarities. The topic is very interesting and consistent with the conference. The results indicate that the proposed adaptive image similarity assessment is an effective approach. However, the paper can be improved from the following aspects:
  1) The adaptive image-based plagiarism detection process includes several key components as shown in Figure 3 and the following subsections. It is not clear which part is the novel and proposed by the authors.
  2) In the evaluation part, only selecting 15 image pairs as the sample is too small. 
  3) ""Given the moderate size of our test collection, we assume images that were ranked at ranks beyond 10 as unsuccessful retrievals"". The authors should give some references for the unsuccessful retrieval to make 10 as more reasonable.
  4) To demonstrate the proposed approach, I suggest the authors to use other related approaches as baselines to compare. 
  5) I am wondering whether the image retrieval related metrics can be used to evaluate the retrieval results for image-based plagiarism detection.
  6) As a full paper, it should have discussion section to analyze the interesting findings and implications of this work.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/16/2018,19:52,no
196,56,103,Patxxxx Fx,3,"(OVERALL EVALUATION) The paper studies an interesting problem.

The approach they proposed is rather piecemeal.
Many components are used, yet not all of them are fully validated. 

The evaluation data sets used for evaluation is rather small.

It is hard to know the novelty of the proposed approach.The authors need to make this more salient in the paper.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2/23/2018,14:42,no
198,56,204,Minxxxx Kx,4,"(OVERALL EVALUATION) This paper examines the problem of detecting academic plagiarism using image based methodologies for plagiarism of figures/charts and other graphical contents, using both ratio and perceptual based hashing, involving the repurposing of the AlexNet 2012 architectured convolutional network net (CNN) for one of the 4 methods ensembled to make the system.

The image-based approach to plagiarism is a good idea that has been raised but little practical studies have gone onto working on this area to my knowledge.  It is heartening to know that this work has been executed.  The paper is well structured and argued, and the model and architecture is sound and reasonable.

Minor points.  I'm not sure that I would agree with ""Strongly disguised images"" as plagiarism.  Certainly photographs of different landmarks don't constitute plagiarism and I think it would be good for the authors to state why the VroniPlag collection deems such cases as plagiarism.

Figure 3 can be better drawn to be more vertically compact and use space more efficiently.  The aspect ratio looks off.

I'm not sure that the implementation details (in Py 2.7) would be necessary details to discuss.  The algorithmic steps in 3.4 however, are nicely described and reduced to a practical description, which is helpful.

It would be great if the authors could address how their system would scale and the constants modified when dealing with a larger corpus.  For example, in S 3.11 there are some constants (for #m of strongly related images) that would seem to be quite corpus and scale specific.  How to select these well would be a useful side discussion if there is sufficient room for discussion.

The approach is also scalable as the system uses parallel processes to execute different forms of plagiarism detection.  

There are two concerns that I hope the authors can address:

- The evaluation.  I'm pretty skeptic that the results are actually as strong as they report them to be.  13 of 15 is ok, but how about false positives (e.g., problems with precision low)?  Given that there are potentially may legitimate pairs of images that are incorrectly flagged as potentially plagiarized, this problem needs to be discuss in much more detail.  The authors seem to focus only on recalling the 15 images.  13 of 15 isn't particularly convincing, and it would have been good to attempt manufacture some cases synthetically to see whether the approaches can detect them (and accordingly distinguish them) from the others.

- The word ""adaptive"".  I don't see how the approach is being adaptive.  The system parameters need to be set appropriately for the system to work well, and couldn't find how these parameters are set.  It seems the ""adaptive"" part might actually a significant downside (if I am interpreting correctly).

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Overall, a paper with some issues, but overall, nicely structured, well-written and timely.  Will be a good basis for others to improve upon later, especially if the authors could share their dataset.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,2/23/2018,15:25,no
199,57,163,Fexxx Haxxxx,1,"(OVERALL EVALUATION) The paper describes a very promising approach, coined WELDA, that enhances topic models, specifically LDA, by adding/removing topic words using information from related word embeddings. Topic words are an important part when working with topics, e.g., they are usually used in further processing steps of an NLP analysis pipeline, e.g., to summarize documents or find related documents. Hence, the quality of these topic words is a critical factor of the workflow. The proposed approach shows a novel way of addressing the issue of low quality topic words by combining two state of the art approaches: LDA and word embeddings.

relevance to JCDL: highly relevant since the approach addresses a very common issue in digital libraries and handling NL.

novelty/originality: the approach combines two state of the art techniques/concepts in a novel, non-trivial way. 

methodology: the development of the approach and important design decision are all well understandable and sufficiently discussed in the paper. 

assessment/evaluation/comparison: in their evaluation, they authors analyze the runtime performance and topic quality of their approach, and compare the quality with state of the art approaches on two datasets. WELDA seems to be superior in most the the cases, indicating that the approach is an improvement over the state of the art. however, the study described in sec 5.4 misses crucial information on the number and age of participants.

style/quality of writing: The paper is written well understandable, technically sound, and very well structured.

replicability: since the approach is currently (see also my note below) not publicly available, reproducing the results would be difficult!

I want to emphasize that it will be great for the research community if the authors decide to publish their code/software under an open-source license, such as Apache v2. Unfortunately, the authors currently do not mention whether they plan to make their code publicly available, so I assume that this is currently not the case.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The authors should be asked to publish their code/software under an open-source license.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,1/31/2018,13:23,no
200,57,70,Sauxxxx Chaxxxxxxx,2,"(OVERALL EVALUATION) This paper uses the combination of topic modeling and word embedding to improve topic quality. The novel part about this work is it uses an effective sampling mechanism to replace a word identified via topic modeling and replace it with a word based on nearest neighbor embedding space. This lets it select a word that is more relevant to the topic. This also provides it an an ability to account for out-of-vocabulary words. The approximation based technique used for nearest neighbor search in the embedding space was a good approach to improve the runtime performance and the authors understood the importance of it and addressed it. The qualitative evaluation methodology was sound and focussed on the important aspects of topic modeling.


Recommendations for the authors : I would have liked the authors to evaluate their techniques on more datasets. As a reader I want to gauge the ability of this technique on modern datasets than span beyond the obvious ones like 20 NEWS. I would want to know how this technique would work on short and unstructured data like tweets.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Recommendation for Authors : I would want the authors to show the break up of the individual times for the runtime evaluation in Table 7. As a reader I want to understand the runtime at the individual corpus level. Also, I would want the authors to make the figures more accessible to users that might be reading a black and white copy of the paper. It relies too much on color coded graphs and points.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/14/2018,4:00,no
201,57,65,Lilxxxx Caxxx,3,"(OVERALL EVALUATION) The paper provides a detailed description of a new approach to combining word embeddings with latent Dirichlet allocation to improve a topic model.  The paper gives the background, reviews pertinent prior work, illustrates the new approach  and describes the data sets used and the results.  There are sufficient figures to describe the strengths and weaknesses of both the techniques that are combined.  The handling of out-of-vocabulary words is a strength.  The topic modeling approach handles previously unseen words well because of the scale of the data on which it is built.  The LDA approach ignores words not previously seen.  Merging the two provides a definite improvement for the LDA work.  

The paper is well written, very clear, and includes all the information needed for others to evaluate the work and to replicate the results.  

Line 241 (thanks for the line numbers) refers to a ""method by Mikolov et al."" but does not include a citation.","Overall evaluation: 3
Reviewer's confidence: 3
Recommend for best paper: no",3,,,,,2/14/2018,22:29,no
203,58,179,Hexxx Hocxxxx,1,"(OVERALL EVALUATION) The paper describes a collaborative digitisation project between a municipality, a historical society, two universities, and local citizenry. The author has done a good job describing how the project was organised and how it progressed, including the roles of each partner involved in the project.

It would be a much better paper however, if the author could leave out some of the details and reflect on what went well and what went wrong, and what are the lessons learnt. Many digitisation projects take place but what are the specifics about this project that is relevant to the conference or interesting to the readers? The author should also discuss long-term digital preservation considerations or arrangement related to the project. 

It would have been interesting to include details on contribution from city residents and volunteers, and discussion on how they might play a bigger role in any future effort of digitising the remainder of the GNMHS collections.

The paper could benefit from e.g. a workflow diagram which would reduce the detailed textual description.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/19/2018,19:13,no
204,58,7,Alxxx Abduxxxxxxx,2,"(OVERALL EVALUATION) This paper describes the digitization of the Greater North Miami Historical Society collection. The paper presents the collaboration between a municipality, a historical society, two universities, and local citizenry in digitizing and hosting the historical collections of the Greater North Miami Historical Society. Although the paper is a good paper to read, it lacks structure and headings. It would also be interesting to see some technical details to be included such as the database used or schemas applied to connect the collections together.","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2/17/2018,21:53,no
205,58,278,Wolxxxxx Nexx,3,"(OVERALL EVALUATION) The paper lacks significant results and discussion of outcomes and methods. It just describes the setup and workflow of a (small) project, the reader does not gain any additional insight.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/18/2018,19:43,no
206,59,328,Raxx Schxxxx,1,"(OVERALL EVALUATION) The paper proposes a query language for processing different analytical queries over collections of n-grams extracted from historic texts. The collection provides frequency information for each n-gram for a sequence of years and thus allows for analysing the temporal dynamics of n-grams. In addition, the paper shows that a number of standard questions from digital histories can be solved with the proposed algebra. It is hard to decide for a non-expert if these are the important questions in this domain. Here, a reference to the work of Koselleck would be useful.

The information types explained in Section 3 seem reasonable, and it is clear that they can be solved with a properly annotated collection and some language for expressing things like selection, aggregation, and joins. Thus SQL would be a possible alternative language. It is clear that SQL is not perfect for somebody without a deep education in computer science, so a simpler alternative focused on the specific problems makes sense here.

The description of the data model in Section 5 is partly not precise enough, partly surprisingly simple. In Definition 5.1, the text should make clear that the ""n"" in ""ngram"" is an actual parameter for the definition. From the definition, an ngram is a tuple with four components, some of which are again tuples of size n (and the last component is a tuple of size T); the definition explains this in a wrong and confusing way. It is unclear how the components of a time series can be mapped to the individual years if that is not part of the definition. How can one, given an ngram, determine which years are covered by the timeseries? Or is this a global parameter of the system? This is not explained.

Sentiment and category information is implemented through a very simple mechanism that assigns to each word sentiments and categories; it is unclear from the paper if there is a constraint (like at most one sentiment per word). From a linguistic point of view, this is a very simple approach since a word can have multiple meanings. How would one differentiate the different meanings of, for example, the word ""bank"", or even different meanings of the word ""war"" (like in ""war of the memos"", which is not a military concept). Similar things can arise with sentiments (like ironic comments). In general, one would expect some more complicated functions here for assigning sentiment and categories that take the context into account.

The actual operators defined in Section 6 are well chosen. The provided examples are very useful for understanding the semantics. The possible patterns for the textsearch operator could be explained in more detail. For the knn operator, it is unclear what a ""time series graph"" is. The example for the value operator is not very useful since it boils down to simply removing sentiment information that was added before, resulting in the plain frequency information. There will probably be a more involved application where this operator does something nontrivial. In Equation 13, D should probably be D^n.

The two use cases shown in Section 8 are convincing and demonstrate that the proposed algebra can be used to compile powerful queries. The paper does not explain how the interface should look like; will users utilize a graphical interface like that shown in Figures 3 and 4, or are they supposed to write textual queries? If the latter is intended, the paper must provide textual representations of the queries. Figures 3 and 4 lack a caption that identifies them. In Figure 4, there is a ""context"" operator that was not explained before; probably, this should be ""surswords"" (which is a bad name for an operator, ""context"" would be much better).

The paper lacks a case study with real users, which should assess how easy it is to create solutions like those shown in Section 8 with the proposed algebra. In addition, a comparison to existing solutions should be shown; this could include SQL, but also script-based methods widely in use now. It seems likely that the proposed algebra will be relatively easy to use, compared to existing solutions, but there is no empirical evidence for this.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/13/2018,16:54,no
207,59,133,Nixxx Gaxxxx,2,"(OVERALL EVALUATION) This paper proposes a query language that will be helpful to capture the information types present in a temporal text corpus mainly focusing on the information types as mentioned by Reinhart Koselleck. It provides a set of operators that are supposed to perform efficient queries on conceptual history while maintaining some hypothesis.

Overall the approach of the paper is good. Literature survey is at per. Hypothesis proving part is written well.

Accept

 But the examples given could have been much more precise i.e.  some instances of the actual text corpus would have added some more value.
 
In section 1.1, last para ?€?analysis?€? is spelled wrongly.

In section 4,  ?€?Step 1?€? , line 3 ?€?sentiments associated with particular concepts, we discuss in section 3?€? , here  ?€?sentiments associated with particular concepts?€? lacks proper definition. In the same section ?€?In the following, we show how each of Koselleck?€?s information types can be identified by one or more data characteristics?€? , it does not say how ?€?Geograpy?€? is captured by the data characteristics.

In section 6.9, numbering of table 13a, 13b is not proper in the picture.
In section 7.2, numbering of Figure 3a, 3b and in section 8.1 numbering of figure 4a,4b,4c,4d in the diagram are not proper.

In section 7, giving a practical example of how operators are combined to form queries  would have been good.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/15/2018,10:14,no
208,59,238,Jonxxxxx Lexxx,3,"(OVERALL EVALUATION) This paper presents an interesting approach to generate a query algebra/language (CHQL) for conceptual history and temporal change in term use and meaning. The undertaking is ambitious and of importance to Distant Reading within the Humanities. 

relevance to JCDL: The proposed query language and its additional functionality (in comparison to existing query languages) is very suitable to JCDL. Its presentation at JCDL would likely be of interest and provoke discussion at this type of search. 

novelty/originality: The proposed query language builds on the basic concepts of numerous prior languages and formalisms. 

methodology: The expressiveness of the proposed language (while not exhaustive) appears to be suitable for the authors' case study. The proposed operators (and their composition) seem to be minimally sufficient for the stated goals (i.e., support for Koselleck's work). While the full specification of this language is not entirely satisfactory from a mathematical/formalism perspective (i.e., see Marcos Gonclaves' 5S formal framework), this long-paper submissions seems to be a reasonable description of the concept given the space limitations.

assessment/evaluation/comparison: The model is shown to work for the simplistic case study in this domain. 

style/quality of writing: The paper is well written and easy to follow.

replicability: The formalisms provided in this paper provide a basis for multiple implementations of this concept. 

adequacy of references: The references are sufficient and suitable to place the work within the context of tailored query languages.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/16/2018,6:00,no
210,60,337,Axx Shxx,1,"(OVERALL EVALUATION) The study investigates search behaviour within a newspaper digital library, and makes use of user interaction with facets as well as users's click data to examine their search behaviour. The literature review makes references to old literature on search behaviour within digital libraries. A majority of the references are dated before 2010. Even specifically relevant references to how users view and make use of facets in exploratory search interfaces are missing. Here are some examples: 
Kules, B., Capra, R., Banta, M., & Sierra, T. (2009, June). What do exploratory searchers look at in a faceted search interface?. In Proceedings of the 9th ACM/IEEE-CS joint conference on Digital libraries (pp. 313-322). 

ACM. Kules, B., & Shneiderman, B. (2008). Users can change their web search tactics: Design guidelines for categorized overviews. Information Processing & Management, 44(2), 463-484.
 
Kules, B., & Capra, R. (2009, June). Designing exploratory search tasks for user studies of information seeking support systems. In Proceedings of the 9th ACM/IEEE-CS joint conference on Digital libraries (pp. 419-420). ACM. 

Here is a recent study that should have been cited as well: Han, H., & Wolfram, D. (2016). An exploration of search session patterns in an image-based digital library. Journal of Information Science, 42(4), 477-491. 

The study is appropriately reported. However, the study does not demonstrate originality or novel methodological approach. The authors do not elaborate on what we can learn from this study or what practical or theoretical implications this study may have for conducting search behaviour studies within digital libraries or how we can inform the design and development of digital library search user interfaces or digital library systems. The findings about users' search behaviour and the sources they have consulted do not provide any specific insight. They do not shed light on how newspaper digital libraries should be designed or studied. Having read the entire paper, the question ""so what?"" comes to mind. The authors make references to the importance of long tail of search, but do not address this important aspect of the study in the literature review.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Although the paper is well organized and presented, it does not provide any novel methodology or new insight or findings that we have not previously seen in the literature of digital library search behaviour studies. There is nothing particularly interesting or useful to gain from this study. The simple question is what can we learn from this study theoretically, practically, empirically or methodologically? As a researcher who has conducted several digital library search behaviour studies as well as search log and query log analyses I have hard time answer the question.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,3/9/2018,2:59,no
212,60,96,Antxxxx Doxxx,2,"(OVERALL EVALUATION) This paper presents an investigation of the search logs of the national library of the Netherlands. The authors are performing clustering based on interactive and metadata features, with the aim to reveal usage patterns.

    Novelty:
    While this type of work is not novel in general, it is interesting to see it applied to digital libraries. All libraries have search logs in-house that could provide useful information for the design of their IR models and UIs.
    
    Soundness/Methodology:
    This is the biggest issue of the work presented, which is very empirical, with choices that are not always satisfactorily justified.
    The choice of k=14 is notably highly empirical, especially as Figure 1 (used to support that choice) is rather inconclusive. Given that the stability of clusterings over 2 periods was the chosen evaluation measure, it would probably have made more sense to optimize the quality of the clusterings by chosing a value of k that maximises that very stability.
    Further, while the choice of stability over 2 periods appears reasonable, there are many other intrinsic measures of cluster quality that function without a ground truth, such as purity. These seem to have been overlooked.
    All in all, the too many empirical choices and analysis imply that reproducibility will be an issue.
    
    Results:
    The main lesson here is that there are different usage patterns. This is the only result upon which the paper can be firmly conclusive. Unfortunately, this is not new information.

    Presentation:
    The paper is very well written and pleasant to read, with the exception of citations: plase observe that [7], [22] and [14] are not words, and that they cannot belong to a grammatical construct such as a sentence. Using them as such only makes the text harder to read. Think of citations as text in parenthesis. One good example in the text is the following, except that the citation is actually missing: ""Wang et al. use unsupervised hierarchical clustering to detect patterns in user behavior from logs in social networks."" ([28] should occur, either right after ""Wang et al., or preferably at the end of the sentence).
    While Figure 3 seems to be potentially interesting, it is unfortunately too low-resolution to be readable (even in its digital version). Please improve image quality, make space for it and enlarge it, or just remove it.
    Table 3, cluster 11: what is a ""home page"" in this context? Does it mean the user remains on the search page (no clicks)?
    References: Several encoding issues, at least with references 1-4.
    
    Overall:
    This is promising work but it seems more fit for a short paper. This investigation and its potential results are interesting, but they lack maturity. The work presented here forms the ground for a potentially very strong paper next year.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/19/2018,5:08,no
213,60,51,Gexxxx Bucxxxx,3,"(OVERALL EVALUATION) Overall I like this paper, but the writing and other parts of the presentation demand quite a lot from the reader. 

The issues vary - e.g. figure 3 is simply hard to read due to the lack of resolution in the file; however, more strategically, it is quite dense reading getting to grips with how the analysis was actually done. 

The definitions of the 'full', and 'small' combinations, 'interaction features', etc. are spread across Sec. 3 and 4 -(e.g. top of 3, 3.1 and 4.1). Please take care to define these well, and explain their relationship with the original log data, perhaps by example. As it is, detailed cases appear later, when one is already struggling to ensure each concept is understood. I appreciate that this is complex work, but the presentation at present took several readings to ensure I grasped it. The lack of detailed information on the original logs also makes things a bit harder for the reader.

How the visualisations were qualitatively evaluated is rather skimmed over, which is difficult because it appears to be a rather important part of the overall investigation? Likewise, Figure 5's content is not immediately tractable to the general reader. Also think of colour blind readers please! Figure 8 also would benefit from more explanation.

Overall I think the outcomes here are useful, but it's tough going for the reader. I suspect discarding some minor points and more carefully portraying the remaining results would in fact improve the paper substantially. A thorough revision would produce a much more readable and impactful paper.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/17/2018,7:50,no
214,62,79,Faxxx Crexxxx,1,"(OVERALL EVALUATION) This paper explores the characteristics of Twitter tweet-trail and proposes an analytical framework for structural and social aspects of Twitter data stream. 

The major weakness of the paper is that the contribution is weak. Most of the work described (e.g. the whole section 3.2) is already published in past work [1]. The only contribution is the Twitter profile analysis. However, this is done manually which is not a real-case scenario if you have thousands of tweets. In addition, the authors do not compare their work with previous works and it is hard to understand if there is any novelty at all. Also, the analysis of the results is superficial and short.  

The paper is well written. 

1. Zhang, Y. and Chang, H.C., 2018, January. Selfies of Twitter Data Stream through the Lens of Information Theory: A Comparative Case Study of Tweet-trails with Healthcare Hashtags. In Proceedings of the 51st Hawaii International Conference on System Sciences.","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,Anastasia,Giachanou,anastasia.giachanou@usi.ch,354,2/8/2018,20:01,no
215,62,137,Danxxxx Gxx,2,"(OVERALL EVALUATION) The paper presents an interesting method of Shannon's information theory applied to explore the features of Twitter tweet-trail.
However, this study was already published in Zhang, Y and Chang, H.-C., An Information Theoretical Perspective in Knowledge Discovery And Data Design Innovation - Proceedings Of The ICKM 2017, Dallas, Texas, USA, 25-26 Oct. 2017, vol. 14, Daniela G. Alemneh, Jeff Allen, Suliman Hawamdeh (eds.).

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This research was already published in Zhang, Y and Chang, H.-C., An Information Theoretical Perspective in Knowledge Discovery And Data Design Innovation - Proceedings Of The ICKM 2017, Dallas, Texas, USA, 25-26 Oct. 2017, vol. 14, Daniela G. Alemneh, Jeff Allen, Suliman Hawamdeh (eds.). I am so disappointed to discover this attempt to republish the same survey at the most important Conference on Digital Libraries.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2/14/2018,11:32,no
219,62,324,Haxx Salxxxxxxx,3,"(OVERALL EVALUATION) I was not aware of the previous two almost identical publications to be honest. So my initial review you can see below. But when I looked up those previous works I went and checked them and the paper ""Decomposing and Visualizing the Twitter Data Stream with Healthcare Hashtags: An Information Theoretical Perspective"", is almost exact copy of this one with a change in the hashtags they used. Which in turn is the exact copy of the HICSS paper. This is severely unacceptable. That's why my evaluation now is strong reject.

Original review (weak reject):

The authors present the problem of analyzing and remodeling digital social media platforms in the eyes of information theory. Namely, the authors propose a novel analytical framework of analyzing social media data stream (Twitter) using the entropic method of Claude Shannon's information theory.

The flow of the paper is good and it is clear to a large extend, and the authors placed considerable amount of related work throughout the paper going back to works published in 1924. There was a little bit of repetitiveness throughout while describing information theory in general and Shannon's work. The idea is rather novel and quite interesting but I have some points of concern as follows.

In their case study, the authors extracted a minute dataset totaling 300 tweets and further sub divided them into two sub datasets, for the period of just 72 hours. In the realm of twitter this is very small to extract most meaningful signals. Granted they had to manually label certain aspects of user accounts but they could have opted to create a script that mine official and non-official profiles in the field of the tweet (flu, medical) and extracted content in the size of thousands. The paper spread almost two pages to explain what is entropy and calculation matrix which is described thoroughly in previous works. Furthermore, I believe analyzing images in social media interactions merely with its RGB aspects is lacking. There are numerous precious computer vision / social media analytics papers that explain the exact opposite of this and that content is important, what is in the image is important than even its size, color scheme or filters applied. Black and white and greyscale images are proof of that. Lastly the trends they picked were fairly limited, it would have been tremendously exciting to see several other ""current"" trails and the differences between them, and analysis of the origins of the trails and how they pick up speed in retweeting or die, that would give invaluable insight with their powerful application of entropy.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I hope the committee reprimand the authors. This is unacceptable and a huge waste of the time for the reviewers.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2/19/2018,8:14,no
220,63,164,Fexxx Haxxxx,1,"(OVERALL EVALUATION) The authors propose a system that uses state-of-the-art methods, specifically a sentiment analyzer by the NLTK library and a Random Forest, to predict prices of cryptocurrencies. While in overall predicting prices of stocks and cryptos is an interesting topic, the novelty of the proposed method is rather low, since only two well-established methods are combined. This also shows in the evaluation, where the F1 score of the proposed methods is not significantly higher than that of Goel's formula.

The motivation needs to be strengthened, e.g., in the first paragraph of Section 2, the authors mention that randomly investing and selling cryptocurrencies yields (naturally) similar results as optimized approaches. In the current form, this makes the whole contribution of the paper questionable. I'd suggest to further discuss advantages and disadvantages of the individual methods, and - most importantly - very clearly derive and mention the research gap. The related work section also lacks a discussion of related approaches, or at least going more in depth for the ones that are mentioned. If there are no more related approaches, the authors should explicitly mention that. It is also not clear to me why ""To successfully predict the prices of cryptos we have to construct a framework by linking ML techniques [22] with social sentiment analysis to capture the people's reactions."" is the case. I guess other methods and features could be used as well, instead of ML techniques and sentiment analysis of social media data. One idea would be to use time series analysis. To strengthen the related work section, and motivate the research done in this paper, the authors need to (briefly) discuss the advantages and disadvantages of the most related methods and approaches. Also, it would be interesting of the authors briefly mentioned P,R, and F1 of state-of-the-art methods in the related work section. This could also help to motivate the research project described in the paper.

The authors should also make clear why they only used articles from one news website, and why specifically the used website. Using only articles from a single website introduces a significant data selection bias. Ideally, the proposed approach should be performed on a larger dataset containing articles from multiple, representatively selected news publishers. I'm also wondering why only 22 articles reporting on Ripple were added to the dataset. The authors mention that this as well: ""On Ripple, things are different; only 22 articles are too few to perform a good score, both of the formulas having a poor performance."" I'd suggest to add more article generally to the dataset and specifically make sure that for each of the analyzed cryptocurrencies a statistically reliable number of articles is contained.

Further comments:
- The style of the paper requires improvement, e.g. on page 3 the authors inserted a piece of code in a way that is not very space efficient. I think the code (if it's necessary at all for understanding) should be brought into a pseudo-algorithmic, more dense way, containing only the important procedural steps of the method. The space in the paper that would be saved this way should go into describing the method more specifically.
- The language requires some improvement as well, e.g., there the tense changes from one sentence to the next. This would also help to improve the readability of the whole paper. Or another example from the conclusion: ""Next, the results of prediction models have been shown that the volatility is decreasing over time [...]""
- In some cases, the clarity/readability could be improved by describing more specifically what has been done, e.g., in the conclusion the authors should also mention what specifically the method they devised does and how (one sentence would be enough to make that clear).","Overall evaluation: -3
Reviewer's confidence: 3
Recommend for best paper: no",-3,,,,,1/26/2018,15:13,no
222,63,145,Vixxx Gxx,2,"(OVERALL EVALUATION) The paper presents a machine learning / sentiment analysis based approach of mining web pages titles to predict the closing prices of specific cryptocurrencies.

- presents an idea of filling in gaps in cases of missing data, but no discussion of other possible data imputation approaches.

- the experiments are severely limited in scope, both in the datasets chosen and the approaches tested, and i'm not confident in the generalizability of the proposed approach

Would recommend that the authors build on the proposed work.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/16/2018,20:04,no
223,63,420,Masxxxxxx Yosxxxxx,3,"(OVERALL EVALUATION) Although the topic is interesting, this paper has several flaws.

The description of the proposed method and experiments is unclear in
many points.
The authors used random forests model for predicting the future price. 
The authors should clarify what features they used for
the prediction. Most likely, they are previous SA scores and prices,
but if it is the case what is the window size?
The authors developed the formula (3); however, the formula is not
interpretable because the parameter $n$ is not explained. 
The main claim of the authors is that the formula (3) outperforms the
formula (2). However, from Table 2, the difference of F-measure is
marginal. Because the size of data used in the experiment is too
small, it is unclear whether this result could be generalized to other datasets.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/18/2018,15:58,no
224,64,224,Maxxxx Khxxx,1,"(OVERALL EVALUATION) This paper proposes applying the analytic hierarchy process (AHP) when building a collection for an institutional repository. The AHP process has been applied in operation research, and most recently has been adapted to evaluate interlibrary loan service, but the authors claim that this work is the first to introduce it for the purpose of building collections. 
The paper is well written, and is easy to follow overall. The method is clearly explained, with the authors describing a set of applications that would benefit from the AHP process.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I'm not at all an expert on this topic -- in fact I'm not sure how I ended up with this paper. Therefore, I can't comment on its novelty in comparison to state of the art, or judge the claims within. However, I found the paper easy to follow, and relevant to people in that domain.","Overall evaluation: 1
Reviewer's confidence: 2
Recommend for best paper: no",1,,,,,2/17/2018,0:30,no
225,64,155,Juxxx Grxxxx,2,"(OVERALL EVALUATION) Strategies for developing IR collections continues to remain an area of interest for IR managers and repository communities, though current concerns seems to focus on preservation and connecting open repositories to other systems in an organization?€?s infrastructure; on the content side, because of the rise of open access publishers and open content, libraries are reminded of the growing role they can play in collecting and archiving the grey literature of an institution. 

The article is original in its application of the Analytic Hierarchy decision making model to IR collection development processes. I believe there may be a few missing citations: Which data source did authors use to determine that there are nearly 400 IRs in the US? And how did authors decide that decisions for one successful collection rarely work for another group? (I?€?ve seen IR managers successfully adopt collection development practices of other libraries). Authors assert that all institutions consider the same set of factors in determining their collection development policy; this is not necessarily a true statement. Citations that point to prior research or practice would help to verify accuracy. Authors also state without a citation that the Analytic Hierarchy Model is a standard decision-making model in engineering and business and that it is now beginning to be used in the information sciences; because it is new to digital library/information science it is not yet general knowledge.

A new direction for some repositories is integration with research information management systems (RIM) or current research information systems (CRIS); how would the decision making model apply or need to be adapted to assist institutions with this kind of collection development practice? Because we know barriers to faculty self-deposit are many, and versions of some works are being made available in other repositories, should the focus of IR collection decision making should be less on ?€?articles and publications?€? and more on expanding the types of content and collections and communities supported (e.g. digital humanities and open educational resources). The CNI report, ?€?Rethinking Institutional Repository Strategies?€? does a nice job of framing the current concerns and possible future directions. https://www.cni.org/wp-content/uploads/2017/05/CNI-rethinking-irs-exec-rndtbl.report.S17.v1.pdf 

The list of evaluation criteria in Table 1 also does not include an explanation of how the list was generated. Without a citation or explanation it is hard to say whether the list is comprehensive or missing important elements. Based on my own experience, I do wonder why criteria such as ?€?Solicit items that are at greatest risk of being lost,?€? or ?€?Solicit items that can not be found in other repositories?€? are not included. 

Overall, the paper is well written and the topic would be of general interest to the broader JCDL community.  In terms of small changes recommended, ?€?form?€? should be changed to ?€?from?€? on pg. 2, Table 1, Alternative Decision 6. A period is also needed at the end of the second sentence in the first paragraph of pg 3.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I would like to see the paper considered ""accepted with revisions"". More literature in the DL field about institutional repositories would help to foster more collaborations among information scientists, computer scientists, data scientists, and librarians.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/17/2018,3:22,no
226,64,347,Joxx Smxx,3,"(OVERALL EVALUATION) Summary:

This short paper looks at the effectiveness of Analytic Hierarchy Process (AHP) for building a particular institutional digital repository. Originally developed for operations management (especially in engineering), AHP is a decision-support system which organizes criteria into a hierarchy of importance, assigns values to each level, and uses a matrix-based comparison of the scores for each candidate to reach numeric score for the candidate. The total scores produce a ranked recommendation of the candidates.   

In the case of Digital Libraries, the authors describe using AHP to help determine the best strategies for building an institutional repository - a challenging task when it comes to tracking an institution's own scholarly publications. According to the authors, their AHP proposal is novel. They believe their approach can address a number of problems institutions face when developing a repository (IR) by identifying which factors are most effective, provide justification to stakeholders, and potentially point to methods for increasing faculty participation. 

Review:

The paper is written reasonably well, is very readable, and suffers from very few grammatical errors. They present an example implementation and step through the process of evaluation, including developing assessment criteria. The authors also explain what would otherwise be a bit of a sticking point, i.e., how to be sure that the evaluation matrix is internally consistent given the subjective nature of criteria creation, ranking, and scoring. However, the do not address the impact of equal-scores on the evaluation process.

The example and discussion would be much stronger if the authors had used scores (real or fabricated) to illustrate the AHP decision process. That would also help to clarify the process of building the evaluation matrix and reaching consensus/decision. The bibliography is a bit scant, given the numerous references and professional resources available to AHP practitioners. Since the authors have at least a half column of space still available, there is certainly enough room to address both of these points.

A few other issues: It is unclear how the authors arrived at a count of ""nearly 400"" IRs in the US. Defining the goal is left somewhat nebulous; the authors need to clarify how the goal itself is clearly defined for successful use of the AHP method. I also feel that they  skate over the level of effort required for an IR team to create their own weighted hierarchy.

Overall the paper has a lot of potential but it needs a good deal more discussion of the AHP method, their own experiments, and resource review. The topic would probably be better handled in a long-paper format where there is more room to address these concerns.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Adding a review since we are missing one reviewer. This really would have been better as a long paper, but I suspect the authors are still too early in their project for sufficient detail at that level.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/20/2018,17:58,no
227,64,181,Nixx Hoxxxx,4,"(OVERALL EVALUATION) In this short paper, the authors describe the Analytic Hierarchy Process and how it could be applied to evaluating institutional repository (IR) collection development strategies. It is a relevant paper to JCDL in that it addresses practical issues of digital libraries, and is novel according to the literature review, which states that this process has not yet been applied to the archival or digital library fields.

The paper states that it will map out an AHP model for an IR and use it to evaluate a single IR based on weighted criteria. I felt that the paper fell short of these goals. The model is described theoretically and example goal(s), criteria, and alternatives are provided for an IR but I do not feel that the model is then used to arrive at any conclusion or evaluation of an IR. Moreover, I did not see any weighted criteria assigned to their example model but rather a discussion of how using weighted criteria works. Finally, the model as described states a single decision goal, but multiple goals are frequently mentioned (efficiency and value). I understand that this might be a criticism on semantics, but in the context of a complex model, I think the language could be more precise to be better understood.

I found myself drawing up the model while reading and would have appreciated additional tables/diagrams to express the model that they were constructing in this paper. 

References are adequate, and the writing style is clear for the most part with some errors in the Research Design section and in caption text that makes it a bit confusing.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/23/2018,14:32,no
229,65,134,Nixxx Gaxxxx,1,"(OVERALL EVALUATION) This paper analyzes the distribution of large number of organic and retroactive tags in narrow folksonomies and their difference from professional indexing languages with controlled vocabularies. Retroactive taggers used lots of unique tags and synonyms compared to organic taggers and sometimes they used co-occurring synonyms to increase the visibility of a topic. The paper presents an interesting topic and it is well written, easy to follow. I have some suggestions for the improvement of the paper ---

1. It is better to mention the timeline in the paper based on which you have divided tag set into organic and retroactive classes (Table 1).

2. Some statistical significance test is required for the results reported in Table 3,4 and 5.

3. Table 3 and 4 report the stats based on only top 100 tags. The author should report these stats on a larger set of data. Some of the synonyms (except abbreviations) may be captured through automatic techniques.

4. You may make the dataset publicly available to increase the visibility of the paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) revised as per PC Chair","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/16/2018,11:03,no
230,65,67,Lilxxxx Caxxx,2,"(OVERALL EVALUATION) The paper compares tagging in folksonomies between tags done at the time of a posting and tags done retroactively in a set of items posted before tagging was available.  The paper includes a nice discussion of related work.  The work is focused on postings in the MetaFilter and Ask MetaFilter lists.  

The paper is clearly written.  The methods section describes choices made in the evaluation -- the distinctions among the four data sets, the use of the top 100 tags in each set as organization or identification tags, for example.  It would have been informative to know more about how the data is processed -- what language or tools used.  

The paper describes the use of synonym tags and synonym co-occurrences and makes some assumptions about why retroactive taggers would use synonyms in the same post, perhaps for increased success in search or retrieval.  Do not the search or retrieval interfaces adapt to the possibility of things like cat and cats?","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/16/2018,17:56,no
231,65,54,Roxxx Buxx,3,"(OVERALL EVALUATION) This paper describes a study of the distribution of tags in the community sites MetaFilter and Ask MetaFilter. The main finding that the distribution of tags follows a power law and that this distribution is more or less the same between tags generated organically by posters and those generated by dedicated taggers. In particular, the dedicated taggers were liberal in their use of synonyms, behavior that would not be typical of professionals working with a controlled vocabulary. 

This is a small result, perhaps reasonable for a short paper. I would have been more satisfied if more than a single source were used. I am also missing the specific method that was used for the power law fit.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/16/2018,17:58,no
232,66,235,Jxx Hx,1,"(OVERALL EVALUATION) This paper investigates library OPAC users' cross-device transition behavior by analyzing the transaction log with a goal of predicting the next device users are going to use as well as what kinds of activities they will conduct. The general idea of investigating and comparing OPAC users' cross-device behavior is interesting, and has not been studied much as authors stated. However, the particular scope and goal of this paper is not as compelling. The authors make general statements about how predicting the next device people are going to use is going to help provide better and more intelligent services. However, examples that they give (such as recommending more device-appropriate resources or resources that are more suitable to a particular search context), in my opinion, has not much to do with being able to predict which device the user is going to use next. It would probably make more sense to explore how to best detect the current device being used by the user rather than prediction. I could think of some other potential reasons why this kind of investigation may be useful for improving the user experience but the authors do not do a sufficient job of explaining the motivation of this study as well as the implications of their findings. More specific and concrete information about how this finding will support the users' OPAC search behavior will make this paper a lot stronger.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/16/2018,23:58,no
233,66,52,Gexxxx Bucxxxx,2,"(OVERALL EVALUATION) I like the topic of this paper, but the writing leaves a lot to be desired. 

The total number of transitions is fairly small (248), and the time distance for a transition is quite large (24hrs). Is that timespan justified by prior research? The breakdown of the devices involved doesn't appear to be reported, which seems to be a significant gap in the reporting, though some appears in Table 2 regarding a feature analysis.

Some of the definitions are contestable - e.g. the use of browsing appears closer to document triage (c.f. Loizides and Buchanan, or Cathy Marshall).

The outcomes of the more complex groups of predictors appear to be very mild (Table 3). The writing may not be clear here, but it appears that the RMSE is higher for predicting the next activity (i.e. is a weaker predictor), and hence the text that follows appears to be somewhat inconsistent with the table? 

It appears that the time interval appears to be a poor predictor of behaviour once a session (appears to) resume. Some further reflection on this would be helpful.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2/17/2018,8:23,no
234,66,264,Daxx Mcxx,3,"(OVERALL EVALUATION) This paper has an interesting premise: examining what users were doing when they changed device while searching a library catalogue. The paper makes reasonably clear case for why this is important, though I think they could be stronger in their justification for the importance of the OPAC over information sources such as Google Scholar.

The major flaw in this paper is that it is VERY poorly written. There are a number of problems with the English in this paper, and they affect readability dramatically. The other main problem is that the authors do not reflect on the practical implications of this work for DL design or implementation--what does it mean that users transition between devices when they do? is this a good or bad thing? What, if anything, should we do about it?

Were the authors to correct these issues, this paper would be highly acceptable in my view.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/17/2018,8:26,no
235,67,171,Braxxxx Hemxxxxx,1,"(OVERALL EVALUATION) This is a very nicely done study.   It aims to gain a better understanding of how shared annotations, in this case, highlighting, can be helpful for shared reading situations in digital libraries.  This addresses an area that have not been well studied, the effectiveness of shared annotations like highlights.   To do this, the authors conduct a small within subjects controlled laboratory study to see what differences are caused with ?€?appropriate?€? highlights and ?€?inappropriate?€? highlights.  I like the design of the study, and how it was conducted.  The use of eye tracking contributed novel aspects to this work.  They come up with many interesting results, especially qualitatively from the post experiment subject interviews.   I think this would be of interest to the JCDL audience, and well received. 

A few comments on the study, that might improve the presentation.

The participants are given 5 minutes to read the 3000 word Scientific American articles.  Is this sufficient time?  You quote subjects saying they chose to view highlights because they didn?€?t think they could read the whole article (?€?passages too long?€?). This suggests that at least some subjects felt they couldn?€?t read the whole article in time?   It would be helpful to explicitly state what your goals were, and what was expected of the participants when reading.  Were they given sufficient time to read the whole article?  At a skimming level? At a deep read level?   You could establish this by having some subjects read the articles to see how long it took (for certain level of reading).   (i.e. if it?€?s taking 7 minutes then you?€?re conducting a pressured reading situation that might benefit from a reading aid like highlighting).  And note that reading times would vary by subject.  Also if you asked about this in the interviews, it would be helpful to report.  I.e. ?€?did you feel you had enough time to read the article completely?€? or similar. 
It wasn?€?t clear initially if participants were told how the highlights were  made, and that there two two types (APP, inAPP).   In our experience, highlights and annotations are made for multiple purposes.   Readers like to know the provenance of the annotations.  I.e. were they made by students like me, for this same class, for the same assignments?  Were they made by scholars in the field (what is their pedigree?).  How they use annotations would depend on this (context, who made highlights, what their task is in reading).  Later in the article it sounds like they were not told about APP and inAPP classes of highlights, but recognized this during the process, and perhaps tuned out the inAPP ones more.  I would suggest stating more clearly what they knew about annotations.  And qualify that reader?€?s perception of source and quality of annotations has an effect on the attention and trust they give them. 
Related to this.   ""quality""  of a highlight depends on context, user, task.  For instance high quality highlights for a student taking a test that covers ""surface"" knowledge of terms/dates etc might mean highlights of this specific material that was on test.  However, these same highlights may not be ""quality"" highlights from someone wanting a deeper reading, and understanding of research methods, application of results, etc.   So I would suggest that ?€?quality?€? is not independent of the reader (user), the context, and the task they are reading for.  I suggest making that more explicit.","Overall evaluation: 3
Reviewer's confidence: 5
Recommend for best paper: no",3,,,,,1/25/2018,16:45,no
236,67,11,Maxx Agxx,2,"(OVERALL EVALUATION) The research topic deals with digital reading and fits to JCDL. The experiment is well designed and various data are carefully collected and analyzed. Relation to the previous study by the same authors, however, is not very clear. 
Results are clearly shown on the whole, except for description of findings from interviews (section 4.4). It is understandable to some extent, taking a page limitation and qualitative nature of interviews into consideration, but still it would be better if APP and INAPP conditions were considered in section 4.4 as in the other sections.
In the analyses, only the difference between two cognitive styles (FD and FI) and conditions (APP and INAPP) are considered. It might be useful to divide the participants according to term-definition matching and Cloze scores, or usefulness scores of highlights. 
The conclusion that the impact of cognitive style were inconclusive seems reasonable, and a further experiment on larger scale would be useful.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/12/2018,14:24,no
238,67,7,Alxxx Abduxxxxxxx,3,"(OVERALL EVALUATION) This paper examines the impact of pre-existing highlights on digital reading process through eye-tracking and interviews. The paper is well-written with interesting results on pre-existing highlights. There is a comprehensive coverage on prior work and a good discussion section on the results obtained.

There is a missing reference: Guidelines for Effective Usage of Text Highlighting Techniques (http://ieeexplore.ieee.org/document/7192718/).

Perhaps, because of the page limit there is a limited coverage on the study design such as the device used to conduct the study, the specific texts used in the study, and the study set-up. This is important as it would enable future readers (for example, graduate students) to recreate the study.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/14/2018,15:09,no
240,68,28,Omxx Alxxx,1,"(OVERALL EVALUATION) The paper describes a large census transcription project using crowdsourcing. The idea is very interesting with lots of potential for researchers and other government agencies. 

The author provides a good description of the problems, shortcomings, and overall road map. The paper is a bit weak in terms of data and other statistics, which can help assess the complexity of the problem.

The project is somewhat similar to the work by Hansen et al. on FamilySearch (record transcription on a different domain) https://dl.acm.org/citation.cfm?doid=2441776.2441848. Hansen describes arbitration and peer review as patterns for this type of work and the author should compare. That would be a great contribution or a future publication.

Overall, nice idea but lacks data and other implementation details.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Nice idea. Would like to see more data/details :)","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,3/8/2018,16:52,no
241,68,41,Joxx Borxxxx,2,"(OVERALL EVALUATION) This short paper describes a project to transcribe census data from handwritten forms by using a crowd-sourced initiative. 

It is a very interesting initiative, with a motivation and challenge easy to understand, and the potential value of the results are also easy to perceive. The present state of the project is described, but since it is work in progress, unfortunately no lessons are yet concluded (what is according to what to expect in a short-paper).
However, as it is, we end with a paper about an interesting problem, but with no interesting contents... that could had been addressed if the author could describe in more detail interesting challenges that, we might believe, the project already might had faced until this moment, or...
There were several crowd-source initiatives in the past to transcribe data (especially bibliographic catalogues, in some sense a challenge related with this one of the census data) and even we can recall the glorious ""Gutenberg Project""... It is sad that this paper is not presenting a state of the art of that, and a brief of the lessons, which could be of value for this project...

Anyway, it is a very interesting project, and about a very relevant challenge!!!

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) At least, this could give a relevant poster, in case it is not accepted as a paper...","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/11/2018,20:30,no
243,68,188,Antxxxx Isxx,3,"(OVERALL EVALUATION) This paper presents the OpenGenealogyData project, which aims at crowdsourcing trasnscriptions (first of the 1930 US census) and releasing the data openly for research. Efforts are made to make the transcribers' work easier, notably by helping them transcribe less and better (with advanced OCR and som heuristics to auto-fill data) and presenting them with parts of the census they may be better 'connected' to (by their location, family history, or search interest).

I am a bit split over whether this paper fits the JCDL standards. On the one hand, this is an intersting project, with a good motivation, a clear technology path and some choices made, which are very practical. The more 'advanced' parts of the project (recommendations for crowdsourcing) seems very reasonable, and evaluation is planned.
On the other hand, the project is in a very early stage, there is no data apparently produced at all, nor a demo. This could be ok for a short paper, but there are some areas with the project seems really pre-mature, especially:
- The 'market-focused' approach that brings some positive guidance for the design of the crowdsourcing platform backfires in the sense that there seems to be not much awareness of work done outside of the US. This reviewer is not an expert on censuses, but in the Netherlands, for example, http://www.volkstellingen.nl/ makes available OCRed tables from Dutch censuses. The CEDAR project has also ventured into further formalizing the data for exploitation by researches, and it ran into quite many problems, which could bring a valuable horizon for the OpenGenData project. 
- For all the emphasis on 'open' data, the paper is not clear under which conditions the data are going to be released, and whether this is really going to be open in the sense of e.g. opendefinition.org. HistoricJournals, the orgnization running the project, has a lot of 'free' services but that is not necessarily open. It took some exploration to find CC0 mentioned at https://opengendata.org/terms.html. This is great but could be more visible! It would be good to have more details in the paper including - why not - a brief discussion on the challenges for such an approach. Especially in a crowdsourcing context: transcribers are expected to contribute for really nothing, which is a nice vision but needs quite some motivation. This of course connects with the design choices presented, but the connection (and challenges) should be highlighted in my view.

Maybe some of the huge space taken by the figures (Fig could be sampled, and fig 2 has a bit of repetition) could be used to add some elements long these lines.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/16/2018,17:10,no
244,69,245,Clixxxxx Lyxx,1,"(OVERALL EVALUATION) This is a helpful paper on an important topic that hasn't gotten enough attention. It needs to cover a few more points, if possible, and needs a few points clarified.

I have a number of detailed comments. In the introduction, it's worth noting that historically authority control has been primarily concerned with monograph authors. Also, one of the major purposes here is to bring together variant forms of author names. Authors often don't identify themselfes consistently from paper to paper, and in addition, sometimes change names due to marriage, divorce, etc. This is really badly needed in repositories. 

There is a lot of discussion of linked open data. This has great potential, but also huge problems. How are contradictions and inconsistencies in data resolved in the linked open data cloud? How is this project dealing with that?

In the discussion of the proof of concept application, there needs to be some discussion of how errors (including false consolidations of multiple authors) are handled. In other similar applications, which rely on multiple, independently curated, data sources, where there are mixes of algorithms and human editorial work, interchange and repeated synchronization of the multiple data sources has been a very complex nightmare.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) A good paper on a good topic. I would suggest that we take this.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/14/2018,3:00,no
245,69,78,Timxxxx Cxx,2,"(OVERALL EVALUATION) This paper describes an effort to perform author name reconciliation (with six external LOD name authorities - ECONIS [as source of GND ids], Wikidata, VIAF, RESPEC_ID, DBPedia and ORCID) for authors of papers held by the Open Access EconStor repository. While this is a highly relevant domain for JCDL, the project as described is not especially sophisticated, novel or original by today's standards, and the presentation of the work in this submission is confusing and not well organized. (In small part this latter problem can be blamed on several minor grammatical issues and poor vocabulary choices scattered throughout the paper, but on the whole the writing was okay for this stage of the process. Would need improvement if accepted.) The first half of the paper and nearly all of the references primarily relate to establishing the desirability of name reconciliation for OA repositories. This argument, to the extent it is still necessary today, could have been made more succinctly, especially given prior art available to reference. (The references used were okay, but not the best to establish this point and did nothing to help bolster or provide foundation for the experiment and proof of concept discussed - a missed opportunity.) Also, while the experiment sought to find identifiers in 6 resources, Section 4 on datasets only discusses two - arguably, as it turns out, the only two that matter.  The real meat of the paper doesn't start until page 3. The numbers in the opening paragraph of Section 5 (111,107 documents and 218,185 authors associated with with EconStor publications) are confusing when juxtaposed with the numbers in the first paragraph of section 3 (150,000+ publications and 100,000+ authors). This may simply be a matter of counting distinct author names associated with the corpus versus counting author names derived from the corpus inclusive of duplicate names, but it adds to the confusion. Reconciling authors by matching publications common to ECONIS and EconStor is a reasonable albeit not a new approach. The decision to focus through the rest of the paper on 'prominent' EconStor authors is never explained nor defended; it's not clear this is a good focus or figure of merit. The extremely low percentage of match for GND-identified names in all sources other than ECONIS, especially the low match in VIAF (which integrates the DNB authority file) is counter-intuitive and warranted more discussion. The SPARQL query included (Listing 1) also needed more explanation. The search strategy in Wikidata, and the logic / development effort behind it (including testing of alternative strategies), is not at all clear. The decision (if I am understanding correctly) to rely on Wikidata to discover all other LOD identifiers also warrants discussion.  Essentially this approach makes superfluous for this paper all but ECONIS and WikiData. The discussion of the Author Profile page proof of concept is okay, but doesn't make up for the other shortcomings. There is some potentially interesting work being discussed here, but in contrast to ongoing name reconciliation work elsewhere it seems incomplete or at least immature. Possibly appropriate to discuss in a Poster, but not up to expectations for a JCDL short paper.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/15/2018,23:14,no
246,69,125,Nuxx Frxxx,3,"(OVERALL EVALUATION) The authors present an interesting and relevant work for the digital libraries community. However the work description in the paper disperses over many different aspects and research problems, and, as a result, none of the research problems are covered in enough detail to make a strong scientific contribution for publication in a scientific conference as JCDL.

The general problem addressed in the paper is indeed relevant and current, because of all efforts being done in Europe with CRIS systems, and evaluation of research results, where unambiguous references to authors are an essential component. 

The approach presented in the paper focus on a technique based on identifiers for linking authorship data in the repositories with external datasets. 
An approach just based on linking by identifiers is not very innovative and covers only a portion of the authorship data, however. There is much related work with more comprehensive approaches.
Nevertheless, the paper could still make a scientific contribution if the study was more focused on the analysis of the identifiers in the data sources, but the paper does not focus enough on it. 
Other problems include some lack of essential detail on the linking aspects. Section 5.1 makes just a narrative description of the data used, making the approach impossible to reproduce. It also omits details on how the matching of names is done.

Another weakness of the paper is that the evaluation is very superficial. It shows that some level of linking can be achieved with this approach, but it does not indicate how comprehensive and accurate it is.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/17/2018,9:48,no
247,70,382,Douxxxx Tuxxxx,1,"(OVERALL EVALUATION) This paper describes an interesting pilot project arising from a COST action towards a European database for social sciences and humanities databases. The technical solution proposed builds upon an existing Finnish data hub of this kind, tailoring the technical approach for a European dimension. Appropriate standards and methods are followed. Automatic ingest from local CRISes via an XMLschema seems an appropriate basis for the work. Some 50,000 reference were integrated into the pilot database.

The work is interesting but at a very early stage, not ready for a full paper at JCDL. Furthermore, the paper is very short, only 5 pages (with only 12 references). This is significantly less than the anticipated treatment of a full paper (up to 10 pages). 

The authors could consider a poster at this year's conference and submitting a full paper after further development to a future JCDL.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) If there is scope to consider as a short paper then this could be a possibility (but realise probably too late for that).
The authors could be encouraged to submit a Poster presentation on the pilot work.","Overall evaluation: -3
Reviewer's confidence: 3
Recommend for best paper: no",-3,,,,,2/2/2018,16:03,no
248,70,4,Juxx Abxx,2,"(OVERALL EVALUATION) The topic is within scope of JCDL and describes a proof in concept for developing a digital repository of European Humanities and Social Science publications modeling the system on the existing VIRTA system. It would be useful to the JCDL community to learn more about the issues they encountered and their solutions for dealing with each but these are only briefly described in the paper. The authors discuss interoperability and quality issues of the existing system, as well as the issues encountered when matching CSV files between VIRTA and the VIRTA-ENRESSH-POC. The submission was only 5 pages and could be expanded to discuss these issues and others the authors mentioned that they would present at the conference but this revision would be significant and require a re-review by the program committee. I would recommend that the authors resubmit the paper when it is fully developed.

There are also a few formatting issues that could be resolved if allowed to resubmit as a short paper, such as the copyright block being out of place and the reference entries in all caps.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This long paper submission is better suited for a short paper or poster. The work is incomplete and the authors state at the end of the paper what they will present at the conference that is not in the paper. The paper presents a proof in concept and not a finished project or research study, making it more suitable for the short paper category. The submission is only 5 pages and while it eludes to what will be presented at the conference, these important aspects if included, would have strengthened the paper and should have been included.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2/15/2018,17:35,no
249,71,169,Daxxxx H,1,"(OVERALL EVALUATION) The paper propose a model(TDQS) for query suggestion diversification for time-aware queries. The method contains two steps: generating time-aware query suggestion and diversify the suggestion. It not only consider the diversification of topics but also time. Preliminary experiments have been done in AOL dataset with comparison with two baseline methods.
The proposed method achieved the best performance among them.

Strength:

Temporally ambiguous and atemporal queries will benefit from such the proposed model. The provided example about presidential election clearly demonstrated the point.
The paper provide preliminary results for the proposed method and results show that the proposed method is better than two baseline methods.
In general, the paper is written clearly. More aspects of the TDQS could be analyzed and extended to a longer paper in the future.

Weakness&Question:
Some description is not clear. For e.g: what time-series analysis methods have been used to smoothed the relevant score. Maybe a longer paper is better in terms of giving more details.
How large is the query-URL graph for each day in the training set.
Missing a ?€?P?€? in equation 1. The ?€?Q?€? in ?€?Generating time-aware query suggestion?€? and the ?€?Q?€? in ?€?diversifying query suggestion?€? have different meaning? If so, this is not recommended.
Please explain orthogonal but not independent aspects in the ?€?future work part?€?.
It would be interesting if there is any example about the similarity calculated by word2vec model.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/12/2018,15:01,no
251,71,36,Joxxxx Bxx,2,"(OVERALL EVALUATION) The authors propose a method for time-aware query suggestions and evaluate their approach on some AOL logs from 2006. 

Overall, the paper is nicely written and easy to follow. However, there are significant shortcomings.

First, I am sceptical how relevant an evaluation based on 10-year old logging data is. Ten years ago, websites were vastly different from today's websites -- no html 5, few mobile devices, much slower internet connections, no responsive webdesign, ... It also almost ironic that the authors use the query string ""Flash"" as an example -- a technology that was widely used a decade ago but that has no significance nowadays anymore. 

Second, the presented work was not conducted in the context of digital libraries. Frankly, I feel the authors have written this paper originally for an Information Retrieval conferences, where it was rejected and now JCDL is the fall-back solution. I feel the authors should have at least explained how their work is relevant for digital libraries. Similarly, there is not a single reference to a journal or conference relating to digital libraries. The references of the manuscript exclusively point to IR and AI venues (in particular SIGIR). 

The authors did a significant pruning of the dataset (removing queries starting with special strings, remove redundant white spaces ...). While the actions seem plausible, I wonder how many queries and users remained of the original 16m queries. 

Figure 1 shows a very strong peak for MRR at Alpha=0.6. I wonder why this is -- the authors should discuss this and maybe redo the analysis. 

Figure 1 displays ""accuracy"" as the evaluation metric, but ""accuracy"" is not mentioned anywhere else in the manuscript (only MRR and nDCG). It is unclear if accuracy refers to one of the other two metrics (if so, which one), or, if not, how accuracy was measured. 

Given the old dataset, the weak related work section and rather limited relevance to JCDL, and a partly vague evaluation (metrics; pruning), I recommend rejecting the paper. However, I want to ensure the authors that I see the potential of the approach and hope they will re-submit next year to JCDL.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/19/2018,11:57,no
252,71,162,Kisxxxxx Haxxx,3,"(OVERALL EVALUATION) The authors present a system for suggesting candidate queries that are diversified in terms of subtopics and time.

Relevance to JCDL: I am a bit concerned about the relevance of this paper with respect to JCDL. The system is quite generic in nature, has been tested with AOL data, and has very little to do with scientific documents. I feel the paper would have been more appropriate for IR venues such as WSDM, WWW, or SIGIR.

Novelty/Originality: Although the authors present the idea of incorporating 'time' in the framework, the proposed algorithm is similar to the one in ""Diversifying search results."" Agrawal, Rakesh, et al., WSDM, 2009. Although that algorithm was applied to diversify search results instead, the approach is quite similar. The authors should address this in related work.

Methods:
They have presented a probability based utility function to score the candidate queries in terms of topical and temporal diversity. I have the following concerns.
1) The parentheses are not matching in equation 1. Also I think P_t is missing. It would be clearer if the multiplication is shown with '*' or '.'.
2) Line 193 should be rephrased as it gives an impression as if the model is personalized per user, but it is not.
3) It would be better to introduce \bar{S}, and \bar{T} formally.
4) Although the approach seems interesting, I am wondering shouldn't the diversification approach be applied only for those queries which have some sort of temporal ambiguity? Not all the queries need to fall in this category necessarily. In that case, how should we define temporal ambiguity quantitatively?

Evaluation: 
The authors have used AOL data and have done manual evaluation in terms of MRR and NDCG, which is reasonable given the difficulty of automating it. I have the following concerns about the evaluation section.
1) Ideally the system should be used for only those queries which are temporally ambiguous. The choice of test queries is not clear to me. Some of the examples shown in Table 1 do not seem to be obvious (e.g, `ebay').
2) Some example suggestions made by the system should be presented. Right now it is very difficult to imagine what the temporally diversified query suggestions would look like for say 'harry potter' or 'ebay'.
3) Assuming most of the users are interested in the recent events only (e.g., olympic (issued in 2006) might refer to the winter olympics 2006), is the AOL data enough to run similar experiments as it only covers 3 months of 2006?
4) In the introduction (line 93), the authors mention that both temporally ambiguous, and unambiguous queries would benefit from their method. However no such evidences were provided in the experiment section.

Style/Quality of writing: The paper is well written in general. The authors could have avoided repeated usage of 'aspects (subtopics)'. Some sentences are very long (e.g., line 239)-- would be better to rephrase. In particular, the sentence starting at line 93 ends at line 103, spanning 10 lines! Please break it down to smaller sentences.

Typo:
1) line 355: PQSD => TDQS
2) The CCS concepts are wrong. I am guessing these are the template ones.

Missing References:
1) Agrawal et al. ""Diversifying search results."" WSDM, 2009.
2) He et al. ""Learning to Rewrite Queries."" CIKM, 2016. - I feel there is a connection between rewriting, and suggesting query. It would be good to discuss since there is space.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/21/2018,11:35,no
253,71,203,Jaxx Kaxx,4,"(OVERALL EVALUATION) The short paper investigates query completion, and in particular time sensitive diversification of the suggestions.
      
      There are some clear strengths:
      
      - The general problem of query completion is of key relevance in any operational DL or other system.
      
      - On dynamic collections like the web, temporal changes and trends are well-known, and capturing recency and other trends is of key importance -- and standard practice for long in all major search engines.
      
      - Likewise, on topically diverse collections with a multitude of different users and uses, personalization and diversification of query completions is of key importance -- and standard practice for long in all major search engines.
      
      There are some clear limitations:
      
      - The use of a rather dated query log (i.e., AOL) significantly limits the relevance and applicability of any findings for the current state-of-the-art.
      
      - The use of a log from a, although dated, industrial strength search engine leads to a combination effect, as the search engine used highly optimized query completion (of a far more advanced nature than the proposed approach).   This means that we observe only the combined effect -- and a small increase due to the different optimization objective in the original search engine, and used measures on the filtered down subset of the experimentations.
      
      - The results are rather unexpected, and there is unclear generalization to specific DL applications -- often plagued by less query volume, and many more (long) unique queries.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) No major contribution, but a topic that will generate some useful discussion at the conference -- so a good poster.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,2/23/2018,11:31,no
255,73,178,Hexxx Hocxxxx,1,"(OVERALL EVALUATION) This paper is relevant to a number of topics in the JCDL 2018 cfp, including e.g. data curation/stewardship, information retrieval and preservation. It is a well-structured and well?€?written paper, which introduces a new framework for aggregating private, personal and public web archives while addressing information privacy. It also includes detailed descriptions of how this can be achieved by amending the Memento syntax and including additional semantics. The framework allows personal web archiving effort to be ultilised to provide a more representative impression of the historical web. The methods and the discussions are both sound and comprehensive.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/19/2018,16:24,no
256,73,94,Mixxxx Dobrexxxxxxxxxxx,2,(OVERALL EVALUATION) Clearly written and well illustrated submission on expanding Memento with solutions allowing to aggregate private and mixed private-public Web archives. The topic is definitely relevant to the conference community.,"Overall evaluation: 3
Reviewer's confidence: 3
Recommend for best paper: no",3,,,,,2/16/2018,6:26,no
257,73,260,Roxxxx Hx,3,"(OVERALL EVALUATION) This paper builds on previous work in the Memento line by looking at novel solutions to aggregate public, private and personal web archives. In this context the paper looks at aggregation across various types of internet archives (Personal, Private, Public) for use with that of Memento-style aggregation of identifiers and not the ?€?aggregation?€? of representations in the Web Architecture process used to assemble a web page. The paper build on past memento research by contributing in the following areas of web archiving practice: 

Archival Query Precedence and Short-circuiting
TimeMap/Link Enrichment: 
Multi-dimensional user-driven content negotiation of archives
Public/PrivateWeb Archive Aggregation

Overall a very well written paper that covers the technical background on the Memento framework while outlining novel methods for use of this framework in aggregating personal, private and public Web archives for a more complete picture of the context of content over time.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This is an extremely well written paper that looks to extend use of memento architecture for creating a more complete and context sensitive picture of Web archives from a personal and customized standpoint while utilizing public Web archives to complete the content aggregation for the end user.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: yes",3,,,,,2/16/2018,19:10,no
258,73,142,Bexx Gxx,4,"(OVERALL EVALUATION) I recommend this paper for acceptance at JCDL. The authors propose a framework for aggregating private or personalized Web archives with public Web archives, while taking users?€? privacy considerations into account. This idea is novel and of high relevance to web archiving and digital library communities.

The topic is also current given that personalized and private Web archives are becoming more prevalent and we will soon require methods for securing the aggregation of various types of Web archives while also addressing privacy concerns.

The paper?€?s primary contribution can be defined as the amendment of the semantics of Memento TimeMaps to encourage aggregation of captures from multiple archives. As such, the work builds upon the authors?€? prior work, however, prior Memento aggregators could only query a static set of public Web archives specified at configuration time. The paper itself claims four individual contributions, where each is presented in its own section. The contributions are: (1) querying the archives of an aggregated set in a defined order with the series halting if a pre-defined condition is met, (2) adding attributes to identifiers for captures (URI-Ms) for more efficient querying (3) increasing user involvement in requests for URI-Ms, and finally (4) introducing a method for public/ private Web archive aggregation.

The only critique I have is the within Background and Related Work, the section on Privacy and Security could go into greater depth. Especially since this is a long paper with the stated aim of taking into account ?€?privacy considerations?€? in Web archiving, the scope of the privacy and security section should be significantly expanded upon.  

At times, the paper is unnecessarily wordy. Concepts are not always described in the most straightforward way. There are several instances where the language is convoluted or vague. For example: ?€??€?for the process of accessing Web archives of a variety of sorts to encourage concepts in Web archives like negotiation in additional dimensions beyond time ...?€?. Note that this is an excerpt from a sentence spanning 6 lines, which brings me to the next issue: Sentences are far too long, often spanning 5-7+ lines. For the sake of clarity and readability sentences must be shortened. Because of these issues, I found the paper a cumbersome read despite the interesting topic.

Other feedback:

-  The abstract is written rather vaguely. For example: ?€?We provide a means of involving the users further in the negation of..?€? (what means? how?) Or: ?€?..introduce a model for .. as appropriate?€? (what is the method? What is the definition of ?€?as appropriate?€??) Especially given that this is a full paper, I would urge the authors to make their key contributions very specific in the Abstract.

-  The description of the 4 contributions of the paper on p. 2 and 3 appear too late. Ideally, the purpose of the paper should become clear to the reader at the end of the main Introduction section.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,Corinna,Breitinger,breitinger@sciplore.org,117,2/17/2018,22:18,no
259,74,38,Toxxxx Blxxx,1,"(OVERALL EVALUATION) This paper provides a fascinating account of how OCR error influence a user-centric IR measure 'retrievability'. It is novel in the sense that it provides a detailed account of a newspaper collection but also because it offers clear insights into the methodological background. Finally, it advances existing, published research by the authors. I especially liked the idea to use GINI and Wealth for the evaluation. I will see how this works for my own experiments in the future. Just for reference, it is actually already used in Machine Learning such as Decision Trees.

It is refreshing to read a real digital libraries paper. A couple of small comments. Please review format and language. I found nothing major but here and there improvements can be made. If there is something to add, I would work more on justifying the collections that you used. What would be required in the future to make this approach work for larger collections? Finally, a summary paragraph comparing results and pointing to next steps would be useful.","Overall evaluation: 3
Reviewer's confidence: 3
Recommend for best paper: yes",3,,,,,2/14/2018,11:54,no
261,74,88,Gioxxxx Maxxx,2,"(OVERALL EVALUATION) This paper presents a study on the impact of OCR corrections on the retrievability of documents.
The paper is well written and well organised.
My only concern is about the impact of the results: the strong correlation between OCR errors and retrievability is more than predictable, the quantification of the effect is appreciable though.

Please find hereby a few suggestions to improve some parts of the paper:

In Section 4.1, I would present the original corpus (the one with errors) *before* the corrected corpus. 

Figures are nicely plotted (I appreciated the histograms at the two sides of the plot) but the shape of the marker does not help to visualise where the different categories of points are (it is difficult to detect any difference between to crosses that have a very small difference in size). I'd suggest to use different shapes, not different sizes.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/18/2018,22:48,no
262,74,29,Daxxx Baixxxxxx,3,"(OVERALL EVALUATION) This submission presents work undertaken in establishing the effect human correction of OCR errors has on retrieval of document in electronic resources such as digital libraries.  The analysis work centers on studying retrievability bias in a set of newspapers from a collection from the National Library of the Netherlands.  The work continues, but it different from, work previously undertaken by the authors. 

Overall the submission is clearly written, and well structured.  The results presented are detailed and of interest to the DL community.

In reviewing the article, there were some issues encountered that could be improved with modest editing.  In the related work section, reference to ?€?our corpus?€? and ?€?in our case?€? confound the discussion, because it has not been described by this point in the article.  In describing Ohta et al.?€?s work, you say what they were looking to achieved, but don?€?t report on what their findings were.  The text gets a bit repetitive in places, such as the fact that the reported work was based on Azzopardi et al.?€?s concept of retrievability.  

There is space for some of the figures to be made larger.  I would suggest this is at least done for Figure 1, as this would help the reader more easily see the structure of this type of plot, which is then used again in some of the later figures (Fig. 3, 4 etc. even if they themselves are not produced to the same size).","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/19/2018,9:30,no
263,75,177,Anxxxx Hixx,1,"(OVERALL EVALUATION) positive:
+ clear description of addressed goal
+ the approach is evaluated with experimental testing


negative:
+ summary of approach in abstract is not clear enough - either clarify or leave it for the main body
+ the argument for why such a ranking is necessary is somewhat unclear.
+ for an argument on user-facing applications, one would expect an evaluation including users 
+ the related work seems to discuss methods for keyphrase extraction, not methods for comparing the quality of keyphrase extractions/rankings. Where it does, it is not clear exactly how those other measures compare to the one you are proposing.


minor: 
- remove the footnotes on the first page (????€??€?)
- figure 2 -> Figure 2 (etc.)","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,1/26/2018,5:14,no
265,75,391,Jenxxxxxx Wxx,2,"(OVERALL EVALUATION) This paper proposes to re-rank the scores of keyphrases extracted by 
existing keyphrase extraction methods by TF-IDF weights.

In general, the method is not innovative.
Also, th experimental results cannot justify the contribution of the paper.

The major issues include the following:

1. There's a lack of novelty in the keyphrase scoring approach in sec.2.
Eq.(2) adds a penalty term in the token score, but the effects were not evaluated in the experiments.
It's not convincing to add such a term.

2. The major contribution of the paper is not clear.
Although it's identified in Fig.1, there's no corresponding evaluation in the experiments.
For example, how would you compare the different keyphrases extraction methods in this framework?

3. The experimental results in Sec.3 are not convincing.
Specifically, Figs.2 & 3 compare the results by the proposed ranking and random ranking.
Is there a ranking for Rake and other methods?
If so, why not compare with Rake and other keyphrase extraction methods?

4. It's not clear how to deal with multiple keyphrase extraction methods 
with the proposed ranking.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/20/2018,3:14,no
266,75,308,Anixxxx Prxxx,3,"(OVERALL EVALUATION) Summary of the review: This paper proposes rescoring primary keyphrase extracted by ""any"" algorithm to get a new re-ranked list of keyphrase. The ranking is done as penalized summation on the tf-idf of the tokens in the keypharse. The new ranked list now can be considered as the new ordering of keyphrases (best being at the top). In short, the work presents a compensation technique for the length of the keyphrase. 


Relevance to JCDL: The problem of keyphrase scoring and hence improving keyphrase extraction performance (in a given budget) is a prime information task relevant to many digital libraries indexing and search. Hence, the work is highly relevant to JCDL. 


Novelty/Originality: 
The idea of compensating for the length of keyphrase is not new and is well explored in terms of feature engineering. many of the systems in SemEval 2010 task use keyphrase ranking with a varied set of candidates where the extracted features included the original tf-idf of the tokens and the length. Further using POS, chunk and other kinds of parse as features can also result in compensating for the varied length of the keyphrase. Such features are also widely used in many systems.    
The only novelty is the introduction of a heuristic formula for getting the score which includes the penalized addition of the tf-idf.


Assessment/Evaluation/Comparison:  The assessment seems reliable and the results are believable. However the results inadequate to substantiate the claims. As mentioned in the previous section of the review the central hypothesis is a well-exploited fact. Even if we assume that the re-ranking formulation is the novel and central contribution, it should be shown working on at least 2 baselines. The baseline (Hulth 2003) used here is also particularly weak and doesn't use rich features. 
The result of performace@K (shown as graph) clearly shows that the proposed model works better than the baseline, but since the baseline doesn't use tf-idf and length features, the result might be just proving that it's a good feature which can be incorporated in the baseline. In short, this same experiment needs to be done on a baseline already using some sort of length normalization feature or indicative rich features.


Style/quality of writing: The writing is good and overall the work is a pleasure to read. Different portions of the paper like motivation, background, technique etc are well weighted. related work needs to cover up more recent advancement in the field keyphrase extraction (including ranking). Related work only covers graph-based keyphrase extraction techniques, whereas there exists numerous other unsupervised, semi-supervised as well as supervised featured based techniques. In general keyphrase extraction paper its sufficient to cover single line of argumentation/work which aligns closely with the claims. However, the author claim it's a method agnostic scoring and hence all the types and at least one example of two different types of extraction methods should be covered in details.


Replicability: The experiments are generally replicable. The dataset is public and the experimental details are adequate to replicate the setup. 


Adequacy of references: The references are inadequate and miss some of the most important advancements in the field.


References:

[1] Hasan, Kazi Saidul, and Vincent Ng. ""Automatic keyphrase extraction: A survey of the state of the art."" Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2014..

[2] Kim, S. N., Medelyan, O., Kan, M. Y., & Baldwin, T. (2010, July). Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles. In Proceedings of the 5th International Workshop on Semantic Evaluation (pp. 21-26). Association for Computational Linguistics.

[3] Nguyen, Thuy Dung, and Min-Yen Kan. ""Keyphrase extraction in scientific publications."" International Conference on Asian Digital Libraries. Springer, Berlin, Heidelberg, 2007.

[*] In addition to these, there are numerous other works (post-2015) on keyphrase extraction modeling specific aspects for certain tasks like scientific documents, twitter, AKB generation etc or/and modeling certain ML behaviors like using RNN, CNN etc.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2/19/2018,21:59,no
267,76,124,Nuxx Frxxx,1,"(OVERALL EVALUATION) The authors present an interesting and relevant work for the digital libraries community. However the work description in this paper may not be mature enough for publications. In general, the paper is omissive about some parts of the approach, and lacks justification/discussion for some of the options taken by the authors for evaluating certain aspects. These flaws make the scientific contribution of the work hard to interpret and brings doubt about its reliability. Even for this review, some technical details could not be verified because of this limitation.
In my opinion, the authors appear to have done all the necessary steps and evaluations to enable the publication of the work, however, publishing the work as it is in the paper, would be 'wasteful' of their effort, since it would not properly enable future work based on its conclusions.

A specific difficulty in interpreting the paper is in Section 2, which makes a good and comprehensive description of the state of the art, but it lacks connection with the approach of the authors. Which of the mentioned research is this approach trying to improve, or provide a motivation for it, or influence the choices of the similarity techniques evaluated, or is directly related for some other reason?  

A major difficulty in interpreting the work done and its results, is the very first part of the process undertaken - the 'paragraph retrieval'. It is not clear to what extent it is actually part of the approach, or was done just to gather the evaluation data set. It is presented as a part of the approach, but parts of it are not explained and evaluated. The description of the approach, in Section 3, does not describe the approach fully. It misses a subsection describing the part about the question types: how is the questions type assigned to a question?; and what elements are used to filter the paragraphs for each of the types?

In the evaluation section, the motivation for comparing against [6] is not explained with other QA approaches is not described at all. The evaluation against [6] was done in a corpus gathered for the paper approach, it should be discussed if this could have affected the evaluation results. 

The formal formulas for the evaluation metrics uses variables that are not defined in that section (or actually in the whole document). Nevertheless, these are well know metrics, it would be of greater value to describe their relevance for this work, and refer to descriptions of the metrics is the references or footnotes.

Some 'editorial' remarks:
title:
the use of acronyms (the QA) should be avoided.

abstract:
line 2 - 'accessing' has an unclear meaning

section 1:
Last paragraph mentions the sections with roman numerals, but that is not how sections are numbered under the ACM template.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/13/2018,8:43,no
268,76,20,Suxxx Alxxx,2,"(OVERALL EVALUATION) Relevance to JCDL: The topic of this research is relevant to the JCDL community since it deals with using NLP for automated question answering. This is an area of interest to many in the community.

novelty/originality: The use of textual entailment for validation is useful but doesn't seem particularly novel. In addition by focusing on factoids and an exact answer  this seems to be an effort moving towards an inflexible system that does not reflect the cognitive processes of people which are not as constrained. Perhaps the author's can add a stronger argument about the importance of their work.

methodology: The approach is well-described and rigorous. 

assessment/evaluation/comparison: While the evaluation of hte outcome is thorough - the paper would benefit from discussing the results in terms of answering the ""so-what"" question. 

Style/quality of writing: The writing is grammatically acceptable and seems well-edited.  However, the authors depend heavily on jargon and often do not provide opportunities to follow the overall flow of the paper with plainly written sentences.  While jargon is important for experts to read, one value of the JCDL is a coming together of diverse communities to share ideas beyond their own expertise. In this regards, effective science communication is needed and this paper would benefit from more attention to doing this.

replicability: given the thorough discussion of methodology it appears it could be replicable,

Adequacy of references: References appear adequate.","Overall evaluation: 0
Reviewer's confidence: 2
Recommend for best paper: no",0,,,,,2/16/2018,13:39,no
269,76,426,Zhxxxx Zhxx,3,"(OVERALL EVALUATION) This paper studied the question answering problem from factoid questions. To that end this paper proposed a system that includes: using google to return candidate paragraphs, using text entailment method to identify relevant answers, and use four similarity scores to rank the final answers. 

There's no doubt that I believe this proposed framework will work well in practice. However, as a scientific paper it lacks some other important ingredients, which finally makes me ""weakly reject"" this submission.

1.The dataset problem.
If I understand correctly, this paper used two datasets: one with 1370 questions that are used to analyze the proposed algorithm, and the other with 400 questions from TREC to compare with other methods (i.e. the Merged Feature Scorer). 

There are two problems with this setup: first, there are no explanations about how the first dataset is collected. I think it is a dataset that authors themselves collected for their purposes. In this case, the authors should consider open-source it or list some examples in the paper so that readers can get at least some sense of it. Second, I'm afraid the comparison with other methods should be done on both datasets, rather than just the second dataset as it is now in the submission.

2. Using Google in the proposed method.
I'm not sure if this is an appropriate way of doing scientific publications. I mean, it is good enough for practical, engineering considerations. But I rarely (if not never) see a paper using Google in its framework. The major issue here is, it is probable that it is the powerful Google that makes the proposed method works so well. If you switch to other search method (e.g. let's say a simple TF-IDF or lucene search engine on an academic corpus) then the proposed method (which is the major contribution claimed by the authors) might not work so well. 

3. The comparison with other methods is not enough
I can see that the proposed method only compares one other method, which is the Merged Feature Scorer method. On one hand, this might not be a strong baseline to compare with. On the other hand, that method is yet another similarity scorer, which is of the same type as the proposed method. There are numerous latest publications in this field, including various algorithm types such as IR based, NLP based, neural network based, etc. The comparison should be broader and more. For example, the following work is recommended for comparison:

 A Neural Network for Factoid Question Answering over Paragraphs, EMNLP'14.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/16/2018,18:49,no
270,76,344,Daxxx Smxx,4,"(OVERALL EVALUATION) The authors present a system for open-domain factoid question answering and evaluate it on 1370 test questions, using snippets from Google as the corpus from which to draw the answers. The paper suffers, however, from a vagueness about its relationship to the extensive prior work in question answering. It's true that the paper contains a literature review back to the 1950s, but the paper does not compare its proposed method to any prior work in particular. Similarly, the experiments compare the current approach to only one other system, on a smaller test set of 400 questions. The comparison system (Merged Feature Scorer) is not said to be state-of-the-art on any particular task. The question, therefore, is how a reader should consider the proposed system in comparison to the many other approaches to question answering. Does the current system perform particularly well on some task compared to widely recognized baselines? Or does the current system possess other advantages in what resources it uses, etc.? The authors should clearly make the case for their approach.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/20/2018,12:54,no
271,77,112,Schxxxxx Fx,1,"(OVERALL EVALUATION) Paper attempts to provide framework for extracting answers to address questions of various categories found in online forums.  Due to the many facets of the approach, most of the paper focus on the techniques employed to address these facets making up the framework leaving limited room to present and discussing the findings in Section 4.   There are a notable number of issues detected in the paper:

Section 1. It is better to present the problem statement and background of the study. Avoid discussing about your approach and comparing it against others in this introductory section.  You mention about a ?€?question-driven approach?€? but this does not seem to be expounded sufficiently in Section 4. 

Section 2.  The related work section should focus on previous work done on this area of research, not so much as your research (E.g. ?€?Somewhat more closely related to our work ?€?  We adopt a different strategy..?€?.  You are basically describing your work in a Related Work section which generally should be presented in the Methodology section.   There is a need to rewrite these first 2 sections to make it more precise and appropriate. 

Section 3.  Figure 1: What are the ?€?1?€?, ?€?2?€? and ?€?3?€? labels mean? Be careful to ensure the modules are correctly referenced in the text.  ?€?answer configuration module?€? ??? Question-Answer construction module; ?€?answer inspection module?€? ??? quality answer inspection module. You need to describe the work of [26] in more detail as the reader does not know what is been done previously.  E.g. [26] by augmenting each question class with the expected answer ?€? what information was stored in the question class in [26], what is meant by ?€?expected answer?€?. In cases where there are no answers, how is this addressed?

Section 3.1.  It is not immediately clear where this is implemented in your system architecture until Section 3.3 (f) and 3.4 are described more fully. 

Section 3.2.  You mean ?€?equation 2?€? not ?€?equation 1?€?.  There is a need to explain Tables 1 and 2 better, especially the Question category (is it the horizontal or vertical?)

Section 3.2.1 is hard to follow.  ?€?a weight value is assigned?€?  How?  Some keywords are automatically added ?€? include digit, uppercase words.. not found in the WordNet dictionary?€?  Equation 4 suggest you are adding words from WordNet!

Section 3.4.  ?€?AE = Fanswer..?€?  AE is the answer, and Fanswer is a computed score ?€? how can the 2 be the same as shown in Equation 9?  What is the range of possible scores for text quality?

Section 3.4 ?€? repeated twice.

Section 3.4.1 versus Section 3.4.2  Seems contradictory to each other the way 3..4.2 is worded.  You mean where there is no related questions and no distinct answer found for a specific question.

Section 4.  This section is difficult to follow. E.g. The text group analysed for answers ?€? adopted (what is adopted?) from [25,26].  The number of patterns and keywords generated ae varied (How?) to observe the performance of the system. The size of the group is 1000 (why ?€? on what basis?), etc. 

Table 5a versus Table 5b.  Why is Row 3 different for the 2 tables?  Why not use the same example for Row 3 across tables?

Section 4.1  ?€?The table also shows that there is a total of 229 questions..?€? where?

Table 5a.  Totals don?€?t add up ?€? did you miss out ?€?No answers?€? from the column?

Table 6.  I am not sure if the values of Table 6 is correct. If P and R is less than 1, then C and B have a value (Equations 24 and 25), so Accuracy (shown in Equation 26) cannot be 1.000 unless I am missing something here.   I am also not sure if this is the way to tweak the 90% pattern and keyword threshold value to demonstrate the ?€?impressive result?€?.  Rather, you want to present your results in different % threshold values.

Table 8. Again this is in suspect of how this can be compared to work done by [14] in 2009, as their dataset used, assumptions, etc. could well be different from what you are doing.  I would suggest deleting this section unless strong justifications is given for it to remain.","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2/8/2018,4:46,no
272,77,247,Gaxx Marxxxxxxx,2,"(OVERALL EVALUATION) This paper describes a strategy for mapping answers to questions posed to question answering sites.  The authors describe a architecture that extracts ?€?answers?€? from a corpus (ResearchGate was used as the test corpus), extracts questions from the corpus and classifies them to aid in mapping to answers, and conducts a set of quality assessments and ranking procedures to map questions to answers.  A number of techniques are applied to a variety of text features.  Although many of these techniques are supported by literature (e.g., KL-divergence is used for similarity, Flesch metric for readability), the overall suite of techniques and features seem more like a set than a carefully crafted collection with respective weights and an overall rationale for fit.  The authors note that the threshold value for association in the KL divergence is important and tunable, but do not explain its use in the implementation.  The work depends crucially on a classification of answer types and query types but these are complex issues that should be explained (there is a reference to two papers for answer type and one paper for question type).  Consideration of authorship as a feature is useful (and typical) but simply depending on ratio of replies and posts as a proxy for authority seems wishful and certainly would not apply in questions of opinion rather than the known item types illustrated in this paper.  The relationship between two questions (formula in 3.4.1) uses author1 as a parameter---it seems arbitrary---why not author2 or some co-occurrence relationship between the authors?   It is simply wrong to say that ?€?if there are no answers for a given question, and there is not relationship with other questions, it is understood that the question is incorrect, irrelevant or such question does not require an answer.?€? (section 3.4.2).  You are saying that if your system does not work, it is the question?€?s fault!   The notion of text quality is similarly treated casually.  It seems to conflate typos and the information theoretic estimate of related terms based on word distributions.  Visual quality, grammaticality, and readability are sensible features to include but they seem to be equally valued in the similarity modeling.  
The evaluation is awkwardly explained and there are several obvious statements (more keywords and patterns will result in more answers to be identified because there is more data available to search for answers).  The main results shown in Table 6 are quite telling---you are basically demonstrating that your system can pick some questions from a corpus of questions and match them to the answers that people gave to those questions in the corpus.  The attempt to assess the value of the different features (Table 8) is not informative.  
Overall, this paper is very hard to read (needs significant copy editing and a coherent rationale for why this set of techniques and features were brought together).  It requires a more extensive evaluation (perhaps the plan to use TREC QA will help in the future).

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I would like to have Nigeria represented at the conference, but this paper is just too problematic","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2/8/2018,20:07,no
273,77,83,Daxxxx Haxxx,3,"(OVERALL EVALUATION) The authors provide an approach to extract answers from a Question and Answering website (ResearchGate). They have provided an interesting solution to this problem using NLP techniques. However, there are some issues which are important to improve the article.

Authors evaluated the results using a Research Gate dataset.  However, the parameters ""Percentage of pattern and keywords considered"" (cf. Table 5a and 5b) were discovered in the same dataset which they evaluated. This is not a good methodology as can lead to overfitting. Because of that, the high results obtained from this method could not generalize to other datasets. The authors should use a part of a dataset as a validation set in order to avoid this problem. In addition, the authors did not mention the value of the beta threshold (Eq. 12) during the experiments.

Some equations are confusing. For example, there is a lack of formalism in the Equation 13, making it difficult to read. The Equation 12, as far as I understood in the text, presents the set of answers related to a question. However, this equation should use set notations to be clearer. In addition, Equations 12 and 8 use the same output: answer(S_{q_i}). Regarding the proposed method name, authors use the acronyms QB, PB and QB+PB without defining them.  

The study is promising, however, more experiments need to be done (using a validation set to discover the parameters, and evaluate the method in others datasets) in order to ensure the obtained results.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/16/2018,15:35,no
275,77,234,Jxx Hx,4,"(OVERALL EVALUATION) This paper investigates online discussion forums as a potential source for extracting answers to user questions, specifically focusing on ResearchGate website. While the general idea behind the study is quite interesting, I think this paper can really benefit from authors explaining some of the details better: for instance, the authors state that KL-divergence model is the appropriate one rather than using Cosine Similarity or Query likelihood model, but why? They explain why the other two are not appropriate but does not tell the readers why the KL-divergence model is, other than stating that it is appropriate. The authors provide a fairly detailed explanation on how the answers are extracted from the questions through multiple steps but I am unclear about how how exactly they determined that the answers were ?€?right?€? or ?€?wrong?€?. The earlier sections talk about determining the existence of an answer and also about different aspects examined to determine the text quality, but that does not tell us if the answer is accurate or not. Could another approach (maybe a crowdsourced approach) help better determine the accuracy of the answers in the evaluation phase? 

Now the minor (but still important) points:

Really consider putting the author names before the bracketed number in the text. Simply referring to the numbered item in the sentence makes it difficult to read sentences like "" Other similar works include [12] that fused different templates using SVM
for finding answers to questions and [29] presented the combination of
structured (ontological)..."". Also please spell out any acronyms before you use it, like QB or PB in the first page. Lastly, if this paper gets accepted, it needs to be carefully proofread as there are numerous typos in the paper.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/17/2018,0:36,no
276,78,266,Noxxxx Meuxxxx,1,"(OVERALL EVALUATION) The authors present a blockchain-based approach to document and verify authors?€? contributions to writing academic publications. The approach is novel, interesting and highly relevant to JCDL. 

Whether contributions to writing are the only criterion that should be considered for assessing the appropriateness of authorship is debatable. However, in my view, these ethical and political issues should be separated from the technical question of how to track authors?€? contributions in a tamper-proof way. Given the premise that writing contributions should be tracked as a criterion for authorship, the paper presents an innovative technical approach to accomplish this task.   

The description of the general process is clear and easy to follow. The authors demonstrate the application of their approach by documenting the creation process of their paper using the NEM blockchain network. For the final paper, I would be very interested to see some more hands-on information about the approach and prototype, e.g., which document formats/word processors are supported and what does the process and software to create, encrypt and submit changesets looks like. To accommodate this information, the authors could shorten the introductory explanation of the blockchain data structure and the discussion of the well-known ethical issues concerning authorship.

The paper is well structured and easy to follow. There are a few grammatical mistakes throughout the paper, especially in Section 2.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/16/2018,17:04,no
277,78,325,Haxx Salxxxxxxx,2,"(OVERALL EVALUATION) The authors are presenting a problem of authorship contribution and how to measure this contribution rate in scientific papers. This rate is known internally within the authoring team but it is hard to gauge and measure each author's contribution to external parties. The authors propose a blockchain based framework to assess this contribution.

The flow of the paper is clear and precise,the related works are descriptive, exhaustive (to the best of my knowledge) and to the point. It is quite refreshing to see various crossovers from techniques like blockchain with other seemingly unrelated fields of science (like authorship analysis) and using it effectively to both solve the problem and also maintain the integrity and preservation of the data.

I do have quite a few concerns though, the idea is novel and that is granted, but the paper just demonstrated the idea in a form of an algorithm and a private implementation, with no form of verification, experimental analysis to verify the proposed work (except for the analysis on this paper itself). The idea itself is based on word contribution to the various drafts of the paper, and that in some cases would be flawed as sometimes in theses the background, introduction, related works, are considerably larger in size than the proposed solution itself. Thus making the content size irrelevant to the contribution itself. The authors mention that in the drawbacks but it was too critical not to mention in this review. Furthermore, from software engineering prospective, considerably similar results could be achieved using various forms of repositories or SRNs that are cheaper, more stable, and also encrypted and they do give the same stats of author contributions (number of edits, number of iterations, word commit size, etc) while keeping every revision of the paper. For these reasons I am giving the paper a weak accept for solely the novelty of the work and i encourage the authors to address these points","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/16/2018,19:12,no
279,78,143,Bexx Gxx,3,"(OVERALL EVALUATION) An interesting paper with an extremely ambitious goal that is relevant to the JCDL, but the title is misleading. Instead of ""Modeling Author Contribution Rate with Blockchain"", a better fitting title would be ""Storing the individual EDITING contribution using Blockchain Technology"".

The described approach does not measure the individual contribution, but only the extent to which each author edited a paper (e.g. measured by the number of typed characters). However, writing/editing is just one form of contributing to a paper besides e.g. the data analysis, evaluation etc. This is briefly discussed, but not sufficiently considered.

The paper would benefit from discussing the research questions separately: 
1.) How can the individual author contributions be automatically measured?
2.) How can this information be stored in a tamperproof manner and linked to a given paper?
 
Regarding 1.) the authors offer an interesting discussion, but not a convincing solution. However, the authors are aware of these limitations.
Regarding 2.) the authors present a possible solution, but it remains unclear why blockchain technology is required for this. Why not just add a note to the paper itself? E.g. as percentages behind the author names: Peter Jackson (70), Julia Vogt (20), James Luck (10).

Nevertheless, I like the general idea of the paper and see the potential for this paper if the points above are more convincingly addressed.

The paper is well written and also for non-experts easy to understand.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/16/2018,20:52,no
280,79,53,Roxxx Buxx,1,"(OVERALL EVALUATION) This paper explores a heterogeneous network approach to tag recommendation for social question answering. 

The paper is reasonably well written. I appreciate the user study to validate the tag recommendations. Some aspects of it are weak, however.

Research questions: There are really only two research questions here: RQ 2 and RQ 4. RQ 1 is trivially true since there is no definition of ""meaningful"" proposed. RQ 3 is not a research question. You need to decide specifically what is of interest relative to social networks and spell it out in the question for it to be a research question. 

The answer to RQ3 is the least interesting part of the paper. It would be sufficient to say that the metapath method is superior across all the different bins and skip the entire page figure that conveys no new information. The only way this would be interesting is if there was a finding that it worked worse on some subset or if there was analysis that dug into the differences between conditions. 

As the authors note, there are numerous types of metapaths that might be considered in this type of work. They say they use the three ""most relevant"" but it is not immediately clear that these are the right metapaths to use or the criteria for being the most relevant. 

The authors miss an entire stream of very relevant research on linear weighted hybrids for tag recommendation and recommendation using metapaths in heterogeneous networks. These should be incorporated into the discussion of related work and used as baselines:

The work of Yu on HeteRec is cited, but not used as a baseline.

Gemmell, J., Mobasher, B., Schimoler, T., & Burke, R. Hybrid Tag Recommendation for Social Annotation Systems. In Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM ?€?10), pp. 829-838. Toronto, Canada. October 2010 

Vahedian, F., Burke, R. & Mobasher, B. Multi-relational Recommendation in Heterogeneous Networks. ACM Transactions on the Web. 11(3): 15 (34 pages), 2017.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/7/2018,2:44,no
282,79,35,Joxxxx Bxx,2,"(OVERALL EVALUATION) The authors introduce a novel concept for tag recommendations and the paper is well written and easy to follow. 

1. Scope
While tag recommendations is an interesting topic for digital libraries, the authors' work does not relate directly to libraries. The authors focus on StackOverflow, which is a community-based question/answering platform, primarily for software developers. Frankly, I feel the authors have written this paper originally for an Information Retrieval conferences where it was rejected and now JCDL is the fall-back solution. I feel the authors should have explicitly explained how their work is relevant for digital libraries. The authors have also submitted a short-paper to SIGIR and in that paper, they introduce a somewhat similar approach. The authors should have explained in their submission how this submission differs from their SIGIR submission. 


2. Related Work
The related work section is rather weak, at least with respect to digital libraries. On the one hand, many references to work about tag recommenders in the context of digital libraries are missing. For instance, there are lots of articles from Gerd Stumme et al. of which I would expect at least one or two to be cited (and ideally used as baselines). Generally, I realize that only 3 or so references out of the 43 references relate to digital libraries. The vast majority of cited papers is from RecSys, SIGIR, WSDM and other IR related venues. On the other hand, the related work section is a superficial summary of related work that does not demonstrate a deep understanding of the field. For instance, this paragraph:

""Heymann et al. [9] employ an entropy-based metric which captures the generality of a particular tag and indicates how well that tag can be predicted. Lu et al. [18] propose a content-based method that incorporates the idea of tag/term coverage while Song et al. [27] propose a two-way Poisson mixture model for real-time prediction of tags. Sen et
al. [24] propose a user-based vocabulary evolution model. Fu [4] employs a rational model that captures how social tags influence knowledge acquisition and adaptation. ""

This paragraph demonstrates that the authors have found a number of articles relating to tag recommendations, and that they were able to summarize the titles of the related work. There is no deeper information such as what the advantages and disadvantages of the proposed approaches are, how well they performed, what their common features are etc. (there is one paragraph that shows some critical analysis, but there should be much more).

The authors write they use 0.5 million posts and remove words with a frequency less than 20 and tags with less than 50 occurrences. This seems like quite a strong pruning of the dataset that may affect the evaluation. I consider it important that the authors provide information on how many words and tags remain after the pruning. 

I couldn't find any information on the significance level of the results. 

A few minor issues:

A general advice about framing research question: It is rarely a good idea to have a research question like ""Can we..."" because the answer is almost always 'yes'. What you probably want to know is ""How good/effective/efficient can we...""?

The authors write ""we randomly selected 0.5 million posts (approx.) consisting of 109886 questions, 371415 answers and 38196 tags"". I see no reason why the number of posts is rounded (0.5m) but the numbers of questions etc is provided exactly. Either round all numbers (110k questions) or provide exact numbers for the number of posts. 

I am not sure if ""Content-cum-Network"" is a suitable name for the approach. I suggest looking up the meaning of ""cum"" in the Urban dictionary. In addition, I couldn't see any explanation in the manuscript what 'cum' is supposed to mean.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/19/2018,11:29,no
283,79,14,Moxxxx Akxx,3,"(OVERALL EVALUATION) The paper needs more work. The idea is interesting, but needs to provide more details and justifications. The evaluation seems promising. However, I recommend rejection, for mostly the following concerns.  

The three types of meta-paths presented in the paper represent various connections between tags. All three are based on assumptions that may or may not be true. The first type of meta-path is where tags are connected if the tags appear in different questions posted by the same user. The underlying assumption is that ?€?more likely that questions posted by the same user contain similar tags?€?. It would be helpful to know if this is truly the case. Did the authors conduct any study to validate this assumption? Other two meta-paths seem like an extension of the first meta-path, where one is extended by the answer of the user who posted other questions, another is extended by similar questions. 

The definition of meta-path is not clear. What is T(g), A, and R? 
The paper mentions ?€?nodes of meta path?€?, what are these nodes? How do the authors define an edge? 

Is meta-path and tag-tag graph the same thing? If it is, why does not the 2nd and 3rd meta-paths consider the tags related to the questions that were (i) answered by some user, or (ii) similar to other questions?  In that case, for example, meta-path 2,  ?€?T -Q -U - Q - A -U - Q ?€?T?€?, should have ?€?T?€? linked with the Q in the middle of the path. 

Tag-tag relevance formulation states that there is an edge in the tag-tag graph if and only if the weight of the edge is the same as the similarity score between the tags in a meta-path. Given equation 1, which shows the similarity score between two objects, the similarity is computed using random-walk-with-restart method, which depends on probability. So, the similarity of two tags is likely going to produce a fractional number. The question is, does the weight of the edge, which seems to be a round number without fractions, given the description in 5.2 (?€?weight of same type of outgoing links equal to 1?€?), ever going to be same with the similarity score of the tags? 

Terminologies need to be consistent. Questions, answers, users, etc. are listed as ?€?entities?€? in one section, and ?€?objects?€? in another section. Is ?€?weight of link?€? and ?€?weight of edge?€? the same? 

The relationship is directional (?€?outgoing?€?). What is the significance of the directions? The authors mention that the weight of the P-U relationship is one (Section 5.2), but the relationship in Table 2 does not list any P?€?>U relationship. Clarification needed.

The research questions need more work.  Also, the questions were not answered directly (e.g., RQ1, RQ2). In most cases, the evaluations are not providing supporting evidence to the questions, mainly because the questions are not properly aligned with the questions.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/23/2018,22:42,no
284,80,221,Mxx Kexx,1,"(OVERALL EVALUATION) This work explores the perceived boundaries by web archive users as to what should be associated as a single ""resource"". For various components of a web page, Mechanical Turkers were asked about the boundaries in the context of institutional and personal archives and the results were compared.

The loose usage of ""resource"" throughout this work was disconcerting as to the understanding of what was being asked in the survey. This may have had skewed results from the respective respondents based on their technical knowledge. 

Section 4.3 makes a claim about the ""commercial web"" without any citation.

There were a few issues with the quality of the writing, for example:
* ""Groups of pair web pages [were] presented to the participants.""
* The ambiguity of the ""A second study..."" sentence in-terms of what might be (though it's unclear) a parenthetical aside.
* ""Individual web...more (...) or less"" - What is the subject here? More or less what?

Though it may be out of the scope of this study, the questions relating to whether advertising is part of the same resource beg whether respondents would report differently were the advertising from the site owners and not a third party (as is inferred).

Overall, the results of the surveys were not surprising, i.e., users expect institutional archives to capture more than personal archives. While it is not to be expected for Turkers to understand web page dynamics (e.g., are images embedded or pulled in from elsewhere) or Web Architecture semantics (like ""resource""), it is also questionable whether those same Turkers would understand what constitutes a personal versus institutional archive, as the survey does not make this clear.

I would recommend the authors repeat the MTurk surveys with more clarity of the aspects mentioned here (e.g., what's a resource? What constitutes a personal archive?) to be able to more accurately infer results.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,1/31/2018,22:11,no
285,80,50,Juxxxx Fx,2,"(OVERALL EVALUATION) In this paper, the authors extend their JCDL2017 work (understanding use expectations of institutional archives) by measuring the differences between user expectations of institutional vs personal archives. Their findings appear to indicate that users are more tolerant of missed embedded/associated resources in personal archives than they are of missed embedded/associated resources in institutional archives. Further, their findings indicate that the page types show similar patterns of expectations with regards to follow-on pages, links, and associated resources.

This work is clearly important (particularly given the nature of the authors' already published work). However, this extension is preliminary and deserves additional analysis and a cleaner presentation. Supporting comments are presented below.

1) The authors use terms such as ""representation"", ""page"", and ""resource"" in confusing ways. For example: ""we are exploring how to determine when additional web pages are part of the same resource as a given page"". I think the authors are trying to convey the concept of embedded resources or composite representations. Cleaning up some of the terminology and making it align more closely with the definitions of ""representations"", ""resources"", and ""web pages"" would aid the readability (particularly when discussing notions of ""more"" and ""less"" ""content"" but referring to concepts like ""links"").

2) While the four categories presented show intriguing trends, the analysis seems incomplete. For example, performing a more careful comparison of multi-page stories and multi-image series would be beneficial to the readers' understanding of the phenomenon observed in the data; is the similarity in expectations between personal vs institutional archives ""later pages"" and ""later images"" a coincidence? Is the fact that the deltas between personal and institutional are greater in multi-page pages than multi-image pages significant? I think there is enough to explore here to warrant further analysis.

3) Single page vs multi page stories seemed to be have discrepancies in measured included resources. For example, why wasn't the presence of an Author Page included for multi-page stories even though single page resources had it included? was this found to be inconclusive, insignificant, or not enough data recorded? Why are author pages more important for image sequences than single page stories? Why isn't this the same for multi-page stories? Again, getting back to 2), it seems single- and multi-page comparisons would benefit the impact of this paper.

4) The readers would benefit from a more complete methods discussions. The authors reference their prior work, but note changes to the MTurk questions asked as well as the new execution of the MTurk tests with new participants and paired resources. 

In all, this work is important and intriguing, but there are enough questions and issues with the presentation, analysis, and methodologies to prevent me from recommending it for acceptance into JCDL2018. I recommend the authors expand their analysis and submit a full paper with a more detailed methods discussion and investigation of the phenomenon they observed.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper seems like a 7 page paper that got condensed into a 4 page analysis; not quite enough for 10 pages but the compression to 4 left out a lot of good analysis essential to our understanding of the work being presented.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/5/2018,19:31,no
286,80,368,Nicxxxxx Taxxx,3,"(OVERALL EVALUATION) Authors survey Mechanical Turk workers' expectations of the scope boundaries for different categories of web resources (e.g., multi-page articles, sequenced images, reviews/rating pages) as candidates for either institutional or personal web archives. Survey findings suggest slightly greater expectations that institutional web archives include such web resources as advertising, wish lists, and outbound links.

I agree with authors' characterization of the general value of this kind of research as informing the application of limited institutional collection development resources toward the archiving of those materials expected by its users to have been collected. I think the article would benefit from articulating this more up-front, as well as what contributions this specific research makes in that regard.

I had some concerns about the strength of conclusions that could be drawn from the survey. For instance, is there reason to suppose that comparatively disinterested Amazon Mechanical Turk contractors are a reliable proxy for the expectations of future users of institutional web archives? Even taking that for granted, it seems hard from a system design perspective to meaningfully translate, e.g., a 13% difference in expectations for the archiving of advertisements between personal and institutional web archives, much less a 2% difference in expectations for the archiving of the sixth image in a sequence.

While usefully corroborated by this research, that generic survey respondents have expectations that institutional web archives have a more expansive scope than personal web archives seems a common-sense conclusion. I also wonder whether collection development decisions one way or the other for web resources like advertising or wishlists might not be more strongly determined by narrower classes of users (e.g., researchers) with a significant interest in their archiving, rather than moderate interest by a more general class of users.

A couple of point comments:
1: ""What is and is not part of a web resource does not have a simple answer.""
This opening statement belies that the article is about perceptions of the boundaries of web resources rather than of archived web resources, which seems like a meaningfully different question. Moreover, without the later explanation of the way that the term ""web resource"" is being used here, the conventional definition of a ""web resource"" as a URI-addressable object would seem to imply that web resources are in fact definitionally discrete, in contrast to the opening argument.

1: ""Additionally, personal digital archives are usually thematic or task- or subject-oriented.""
Are institutional web archives distinguished by a focus other than thematic, task-, or subject-oriented? In other words, do these characteristics in fact distinguish personal digital archives?

A few opportunities for copyediting:
1: ""Groups of pair web pages presented to the participants.""
1: ""...support the retrieval the material on demand.""
3-4: ""We also explored perceptions of linked content from single-page resources to better understand how the data concerning the above complex web resources.""
4: ""...can make archives' more efficient in the use of resources."" (superfluous apostrophe)

Sentence wording could be improved:
1: ""This led to the question of whether assumptions about the role of a personal archive were playing a role in the responses.""","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/15/2018,17:09,no
287,81,101,Kxx Ecxxx,1,"(OVERALL EVALUATION) The authors introduce a university ranking based mainly on the follower number of the university twitetr account and other affiliated accounts. The paper is well-written and the approach that the authors take is very clear and makes sense- The authors demonstarte that their ranking has a high correlation with standard university rankings and also give some interesting insights into correltaions between different rankings.

I have one bigger concern, though. From reading the paper and using my own intuition, I would suspect that the UTE ranking works because of the other rankings, as highly ranked universities simply draw more followers. With the example used in the paper, this seems to be very obvious. Everyone in the world knows Harvard, arguably because of the rankings, but at least I have never heard of the Virginia Military Institute before.

On the other hand, this is really rather a suspicion, and I am not sure if this alone would account for the correlation as in 264 univeristies, there are obiously much more than the usual suspects like the ivy league universities.

While the authors list this as part of future work, I would really have liked to see some examples for international universities that are not listed in the top ranks of the used rankings.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Sorry for the late submission.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/18/2018,14:32,no
288,81,176,Anxxxx Hixx,2,"(OVERALL EVALUATION) I am unsure why this paper was submitted to a Digital Library conference. There is no focus on documents or document analysis, that is not focus on the content but the network of followers. 

The citation of DL related work [18,19] and [25] are mere references without striving for clarifying the linkage. 


I would suggest that a WWW conference would be a much better fit.","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2/18/2018,19:13,no
290,81,341,Giaxxxxxx Silxxxx,3,"(OVERALL EVALUATION) This paper proposes a measure for ranking Universities based on popularity on Twitter - i.e. the UTE. 

I do not see much DL-related contribution here; this looks more like a correlation exercise between a self-defined measure and (debatable) academic rankings. 

One major concern: Ranking Universities on a popularity basis is a risky game and, even though there are some references in the related work section, the authors are not critical enough with this practice. The negative effects of such rankings have been quite recently reported in the literature and also in a best-seller book (weapons of math destruction), which effectively underlines the bias introduced by this practice. 
I do not see any value in adding such a measure even though the paper represents a well conducted exercise. 

Moreover, as future work the authors state: ""Our study is subject to a number of limitations that present opportunities for future work. Campbell?€?s and Goodhart's law suggest that if UTE becomes popular, institutions may seek to artificially increase their Twitter followers in order to increase their ranking.""
To me this is the core of the problem and it should be discussed right away and not as future work. If this topic is not addressed the whole UTE has very little use. 

The authors state that ""We did not address known issues with bots and spam accounts which may over in ate the stated number of Twitter followers, the primary component of our UTE score"". This is a considerable problem with this kind of study since spam accounts and bot may hugely inflate the ""popularity"" of an institution. 

I am not sure about the ""high reproducibility"" of the study since, just to mention one possibility, tweets can be deleted. It is true that if we store all the tweets in a local archive, we can reproduce the study, but this is not well-described in the paper. The Twetter API has a policy that has to be taken into account: https://developer.twitter.com/en/developer-terms/policy 

There are also issues related to the redistribution of Twitter content https://developer.twitter.com/en/developer-terms/more-on-restricted-use-cases
how do the authors deal with this and the reproducibility of the study? 

These are relevant questions for the whole community and the matter should be thoroughly discussed in general. Nevertheless, I would be more prudent about the high reproducibility of this study.

Anyway, the analysis may be of interest to investigate the effect of social media on public perception (a radically different work with a different goal).","Overall evaluation: -3
Reviewer's confidence: 3
Recommend for best paper: no",-3,,,,,2/19/2018,8:42,no
291,81,202,Jaxx Kaxx,4,"(OVERALL EVALUATION) The paper investigates altmetrics for US universities -- comparing social media impact with traditional measures of reputation.
      
      There are some key strengths:
      
      - The general topic of studying social media impact of universities in relation to various traditional indicators is interesting, and of some relevance to JCDL.
      
      - The data collection is very rigorous and complete, as is the write up in the article.
      
      - There are some interesting differences and upsets -- that generate interesting discussion (without any clearcut final, hard conclusions).
      
      There are some limitations:
      
      - The paper is exploratory in nature, and interesting -- but any definite interpretation of all the moderate and somewhat different correlations is impossible or premature.
      
      - More discussion on what is actually supposed to be measured by the different indicators feels welcome, or even needed.   Some fields outside the DL community have extensively studied difference between particular factors -- in particular in the social sciences, e.g., think of economic differences between status and reputation, including the temporal patterns associated with changes in such indicators.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Interesting paper that will draw considerable attention at the conference.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,2/23/2018,11:53,no
292,82,388,Kaxxx Verxxxx,1,"(OVERALL EVALUATION) This paper explores the analysis of the Contributions sections of biomedical journal articles to automatically extract and structure authors' contributions. The approach involves a number of steps: OpenIE to identify subject-action-object triples, co-clustering of actions and objects in order to identify common actions that are organised into a role inventory, and application of a classification model to a given Contributions section to extract roles.

In general, the authors' approach is interesting and AFAIK makes a contribution to tackling information extraction from a section of biomedical journal publications that has not received much attention.

However, there are some methodological issues with the approach:

- the clustering methods are strongly related to co-clustering/bi-clustering and approaches such as formal concept analysis (see e.g. Joslyn et al ""Exploiting Term Relations for Semantic Hierarchy Construction"" for an example applied to relational predicates; in general there is ample work going back decades), but no mention of the existence this related work is made

- some of the details of the methods involve manual steps that are not well justified. For instance, errors made by the clustering algorithm are identified and corrected (page 5). Why is it reasonable to apply these manual steps? Would you claim that the method is useful for automatically inferring the set of roles, or only for supporting manual inference of the set of roles? The former claim is not directly supported by the work (although it does appear to be what the authors are targeting -- contribution #1 in the Introduction does not mention semi-supervised discovery of roles, rather ""a method for discovering""; the latter claim is weaker and it is unclear whether it saves work in the long run, in particular given that only 13 roles are identified, and these largely map onto an existing taxonomy -- and is missing several (e.g. supervision and funding acquisition) that appear in the test set.

- the description of the classifier used to do the rule extraction is not precise enough. Given that this is a primary contribution claimed in the Introduction (contribution #2), this is problematic.  What are the ""64 binary keyword-based features"" and how were they derived? What is the definition of a ""redundant mention""? Why are redundant mentions removed? Are the features always going to be the same in each case? If there is some context from the original text, is it possible that ""redundant"" mentions might indeed have a different feature representation?  Is NULL equivalent to ""no relevant relation""? That doesn't seem to necessarily correspond to ""all feature values equal 0"" -- but again not knowing what the feature values are, possibly it does. On p 6 the reader is told the features ""correspond to the presence of the object and action keywords"" but this is rather vague. The classification methodology is very basic; my expectation would be that methods that make use of richer context, as is done in state-of-the-art biomedical relation extraction systems, would achieve substantially improved performance.

- the evaluation is not fully sound. The authors opt to remove documents from their test set that don't follow the pattern that is supported by one of the pre-processing tools. However these should be counted as FNs for the method, not removed. It creates a biased evaluation to remove these documents due to a limitation of the methodology. Instead, they should be included and the source of the systematic errors reported in the error analysis.

- does ""average results"" mean micro- or macro-average? What is the distribution of the various roles in the training and test data?","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/4/2018,12:05,no
293,82,83,Daxxxx Haxxx,2,"(OVERALL EVALUATION) This paper presents an approach for identifying authors contribution in articles by using NLP techniques. The paper is well written, easy to follow and present a solution to an interesting problem which can contribute to better understand author contributions in articles. However, there are some issues which are important to improve in the article.

Besides the problem of identifying author contribution is novel, there is some related work regarding some NLP techniques (e.g fact checking) in which I think that the authors should present in related work.  Also, the authors did not put any baseline for comparison.

During the ""Roles Extraction"" procedure, the authors discarded mentions in which all the features values were equal to zero. However, these mentions should not be discarded as, in these instances, the method fails to recognize the author contribution, affecting the results. In addition, the last paragraph of this section is confusing (e.g. it would be better if authors explained the 64 binary features in step 3).

A suggestion to improve the result is to use a dictionary to identify synonyms and use it in the merge procedure of the Algorithm 1.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/16/2018,15:42,no
295,82,137,Danxxxx Gxx,3,"(OVERALL EVALUATION) This article presents a method based on Open Information Extraction, a well known clustering tehnique, in order to extract automatically the authors?€? roles from some scientific publications using PubMed Central resources. It is an experimental research in order to show that the methodology is effective and it can improve the state of the art. However, in section 5, the authors remark that ?€?The results of the unsupervised roles discovery are similar to existing taxonomy CRediT?€?. 
In general, the content of this paper is clear, but a little bit too descriptive. A short paper should have been more appropriate in this case.
Also, the individual roles are presented too many times even if they are almost the same. It is indicate to identify a solution that does not consume so much space (see pages 2, 4, 5, 6). Indeed, this method could be useful, but the authors do not present the exact utility of their survey. In time, this approach could have good practical implications, particularly since it can use a significant number of texts, but these aspects should be clearly presented.
The manner in which the algorithm from p. 5 is presented should be modified (simplify it).
Also, some typos (such as: ?€?Aa a result?€? from p. 7).
From my point of view, this paper is not very close to the digital libraries domain, but it has some interesting aspects that deserve better exploitation in the future.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2/22/2018,18:27,no
296,82,415,Seuxxxxx Yxx,4,"(OVERALL EVALUATION) - comments to authors

* It could have been more interesting study if:
	- More steps were automated instead of conducting tasks manually
	- Figure 2 is unnecessary
	- Attempt to use the entire 186,874 data instead of selecting only about 1% (i.e., 2,000) data
	- Try out multiple classification algorithms that perform better than the baseline Naive Bayes. Weka or scikit-learn Python package provides easy access to various implementations of such algorithms.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) We may recommend the authors to submit this full paper as a short paper instead.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/24/2018,15:14,no
297,83,220,Mxx Kexx,1,"(OVERALL EVALUATION) This paper discusses and compares a system that, given a mention of an entity, allows the ability to associate a mention of an entity to its unambiguous counterpart. The authors provide a concise example of this in distinguish ""Clinton"" from both Hillary Clinton the senator, Hillary Clinton the presidential nominee (both being the same person), and the former U.S. president Bill Clinton.

This paper is well-written, builds on, and clearly distinguishes what is performed from previous well-cited studies. The experiments described are well-structured in the discussion, clearly discussed, and the results were compared to other conventional methods.

Even with the caveats of the datasets (e.g., one data set no longer exists and another provided too-general classification), the comparative results stated would be extremely useful in contexts outside of the ones mentioned.

I did feel, however, that the heuristic used in their AL approach was not described in a level of detail that it should for the JCDL audience. Without requiring a large amount of domain knowledge, it would be useful to provide concrete examples of the results as processed from both their algorithm and the ones compared. Some measure of true/false positive/negative would also be useful in evaluating the AL approach compared to others.

There were a couple stylistic inconsistencies in the paper, mostly trivial, for example:
* pp 2: ...the definition of ""aspects""...has a mis-turned quote character
* pp 2: Proposed method in details... should likely be singular.
* Footnote symbols are sometimes before and sometimes after periods throughout the paper (use them consistently)
* Section 5.3 uses lower-case ""table 3"" when it's capitalized elsewhere (and should be capitalized here).

I believe that this paper is well-suited to be included in the JCDL program and will be of interest to the typical attendees. I look forward to seeing it presented, if accepted, and for the release of the data set mentioned, as the paper promises post-publication.","Overall evaluation: 2
Reviewer's confidence: 2
Recommend for best paper: no",2,,,,,2/6/2018,0:26,no
301,83,199,Jaxxx Jxx,2,"(OVERALL EVALUATION) This paper describes a new method for linking contextually-sensitive role information to entity-links as means to refine and enrich them. While the reviewers had some reservations regarding the level of detail the authors provide with regards to: how manual assessments were carried out and the positive/negative/false positive/false negative rates of the AL method used for the computational analysis carried out, they all agreed that this paper was an accept.

Overall: Accept

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Min please replace the existing metareview with this one. For some reason EasyChair is not allowing me to revise the metareviews.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/23/2018,15:45,no
303,83,249,Byxxx Marxxxx,3,"(OVERALL EVALUATION) This paper explores a technique for contextualizing selected named-entity references in a document intending to support better retrieval and analysis. This is certainly an important topic and quite relevant to JCDL. It seems that the works was done systematically using appropriate techniques.

I would have liked to have a somewhat more detailed (though still brief) explanation of how this approach might be used in a digital library context. If I have it right, the question answered by this approach is ?€?Which section of a well-organized document (Wikipedia page) is the best match for an entity reference in some other source query or document??€? Presumably, this matching would provide additional data to support query expansion or other analytical/retrieval tasks. I think many JCDL attendees would appreciate an explanatory connection to a DL-relevant task.

I have one criticism that would cause me to request a revision if this were a journal article. However, perhaps some of these shortcomings are less of a concern in a conference submission.  The first dataset is not as well-described as I would like. 

Section 4 says ""we have manually assessed the correct relation between context and linked section."" I am not really sure what that means. If manual judgments were part of creating a gold standard, the method of assignment and details about such assignments (i.e., inter-rater reliability) should be reported. The authors do report Kappa for judgments regarding a second dataset; why not here? Given the final conclusion (adding data about similarity to section headings incrementally increases accuracy as compared to BM25 analysis of the content) one wonders if the ?€?correct answers?€? used in the gold standard judgments were formed while looking at those headings.

Also, the narrative does not provide sufficient examples for this audience. The one politically-focused example used throughout does not provide sufficient insight into the kinds of 'aspects' that might appropriately be identified using this technique.","Overall evaluation: 3
Reviewer's confidence: 3
Recommend for best paper: no",3,,,,,2/20/2018,18:24,no
304,83,277,Wolxxxxx Nexx,4,"(OVERALL EVALUATION) relevant and important task, very promising approach (though still work left to be done)
paper well written, examples and scenarios described clearly
will most probably inspire future work from other groups","Overall evaluation: 3
Reviewer's confidence: 5
Recommend for best paper: no",3,,,,,2/18/2018,19:34,no
309,84,137,Danxxxx Gxx,1,"(OVERALL EVALUATION) The paper describes how to extract machine-readable metadata (here, a specific metadata type set) from bibliographic reference strings by applying, evaluating and comparing ten reference parsing tools in a specific business use case.

One concern is that it needs more clarity in the motivation of their survey and, implicitly, the business case.

Note that this has appeared as an arXiv preprint: Dominika Tkaczyk, Andrew Collins, Paraic Sheridan, Joeran Beel. 2018. Evaluation and Comparison of Open Source Bibliographic Reference Parsers: A Business Use Case, Cornell University Library, https://arxiv.org/abs/1802.01168

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I revised my score on this paper from  -3 to -1 so that my review does not appear too negativ - and to contribute towards reviewer agreement. I recommend to the authors have to change/formulate all the content..","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,3/9/2018,4:16,no
311,84,258,Phixxxx Mxx,2,"(OVERALL EVALUATION) The paper is well-written and structured; good examples.
Research questions are fine and answered.
Related work is sound and comprehensive.
Methodology is fine too. The paper presents some novel results.

The authors could write more about the training and optimization of the retrained models. How much effort was this? What kind of retraining has been done? Was is it a specific retraining for the chemistry domain? More details are need here.

My main point is the missing titles in the extraction task. The authors write: ""Unlike the typical reference parsing task, the title of the references document is not required in our project."" I miss a clear statement why this approach is chosen. 

I have some minor issues:
- What is so special about the business use case? The paper is about processing pdfs from a collection of chemistry papers. I see no need to bring ""business use case"" into the title. More argumentation is needed here. Or just change the paper title.
- The list of possible errors (page 2) is not completed. I suggest adding more cases. E.g. Copy-Paste errors, errors by human handcrafting references, ... 

The limitations of the study can be extended. 

Altogether this is a good paper with some minor issues which can easily be fixed and clarified.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I suggest that Min-Yen Kan should take a look onto the paper. He is more expert on this topic.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/21/2018,21:41,no
313,84,94,Mixxxx Dobrexxxxxxxxxxx,3,"(OVERALL EVALUATION) This is a useful, well structured and clearly presented study comparing the performance of ten parsing tools on automated extraction of bibliographic data. 

It needs more clarity in the motivation of the study and the business case. 

For example the first paragraph of the Introduction states: ?€?However, such data is not always available. As a consequence, there is a demand for automated methods and tools for extracting machine-readable bibliographic metadata directly from unstructured scientific data.?€? ?€? the first of the two sentences needs to be expanded ?€? not available where? The justification of the need is not strong/clear enough.

I was also wondering what was the basis of the decision of using 67% of the dataset for training ?€? this is quite high. It would be interesting to see the reasoning behind it.

It is also not entirely clear who is going to really benefit from the findings of the study and in what particular contexts. 

Can any conclusions be made what is that improves the performance of parsing tools?

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I completely agree that in the case of a journal publication there would be space for revision and I am not sure how much of an improvement can be done within the conference cycle. The paper does not define clearly who and how benefits from the findings. It is also not the type of a paper which develops a new methodology for evaluation which would be useful. It does a systematic work on a bigger scale than in previous studies - but does not persuade who benefits and what will happen next. If the findings also helped to understand what features of the parsers improve their performance this would be helpful but it is not the case. I downgraded my first review. I would suggest to possibly accept as a poster.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/23/2018,5:44,no
314,84,101,Kxx Ecxxx,4,"(OVERALL EVALUATION) The authors conduct an evaluation of several open source citation parsers in the context of a specific business use case which is not further described. The paper is well-written and easy to follow. The presentation of the state of the art approaches that are evaluationd as well as related work is adequate. The results largely confirm what has been reported on earlier papers, mostly papers where the evaluated approaches have been presented and therefore also been evaluated against other approaches.

So far so good, but for me the main question remains what we can learn from this paper. On the one hand the number of approaches compared exceeds all other papers so far (at least to the best of my knowledge and after brief research). From a practical point of view, the authors give a very good overview what to expect and where to start. Also the methodology can be adopted for own evaluations.

On the other hand, the use case of the authors is very specific, they seem to be interested mainly in the journal (aka source) of a reference, without giving any motivation for it (calculating an impact factor comes to mind). For example the title of a referenced article is not even considered. Publications other than journal articles are also not considered.

In the way this is presented this leaves the impression that the results are not generalizable and therefore not very interesting. If the paper should be accepted, I strongly suggest to motivate the use case and the evaluation differently: First, show that the use case matters for extraction quality, with the example of IF calculation where basically only the referenced journal with issue is needed (if this is not your use case, then give more details so that at least your motivation can be understood). Then explain how you set up a tool evaluation with respect to your use case. This way the results are directly usable for all settings that share the same use case and the paper introduces a methodology that can be adapted to different use cases.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Sorry for the late submission.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/17/2018,20:23,no
315,84,406,Jixx W,5,"(OVERALL EVALUATION) The dataset is focused on chemistry papers, but they represent a type of citations. The authors mentioned by the end of the paper that one of the limitations was that they did not extract titles due to the dataset. In fact, a majority of papers published in science domains (physics, math, chemistry) cite papers without showing their titles simply because the journal name, volume number, and page numbers provide sufficient information to locate that paper. 

However, since the challenge of this work is a fair comparison, I do hope the authors can use more text to elaborate how the large-scale ground truth is built. For example, the authors use GROBID to do Step 1 and 2 and focus on Step3, but the results of GROBID (in XML) already contain parsed citations. Even author names are broken down into first and last names. It is a bit confusing how you use the output of GROBID to evaluate other tools at this stage.

Another minor comment is that some results are presented repetitively. The authors can merge them and make it more succinct. 

Of course, ideally, the authors should supplement a computer science corpus to get a more unbiased evaluation by taking titles into consideration.

In summary, I think the paper performs a useful comparison of off-the-shelf citation parsing tools based on a corpus of chemistry papers and the results provide a guidance of which tools should be employed for citation extraction tasks.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/24/2018,20:09,no
316,85,55,Klexxxx B??xx,1,"(OVERALL EVALUATION) All in all, I am not convinced by the importance of the problem studied here. At the technical level, I did not exactly find the proposed solution interesting/do not think that it contains new ideas.

Specifically:
- The introduction does describe the problem, but in rather abstract terms. Where is a concrete setting that will profit from the solution proposed here? It is the responsibility of the authors to highlight the urgency of the problem and to avoid the impression that it is constructed. I for my part am satisfied with the search features offered by the online newspaper that I am a reader of, and I never felt the necessity to search their archive in the way advertized by the authors. I actually find it telling that no evaluation dataset for this problem exists and needs to be constructed.
- ""there is no access to the full contents of the document"" -- Well, even though there may not be access to the full content (possibly because one would have to pay for it), I do not think that this would be necessary to include it in the search. I.e., the document is part of the search result, and if one wants to read it, he or she will have to pay for it.
- ""The idea is to promote documents ... important time periods."" -- Somewhat lapidary and not impressive. How about other kinds of metadata?
- ""Nonetheless, the results returned ... in the requested time period."" -- Does not need to be repeated. Section 2.1 is somewhat verbose IMHO.
- ""Thus, in contrast to our problem, the units ... not archived documents."" -- I did not understand why this is a significant difference.
- ""... we created 24 SPARQL queries ..."" -- Why is this a good number?
- ""We have identified the following aspects: ..."" -- How have you identified them? How does the reader know that this list is complete? Why don't you have experiments showing that this list of aspects is meaningful/correct?
- It would help if you could state somewhere which metainformation exactly must be available. What happens if only some of it is there?
- ""The walker starts from a query-entity ... mentioning that entity."" -- Say why.
- ""... query-entities should be connected with all documents"" -- What does ""should"" mean here? I do not get it.

English:
- Related Works","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,1/29/2018,8:55,no
317,85,281,Erxxx Nexxxx,2,"(OVERALL EVALUATION) A well written paper but somewhat terse in the explanations. The state of the art is well described but in the experimental part no proof is given that the claim ?€?effectiveness?€? actually is achieved as compared to the other approaches. Since a new data set has been produced the other approaches could have been applied to it. 

It would also have been better to use a consistent example, e.g. Figure 1 and Listing 1, to explain the ideas.

The 21 formulas given are of course utilized in the evaluation but a better explanation e.g. of Timeliness, using the formula parameters could for example explain why the timeliness sometimes is of little influence and sometimes not.

The explanations of NDCG 5,10, full list and P@5 and P@10 could have been better for non-experts in the field.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/13/2018,7:58,no
318,85,187,Antxxxx Isxx,3,"(OVERALL EVALUATION) This paper presents novel techniques to rank the results of multi-entities SPARQL queries, in a time-sensitive way. The authors apply these techniques on a news corpus that has been automatically semantically annotated with a state-of-the-art method; the resulting RDF graph being then used as the target of SPARQL queries. They analyze the strengths of weaknesses of each techniques and their components - when applicable - in a relatively granular way.
The paper is well-written, the authors present quite clearly the algorithms and their results. The data for the experience is provided on their website, and seems quite complete.
The ground truth to evaluate the ranking algorithms is not very impressive in scale. But it is hard to build such an evaluation dataset for ranking, and the problem is relatively novel, os this is seems entirely acceptable. The paper seems therefore a very good fit to JCDL.

There are however some issues that the authors should clarify, in my opinion:

- some of the methodological choices could actually harm the claimed aim of the authors, which is to mediate access to information sources for researchers (esp. in Digital Humanities). Focusing results in 'important' periods of times is arguably going to create 'bubbles' in which the researcher could be trapped. My fear is that search results would not reflect adequately the diversity of the coprus. How could such bias be avoided?

- one can in fact debate whether focusing on 'important time periods' would even help outside of specific cases. It seems intuitive in a newspaper corpus, where entities are mentioned much more when they are 'active'. But in a library catalogue, for example, one could expect important biographies to be written several years or even centuries after the events that matter for an entity. A study on a different kind of corpus would make the authors' work much stronger, and I am happy that the author plan it. And I was somehow relieved to see that the evaluation proved that the timeliness did not help much. But more discussion could be added.

- there is not a lot of related work on ranking, and it's very quickly dismissed. There seems to be some SPARQL ranking efforts, for example https://dl.acm.org/citation.cfm?id=2591350 or https://dl.acm.org/citation.cfm?id=2878522 or http://graphdb.ontotext.com/free/rdf-rank.html). Arguably these are not the most visible research efforts, but still. And even if the existing single-entity-ranking work does not tackle the main research issue of the paper, it is certainly relevant to compare it with the components that the authors employ, such as relativeness and relatedness. Efforts like ranking on DBpedia or Wikidata (http://people.aifb.kit.edu/ath/) or all the literature about measuring the similarity/connectedness of two entities in a knowledge graph should be considered more seriously than it is currently.

- the explanation of 'relativeness' has a formal issue: it is based on 'entities count' but on top on an RDF graph for a paper. But with the RDF semantics, on cannot count the entities (at least the one refered to in the SPARQL listing 2 and 3 with ?entity): these would just count as one, even if they are mentioned several times in the paper. Of course I could understand what is being counted. But there's a bit of confusion between the text, the SPARQL examples and figure 1. In fact the name of oae:Entity in the oae: model is not ideal. It's rather an 'entity recognition' or 'spotting' than an entity itself. While schema:mentions would be expected to refer to a 'real' entity like the DBpedia entry for Federer (or something that has an owl:sameAs to it).

- I think the statistical significance of the differences observed in the evaluation could be reported more, even 'informally' (i.e without computing it). I agree with many of the interpretations made, but in most cases the difference in results amounts to one or two percentage points...","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/17/2018,9:15,no
319,86,286,Daxxx Nixxxx,1,"(OVERALL EVALUATION) This paper is a user study of search tasks that are split across devices (desktop and mobile): moving from desktop to mobile tends to reduce performance.

Section 1: ?€?The search tasks of the cross-device search are the continuous, which maybe affect the users?€? search performance over the device switch. The interesting point is that we don?€?t know whether the change of search performance is continuous after the device switching.?€?
I am not clear what these sentences mean. And it is unclear whether this is just language or a more fundamental conceptual issue. The quality of the language does actually get in the way of understanding.

The frequent use of ?€?dynamic change?€? seems unhelpful.

2.2: does not clearly define what is meant by ?€?on-the-spot search?€?. This is not such a widely used term that it can be accepted without a clear definition.

2.2: the abbreviations of evaluation measures are included here - but two of them were already used in section 2.1.

The two paragraphs after the RQs appear to just recaps of previous work: shouldn?€?t they be part of the Related Work?

Screenshots of the two systems would be useful to get a better sense of the tasks from the participants?€? perspective.

The switches between tasks are both the task and the device-type. I?€?m not completely convinced that this is the task ordering with the greatest ecological validity. Would normal people sequence their tasks as in Figure 1? Some more justification for the experimental design would be good to fully understand the advantages of this sequencing.

5.3: is the observed ?€?information forgetting?€? possibly a consequence of the complexity of the experimental design? How can you be sure this behaviour is not an artefact of the study rather than a real world phenomena?

6: ?€?The reason resulting in such phenomena is the users?€? demand for new information after the pre-switch search.?€? This is an unsatisfactory statement for a conclusion: it is too vague and doesn?€?t assist the reader in understanding the contributions of the study.

Overall, this paper suffers from language problems that are sufficient to impede understanding of the study and the results. The topic is important and relevant to the conference; I suspect there is a good paper in this study but in its present form I don?€?t think this is it. Some of the results may be due to aspects of the experimental design - exploring this aspect in a revised paper would likely improve the contribution.


Minor points:

4.1: ?€?retrieval relevant results?€?

3.2.1: define SERP

3.2.2 ?€?iuput?€?

Several Figures in Section 4 are separated from the Figure captions and/or split across columns.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/16/2018,2:56,no
320,86,234,Jxx Hx,2,"(OVERALL EVALUATION) This paper investigates the cross-device search behavior by conducting an experiment where users are asked to conduct four different kinds of searches with a goal of understanding how the search performance change over the search process and between different devices. 

I have some concerns about some of the assumptions made in this paper to determine the search performance. For instance, authors claim that the search performance has decreased in the post-switch session because users are intentionally searching for new results they have not seen in the pre-switch session. Then isn?€?t it problematic to state that the search performance of the post-switch session is worse than the pre-switch session (since they probably have some good information from the pre-switch session and are finding information to complement their current knowledge on the topic)? Also, I am wondering if the authors looked at the actual results pages to see how effectively they are presented in one mode versus the other. In other words, were there any pages that did not show up correctly in the mobile version which may have affected their ?€?better?€? performance in the desktop search? Additionally, it would have been great to see more direct user input on how they feel their search process went and changed between the pre-switch and post-switch sessions. As the authors state earlier in the paper, analyzing massive search logs does not provide that level of information and it seems like somewhat of a missed opportunity to not get more direct user opinions on how their searches went and what can be done to improve that experience. 

I also want to point out that the writing in this paper needs to be *significantly* improved for this to be published. There are so many typos and grammatical errors to list them all.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/17/2018,1:25,no
321,86,263,Daxx Mcxx,3,"(OVERALL EVALUATION) This paper compares users'search performance for the same task between mobile and desktop computing devices. The paper appears to have some interesting findings, such as the forgetting of information and the much shorter search time on desktops. The paper is poorly written, though, and lacks some important definitions (such as nDCG). It would also be good to see the results contextualised in both the mobile search and search HCI literature--some of the change in performance is bound to be down to mobile interaction issues, but these are not mentioned. A further clear concern is that this paper, with an appropriate literature review and author names included will almost certainly surpass 10 pages.

I believe this paper is almost good enough to be accepted to JCDL, it is on an interesting and relatively understudied topic with clear implications for search experience. I encourage the authors to (if the paper is accepted) take my feedback on board, or if it is not rewrite the paper in view of this feedback.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/17/2018,7:54,no
322,89,378,Giaxxxx Tsaxxxx,1,"(OVERALL EVALUATION) This submission is discussing a semi-automated way of generating linked open citations. It has an extra focus on the library environment and how these data are linked to the typical library systems, such as catalogues, and the books that are already in their collections. It is very interesting that the focus is a very contextualized and realistic context and very encouraging that the authors take into consideration actual conditions in libraries. The paper is well written and the authors present a good understanding of the landscape. I think that the study needed some more attention in several parts. For instance: 

1. Are the two ways of citation detection working in parallel or in sequential mode? There is a test at the end of the paper, but the pipeline, according to the authors, have these two approaches together without noticing about their relation. 
2. I also think that the default Dice Similarity Coefficient rate needs to be documented, e.g. why this rate or why not another measure such as F1.
3. The authors need to explain why in the various tests the number of samples varies, e.g. 286 scanned documents in reference extraction test and 444 samples in reference linking stage.
4. Section 5.1. is not describing an evaluation study, rather a user requirements study. The part in the 3rd paragraph of Section 6 is more likely to be characterized as a qualitative evaluation study.
5. Some statements are quite bold, see the last paragraph of section 3 and the claim for conflict between titles and book reviews. I am not sure if this is actually a problem.
6. It would be interesting to let the reader know that the license of this database will be and how open and reusable are going to be.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) It is a good contribution and I balanced between weak accept and borderline. Depending on the other reviews it can be accepted in the main track, but it can be a poster/short paper as well.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/3/2018,9:31,no
324,89,265,Noxxxx Meuxxxx,2,"(OVERALL EVALUATION) The authors present a distributed system for the semi-automated extraction and curation of citation data from printed and born-digital documents during the cataloging workflow in physical libraries. The objective of the presented project ?€? creating accurate, openly available citation data is commendable and highly relevant for JCDL. 

The novelty of the presented system lies in the adequate combination of established technologies for the specific use case, which is undoubtedly interesting to the JCDL community. The authors make their code available as open source, which further increases the relevance of the paper.

Despite the good fit of the paper to the topics of JCDL, I have reservations to recommend acceptance of the manuscript as a full paper, i.e. mature work, because the paper and particularly the evaluation does not present enough information to assess the benefit of the presented system. The authors ask how much it would cost to curate all social science references of a given year using their system. In my view, this question is irrelevant as long as the improvement that the system offers over alternatives has not been shown. The authors should quantify the improvement in data quality (coverage and correctness) that their system achieves compared to, e.g., performing unsupervised linking of extracted references to available reference data. The paper should also try to show how certain alternative approaches affect the manual effort required of users. For instance, the authors may answer the following question: Is it indeed necessary for users to manually review every reference? How often do users need to manually correct entries? Do some fields need to be corrected more often than others? Does information from certain sources need to be corrected more often than information from other sources? Can knowledge about differences in the data quality of certain sources improve the quality of suggestions and in turn reduce the overall manual effort required, because users might be able to simply confirm entries more often? 

The paper is well structured and easy to follow. The use of references is adequate.

In its current state, the paper presents an interesting approach that has not been fully evaluated yet. In my view, the paper is more suitable to be published as a short paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I think this would be a good short paper.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/16/2018,17:11,no
325,89,153,Juxxx Grxxxx,3,"(OVERALL EVALUATION) Authors describe the evaluation and development of an open citation data database. The levels of beneficiaries of such a database vary widely: individual authors, information standards organizations, librarian professional communities, individual librarians, bibliometric researchers, developers of digital library systems, and research organizations. Authors demonstrate that a system can be designed to aid in the production of citation data, circumventing the need for existing subscription-based and restricted-use citation databases. Release and wide adoption of such a database would be a major step in the direction of the academy taking ownership of the citations that describe their scholars?€? work and that scholars produce in the course of authoring a publication. Authors understand that success of an open citation database depends on adoption by libraries, as they choose to focus (presumably) on assessing time required to catalog citations because they consider it a significant barrier to adoption. It would help to have authors explain whether/how these barriers informed their research question. 

Authors reference relevant related work in this area, including CiteSeerX and Open Citation Corpus. An article from 2015 referenced by the authors (Peroni, et al.) suggests the OCC plans to expand the scope and coverage of the dataset to include more of the arts and humanities, to actively reach out to publishers, and to pull more openly available citation information into their dataset (ranging from CrossRef, OA publishers, University Presses, etc.). The originality argument would be made stronger if authors would describe whether the OCC project would find such a tool for extraction useful (or whether they have developed their own extraction tools) and to have librarians or curators within OCC test the usability and utility of the system. What would happen if authors joined forces with OCC to design frontend systems to give authors, publishers, and librarians a way to share and curate open citation data? Libraries could then also focus on historical publications. 

The title is somewhat misleading; the article seems to be less about assessing cost and more about developing a digital library tool and resource. A link to a paper that describes ParsCit is helpful, and a brief sentence introducing the tool and explaining why it was selected for extraction is recommended. Librarians at the authors?€? institutions were consulted; are there plans to involve other organizations in future studies? Is it possible to identify other libraries engaged in citation cataloging and curation? 

Authors describe the system architecture, application of citation standards, and data model in sufficient detail. The authors appear to cite the relevant literature. The quality of the writing is very good. 

In terms of other recommended changes, on pg. 3, authors may want to remove ?€?a?€? from the last sentence in the last paragraph in section 3.2.

Overall, I believe the paper would be of interest to the JCDL community.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/17/2018,3:37,no
326,89,172,Mauxxxx Henxxxxx,4,"(OVERALL EVALUATION) This is an interesting project, and one that should be of interest to the wider library community. The project describes an experimental system which, I believe would be one that could deliver benefits to the scientific community as well (although the ""cost"" aspect of the title, is not adequately addressed.  I found the conclusions concerning the better results from text vs image extraction impressive.

The paper is well written and very easy to follow, with good clear graphics.

I would have liked a bit more detail on the processing for the electronic journals, although I note that the authors have concentrated on only those already available as open data.

On the whole I think that this paper is a good fit with JCDL","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/19/2018,2:29,no
327,90,110,Nixxxx Fexx,1,"(OVERALL EVALUATION) The paper discusses a prioritization algorithms to schedule metadata gathering in DBLP. The algorithms consider the delay in the publication of a conference, the conference ratings, the internationality level, the citations, and the author prominence.

The experimental evaluation on real DBLP data studies and discusses the significant differences between these approaches.

The methodology is sound and the paper is clearly written.

The paper can raise interesting discussion among collection curators.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,1/26/2018,9:58,no
328,90,270,Mirxxxx Mx,2,"(OVERALL EVALUATION) The paper introduces a metric for ranking conferences according to their harvesting metadata priority (to keep DBLP updated). The paper is well organized and well written. The problem is well related to JCDL audience, and the solution has its merits. The methodology is clear, the datasets are publicly available, and the results seem replicable. The evaluation baseline is very naive, but I couldn't imagine another easily-automated way of doing it. 

The authors use DBLP as the targeted digital library. It is not clear how the methodology could be applied to other libraries. 

There is also a crucial difference between conferences and journals. While journals my have many issues over the year, conferences usually have just one (as most of them are annual or bi-annual events). So, ranking a conference at the top (say) in March is not useful if its next edition will be in December. The metric takes such timing in consideration, but it is not clear to what extent. 

The metric also gives points to more international conferences (""The idea of this factor is that it reflects how often the conference venue is changing locations across countries""). But then, European conferences (that go around the many countries in Europe) would take advantage over (say) ACM or IEEE conferences that stay in the US, even though some of the latter are massively international. Again, to what extent such information interfere with the result is not clear. 

Overall, the work is enough for a short paper, but the text could be a little bit more precise in the way the final metric is calculated.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/15/2018,12:42,no
329,90,102,Kxx Ecxxx,3,"(OVERALL EVALUATION) The authors desribe a ranking system to be used to prioritize conferences for data curators who need to check if data for a new conference is available and include the data in the system (or, for automatic processes, check and maintain these processes). The motivation is relatively clear, but nevertheless could be improved. While generally a generalization from a concrete application if preferred, I think in this case it would help to state from the beginning that conferences have to be monitored and that this is more difficult than monitoring for example journals. After all, this is already stated in the title, so why talking in general terms about data sources.

While the idea of the paper and probably even the approach of the authors to address the problem is very good, the paper unfortunately lacks clarity in the description of what the authors actually do. 

For example: while there are several features described, it is never explained how the ranking is actually calculated using the features. Table 1 lists overdue, which probably correspnds to delay, but delay ranges from 0 to 4, while overdue seems to contain the months. Similarly it is not clear what discontinuation actually represents, but the numbers can not be calculated from w_d(c) (which is unnumbered, btw.)

Normalized Discounted cumulative gain (nDCG) (which I assume is meant by ndcg) is never introduced and all in all the evaluation is hard to follow (how about at least highlighting the interesting results?) Maybe examples of actual conferences and how they appear on the list after they are due would help, but at least I found it very hard to imagine what this ranking actually means considering the very practical research question.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Sorry for the late submission!","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/17/2018,18:46,no
330,91,329,Raxx Schxxxx,1,"(OVERALL EVALUATION) The paper considers the problem of annotating certain aspects of reviews. It proposes to differentiate reviews into non-expert reviews and expert reviews, which is a reasonable approach since the two classes usually differ in their terminology and structure. So the overall idea that the paper puts forward is promising.

However, the underlying analysis seems awkward. It compares non-expert reviews for arbitrary books with expert reviews for scholarly literature. It is not surprising that, in this case, expert reviews will not talk about characters or story of a book, but provide an analysis of the argumentation structure or the methodology. The classification that was done thus not necessarily separates expert from non-expert reviews, but may as well separate reviews on scholarly books from those on non-scholarly books. For a systematic evaluation, one would have to look at a fixed class of books and then determine the differences of the two classes.

On the more technical side, the book proposes an annotation scheme for expert reviews (or, after the discussion above, for scholarly books). The provided annotation concepts are reasonably, and they seem to be constructed on a good foundation; the paper cites a number of recommendations for writing reviews. This part would be more convincing if more quantitative results were presented, beyond a single example consisting of two sentences. The additional annotation scheme for non-expert reviews is not novel, but a repetition from a recent publication.

In summary, the contribution of the paper in form of the new annotation scheme has some merit. The experiment that introduces a simple classifier to separate expert from non-expert reviews suffers from a bias data selection.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/12/2018,11:14,no
331,91,292,Denxxxxx Pexxxx,2,"(OVERALL EVALUATION) The paper focuses on the task of aspect extraction from book reviews, designing new annotation schemas, according to reviews generated by expert readers and by general readers. It also presents a method for classifying any new review into one of these two types. The subject is relevant to digital libraries, and the paper is well written.

Annotation schemas are an important contribution to research involving aspect extraction. The paper does an interesting lexical and semantic analysis of formal and informal reviews. The datasets are annotated with aspects, categories and sentiment information. However, the paper should provide statistical data such as the number of aspects and sentiments, number of sentences per category, etc. contained in the schemes generated.

It is also important to make explicit the difference between this paper and that of reference [2]. If the Amazon Review dataset was produced in reference [2] then this work contributes only with the dataset of OpenEdition Reviews.

In the experiment for reviews classification, only one dataset was used for training and one for testing. This is little to draw conclusions. A cross-validation could be more conclusive.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/15/2018,12:09,no
332,91,32,Daxxx Baxxx,3,"(OVERALL EVALUATION) This short paper presents work developing a categorization system for aspect-based sentiment analysis and recommender systems focusing exclusively on expert book reviews, complementing existing work on developing a categorization system for user-written book reviews (as on Amazon).  While much previous work has focused on aspect-based sentiment analysis in the general setting (including even on Amazon reviews), this work aims to fill a gap by providing structure for reviews of books as extraordinarily complex objects.

While I applaud this as an overall goal and encourage the authors to continue work in this direction, the current submission has a few weaknesses.

-- First, in developing a categorization system of book reviews written by experts from OpenEdition and contrasting it with lay reviews written on Amazon, there is a significant confound here in the category of books that are found on either site.  The categorization system developed for ""expert"" reviews (including categories like C#SCIENTIFIC CONTEXT and C#METHODOLOGY) are clearly designed for specific kind of academic book, while the lay categories of C#PLOT and C#CHARACTERS are more clearly designed for fiction.  This leads me to belief that the current system doesn't account for lay reviews of academic books or expert reviews of (e.g.) romance novels; the categorization system is not so much one of the type of review so much as the type of book.  This needs to be fleshed out.  

-- Second, this conflation between of review and type of book carries over to the classification task described in section 6; I am quite sure this classifier is learning to predict the genre or subject classification of the book rather than the type of review.  This could be controlled for here by sampling two reviews for the same book (one lay, one expert), ideally both from the same platform (otherwise again you might simply be learning to predict the writing styles characteristic of OpenEdition vs. Amazon rather than the content of the review).

-- Finally, the summary statistic of the two datasets in figure 1 should definitely be normalized to the average review; the visualization suggests that the number of sentences is the same when in fact the number of sentences in the average OpenEdition review is far greater than those in the average Amazon review.  Presenting these statistics (including vocab size, nouns, etc.) for a the average review would help the reader understand the different stylistic characteristics of these datasets that distinguish them from each other.

Overall, I love where this work is heading, and I think accounting for these weaknesses of this current draft would make it a much stronger paper.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2/17/2018,5:13,no
333,91,262,Roxxxx Hx,4,"(OVERALL EVALUATION) The paper focuses on developing annotation schema for book review analysis for sentiment and proposes to differentiate reviews into non-expert reviews and expert reviews. The idea seems like a good idea because of the language, syntax and structure between such types of reviews.

The key area of the work that needs more consideration and possible solutions concerns the comparison of expert academic reviews vs non-expert academic reviews. In this analysis it compares reviews of two very different types and does not as written seem to consider non-expert reviews of academic texts.

The new annotation scheme for non-expert reviews has merit but more work is needed to separate expert from non-expert reviews

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper does not seem to offer a novel enough solution for acceptance in its current state. It might help their work to submit as a poster and to get more discussion around their methods and concepts.","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2/18/2018,14:48,no
334,92,378,Giaxxxx Tsaxxxx,1,"(OVERALL EVALUATION) This paper narrates the archival exploration of the biography of Joan Burroughs. The author used a mixed approach to collect data in the form of testimonies and physical carriers and in this attempt she was able to reveal several ""covert"" information. 

Although I find it to be a very interesting paper that in several parts contemplates about the contemporary streams of personal information in social media, I think that it's relevance to the conference is not very strong. There is a weak link of the implications of the presented research at the end of the paper, which I think it can not justify strong connection to the conference. The reference list is adequate and is supporting well the paper, which is a very well written and readable one.","Overall evaluation: 0
Reviewer's confidence: 2
Recommend for best paper: no",0,,,,,2/10/2018,18:08,no
335,92,372,Alxx Thxxxx,2,"(OVERALL EVALUATION) This paper is well-written and consistently interesting and the details of biographical research methods presented are rigorous and thoughtful, and I expect the paper will be well-received if presented at JCDL. I submit it as a ""weak accept"" rather than with a higher ranking because the connection between the case study of the author's research on Joan Harding Vollmer Burroughs (d. 1951) and Joan's ""social network"" and ""the future of social media archiving"" alluded to in the paper's title seems insufficiently developed, and this weakness may pertain to the paper's relevance to JCDL. The paper touches on current/future social media platforms (its most JCDL-targeted topic) only in the introduction and conclusion; what is said there seems accurate and well-informed of recent publications on the challenges of archiving these platforms, but the paper's move from Joan's mid-20th century ""social network"" to the online social networks of later and/or future biographical subjects remains mostly an analogy. The paper would be improved by a concrete example of another (more recent, presumably) biographical subject for whom reproducing the author's biographical research methods is shown to be enhanced by use of information from social media platforms (live or archived). On balance, I recommend accepting the paper, as there is value in its detailing of the questions and methods of biographical research using both official records and ephemera and suggested extension of these concerns to online social media archiving.   

Proofreading notes:
top of page 2: ""influence how it kept"" [insert 'is' after it]
middle of page 3, col. 1: ""in different periods of time, some subjects be rendered invisible"" [insert ""can"" after ""some?""]
top of page 3, col. 2: ""and an initial network of social exposed and recorded"" [insert word ""connections""(?) after ""social""]
middle of page 7, col. 2: ""Figure 2(a) is a surrogate marriage record, handwritten on a on a scrap of paper"" [delete repeated ""on a""]","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/12/2018,21:31,no
336,92,364,Taxxxx Suxxx,3,"(OVERALL EVALUATION) This article discusses the tricky issue of social media archiving through the lens of biography. The conclusions are difficult to discern, but the author does a thorough and entertaining job of illuminating many of the issues. 

The central core of the article is a detailed description of the author?€?s quest to assemble a wide range of resources to support her own efforts to write a biography about a woman - Joan, wife of William S. Burroughs - who died in 1949. The author is necessarily having to wade through and assemble a lot of old media - newspaper clippings, county records, death certificates, letters, etc. She describes both the stuff as well as the process, which she likens to social media exploration, as she tries to nail down vague references to people (ex: the young German) by figuring out who is being referred to. 

This central core is bookended by some discussion of the current state of social media archiving and some of the issues to consider when formulating new policies to guide archiving. She draws particular attention to ephemera and how hard this is to define, presumably because it is tempting to not archive these types of information yet they can still be useful. 

I very much enjoyed reading this paper and learned a lot about the research behind a good biography. The paper is weaker with respect to its stated aim of discussing the future of social media archiving. She outlined, clearly, several issues she faced in her own research (doppelgangers, variable records, missing information, etc.) but these were never clearly tied to social media. Does social media make this challenges harder, easier, or just different? Much of the discussion of social media archiving is confined to the final half page of musings under the conclusion. The paper is very good but it would have been stronger if the reflections on social media archiving would have been a bigger part or interleaved throughout.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2/13/2018,0:08,no
337,93,381,Supxxxxxx Tuxxx,1,"(OVERALL EVALUATION) I am no expert in cyber infrastructure. In my opinion, while cyber-infrastructure is an important component in maintaining digital libraries, its involvement is still in common practice levels. I do not expect much research and innovation in digital library-related topics to be discussed in this workshop.","Overall evaluation: 0
Reviewer's confidence: 3",0,,,,,1/26/2018,13:40,no
338,93,154,Juxxx Grxxxx,2,"(OVERALL EVALUATION) The proposal is clearly written and provides sufficient detail of the overall workshop format. A list of specific outcomes of the workshop would help to demonstrate broader impact (e.g. a white paper that provides different viewpoints of the issues, recommendations for next steps in developing a set of best practices, etc.) I?€?m not sure what A&A stands for and a few of the other acronyms are unfamiliar to me. 

While the proposal does not represent an original approach to a digital library problem, how to go about integrating big data analysis and processing environments with digital library infrastructure is of concern to the communities mentioned and a workshop as forum for dialogue and development of best practices would be of interest to JCDL participants. The organizers represent the communities they hope to bring together: libraries and archives, and high performance computing (advanced research computing) communities. It is unclear to me whether the audience would share with authors the same definition of cyberinfrastructure. Authors describe it as a field, but I think of it as a framework for thinking about expertise and resources and services required to engage in digital research. Many disciplines and fields engage in research and practice in cyberinfrastructure and the same is true for digital libraries and archives. Researchers in computer science and engineering and other disciplines are researching ways to integrate digital libraries and archives with big data management environments. It may help to describe the workshop as a session that brings together data service providers (or managers), systems engineers, and computational scientists to address a common set of issues. 

It would help for authors to note whether the workshop builds on related conversations or projects at UT and TACC, and to include a list of citations and related bridging projects to demonstrate depth of knowledge of the issues (several papers have been published on the topic, and there is an IMLS funded Library Cyberinfrastructure Project #LG-71-16-0037-16 exploring similar issues). The group reviewing expressions of interest could also include authors of important papers and related project PIs.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Noting that I currently serve on the advisory board for the IMLS funded Library Cyberinfrastructure Project #LG-71-16-0037-16.","Overall evaluation: 1
Reviewer's confidence: 3",1,,,,,1/26/2018,19:38,no
339,93,194,Adxx Jaxxx,3,"(OVERALL EVALUATION) This submission proposes workshop on cyberinfrastructure for DLs and archives.

The topical scope of this workshop is somewhat unclear to me. Cyberinfrastructure appears to me to be already a part of digital libraries which themselves evolved from traditional libraries. At least this term should be more clearly defined...
Anyway, the raised topics seem to be very relevant for JCDL community. 

I have no doubt that the team can make this workshop a success, however, of some concern is the fact that all the organizers come from the same institution.

From my experience, the attendees' count of 50 is too high, especially, as this particular workshop seems to be organized for the first time.

Additional points:
What is the role of term ""Publication"" in workshop name? Scientific research or focus on scholarly document collections?","Overall evaluation: 1
Reviewer's confidence: 4",1,,,,,1/28/2018,10:22,no
341,94,369,Praxxxx Terxxxxx,1,"(OVERALL EVALUATION) The paper identifies a method for automatically selecting attributes for achieving the most efficient and effective de-duplication of records.
    A review of the de-duplication process and contribution of this work are identified. Various methods utilized to demonstrate the effectiveness of this method in the de-duplication process such as the block key, blocking or sorted neighborhood method, with the Jaro Winkler algorithm as a similarity measure are utilized. It identifies particular metrics of attributes in a record set, such as Duplicity, Distinctiveness, Density and Repetition to calculate relevance of an attribute. Indexing attributes identified using relevance is evaluated for both publicly available and synthetic datasets for de-duplication effectiveness and efficiency.
    While the method evaluation is provided for a variety of datasets, no comparison is provided over previous method identified in the related work particularly, Canalle. The Soundex algorithm utilized in the indexing step is particularly suited for phonetic attributes such as names, states and titles. It would be useful to examine attributes containing alphanumeric entities and or attributes generated as a result of concatenation of other attributes in demonstrating the ability of the algorithm.

    The paper needs revisions to the abstract and introduction sections to improve the readability and provide clarity to the contributions of this work.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/20/2018,1:52,no
343,94,392,Peixxxx Wxx,2,"(OVERALL EVALUATION) Database integrity is a serious real-world problem. Everyone who has a chance to work in database wether front-end or back-end, has the experience of the problem. In legacy databases that were designed before ER modeling became main approach have been migrating to new DBMS systems that are relational since the 20th. However, many such migrations have not been done systematically to identify and remove duplicates before importing data to new DBMS systems. Thus the data integrity issue remain in new systems.

This study provides experimental data to the method of automatic identification of best attributes for indexing (a key with unique value for each record). The researchers also used a computer program to create data sets as well as three real-world datasets from CORA archive and others. The researchers provided evidence whether their method worked or failed to work. Further research was proposed. 

Although I believe that the best way to detect duplicates will be a combination of human experts who know both the scenarios of the data cycle, and can systematically query relevant data fields, the paper makes a valuable contribution to a real-world issue.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2/19/2018,3:33,no
344,94,229,Pexx Knxx,3,"(OVERALL EVALUATION) The paper investigates the problem of selecting the most efficient and effective attribute for indexing in data de-duplication tasks. The paper is clearly written and has a useful goal. The authors make the code available on their website. 

- I was surprised by not seeing any mention/discussion in the paper of approaching the near-duplicates detection problem using locality-sensitive hashing approaches, which are really very typical these days. Instead, the authors refer to soundex coding as the state-of-the-art. I would like to see the authors describe how locality-sensitive hashing (e.g. MinHash, SimHash) are related to their study. 
- On page 2, the authors state: ""The data deduplication outcomes improves by about 82% when using the best attributes,"" however, it is not clear how the 82% was calculated and with respect to which baseline. 
- While it is possible to understand the intuition behind selecting Equations 1-4, the choice appears to be quite ad-hoc. The study would be improved if the authors took a more information-theoretic approach in designing the formulas and then empirically evaluated several variations of the proposed measures. 
- I am unable to reproduce column R(a) in Table 2 by substituting directly to Equation 5. For example, for row Artist, I am getting: 1+0.4+(1-0.4)*1+(1-0.5556)=2.444 (but it should be 1.0). How is this possible?
- While the authors claim their ranking function (equation 5) selects the most effective and efficient, it is unclear what weight do the authors give to accuracy vs scalability. This is quite problematic for me, because it makes it harder to interpret the results in Table 4. For example, Culture is the best attribute (most effective), but not the most efficient. Should it be scored first or not? Thinking about further makes one realise that the authors have in effect not properly/quantitatively evaluated the attribute selection algorithm. For example, the authors could have defined a formula explaining the trade-off between effectiveness and efficiency and then calculate a rank correlation between the attributes selected by equation 5 and this quantity. That would also help to demonstrate if the results are actually statistically significant. 
- Depending on the use case in which duplicates detection is applied, there is typically a different cost to false negative vs false positives. In some scenarios, false positives can be more tolerated (e.g. because of a human validation process) then false negatives. As a result, I feel that basing the evaluation solely on f-measure with equal precision/recall weight is very limiting for this study. 
- I find it surprising that using multiple attributes in the indexing leads to worse results (lower f-measure and higher runtime). It would be nice if the authors could comment on why they think this might be the case. 

Overall, this is a nice paper with a good story line, but I feel it needs some more work, especially in the area of evaluation of the attributes selection approach and testing of multiple variations of the suggested formula.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/19/2018,12:48,no
345,94,220,Mxx Kexx,4,"(OVERALL EVALUATION) This paper introduces an approach for attribute selection of a set of records for indexing without using machine learning, which claims to make it more applicable to more scenarios. One objective the author seek to attain via indexing is to remove data duplication.

In the methodology section the authors state they use the process in Figure 1 entailing classification yet reiterate that machine learning is not used in their algorithm. This conflict was initially confusing. The methodology also cites attaining a 95% confidence interval, accomplished through iteration. This seems like an ad hoc approach for an experimenter to continue up to an only they receive the results they desire.

I would have liked to see the Real World data sets used alongside the synthetic data sets, even if the former were not shared due to the noted privacy restrictions. Perhaps some anonymized derivative data could be used for a Real World set to ensure Real World applicability. Were machine learning used (as explicitly excluded by design), using the combination of Real Rorld and synthetic data sets would form a stronger basis for classification instead of using wholly fabricated data.

In section 4.3 it is stated that a similarity threshold was empirically chosen but a basis for the value selection is not stated. Why 0.9 and not 0.8 or 0.95? In this same vein, using some sort of similarity metric to validate the synthetic data set to the Real World set would give more credence to the set used.

The cursory application of the procedure to Real (albeit not ""Real World"") restored more confidence in the methodology. I would have liked to see this introduced and juxtaposed to the synthetic sets early on in the paper.

While I began to have doubts in the process used by the author, the methodology seemed sound with my one criticism being the exclusion or Real World data sets. The fact that the author have made the data available prior to publication reinforces the notion of the accuracy of the results without the immediate need for manual validation. I believe the topic area is in scope and would be of interest to the JCDL community.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/19/2018,23:06,no
346,95,303,Sixxxx Paxxx,1,"(OVERALL EVALUATION) The paper presents Augur, a system for early detection of research topics which employs a new community detection algorithm developed for the task. 
The method has been evaluated on a new gold standard of 1408 debutant topics and compared with a series of established baselines.

The paper is clearly written and describes in detail the proposed system. Nevertheless, I have a few remarks:

a) While the authors argue that their method could potentially detect the advent of new research topics in the academic landscape, they evaluated it only on a rather small set of publications in Computer Science (CS). As we all know, this field is absolutely not representative of the overall academic environment and our publication practices are completely different to the ones, for instance, in Biology, Anthropology or Medicine. In addition to this, in CS the most relevant publications are conference proceedings, as opposed to many other fields. For these reasons, the choice of labels to use (or to avoid) when describing a paper is strongly influenced by many social phenomena present in our community and it is different from practices in other disciplines. I would therefore suggest the authors to better clarify that the application of Augur has been at the moment tested only for the CS field.

b) While from a methodological point of view the research is interesting, I find hard to imagine a scenario where ""relevant stakeholders"" will use Augur because they  ""need to react as timely as possible to changes in the research landscape"". I would suggest the authors to offer an example of this or to drop this statement from the introduction.

c) I have an additional criticism towards the automatic generation of the gold standard used in this paper, and in particular concerning the way ancestors have been selected. If I understood this correctly, the authors first find a debutant topic when at least 5 papers mention this new topic in a year. Then ancestors are selected using the equation 8. I am wondering whether, given the fact that the creation of the gold standard is completely based on an algorithm, the best clustering approach would actually be the one that is able to fit this algorithm. To avoid this confusion, I would recommend the authors to add a manual evaluation of the gold standard and the produced clusters. From the topic modelling literature, an approach useful for this evaluation could be the topic intrusion task, from Chang et al. (2009):

- from the gold standard, given a debutant, the authors could present 4 ""correct"" ancestors and 1 intruder to the domain experts and ask them to find the intruder.

- the same practice could be used for evaluating the produced clusters.

Ref:

Chang, Jonathan, et al. ""Reading tea leaves: How humans interpret topic models."" Advances in neural information processing systems. 2009.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,Federico,Nanni,nanni.federico@gmail.com,126,1/31/2018,14:56,no
347,95,20,Suxxx Alxxx,2,"(OVERALL EVALUATION) relevance to JCDL: Many funding agencies are trying to predict emerging research areas in able to make strategic funding decisions. This research works to develop a way to forecast these topics.  Since JCDL has always supported and encouraged cutting edge research that expanded traditional boundaries, it makes this research topic is relevant to the conference.

 novelty/originality: The idea is not novel but the approach has some novel elements which make it a worthy addition to the conference. The temporal approach is promising particularly since the time frame can be flexible. It would be useful to have a greater discussion about how this can identify ideas across disciplinary boundaries.

 methodology: The methodology is  well described and appears sound, but my expertise isnot sufficient to thoroughly review each equation.

 assessment/evaluation/comparison: The paper generally does a good job of discussing the results and it is laudable that data will be publicly available.  

 style/quality of writing: Having read many technical papers, I am used to having equations and other details as an important part of the scientific description and results. However this paper relies on a significant amount of jargon and would benefit from adding some sentences that provide the overall concept of a set of paragraphs in plainer language.  JCDL attendees come from many domains and this would help make the paper more accessible to the broader audience. Given the important topic and the interesting findings, appealing to a broader audience is an important aspect to consider. 

 replicability: There is sufficient detail to provide a framework for replicability. Whether the results would be replicable cannot be determined. 

 adequacy of references: References are excellent.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/15/2018,21:40,no
348,95,390,Jenxxxxxx Wxx,3,"(OVERALL EVALUATION) This paper proposes to forecast the emergent topics at an early stagte based on 
improvement of the existing work.

In general, the paper gives descriptions of the technical details and experimental results.

But there are some issues to be addressed:

1. Some technical details were not clearly explained.
For example, in Sec.3.1, some symbols in Eq.(2)-Eq.(5) were not clearly defined.
Specifically, definitions of evolutionary network, pace of collaboration, pace of growth
are not clear.

(1) During the 1999-2009 time period, what is t in Eq.(2) ,
and why are there 5 semantice enhanced topic networks?

(2) Similarly, in Eq.(3), what are u, v?

(3) The definitions in Eq.(4) and (5) are not clear.

(4) In Eq.(6) in Sec.3.2, what are a, b, c?

(5) Some notations need to be clarified.
For example, the symbol k was used to denote both the k-th vertex
and the k-cliques.

(6) What's the difference between Wi and Wj in Eq.(7)?

2. There are some typos:
(1) In Algorithm 1, what is Eq.5.7?

(2) In step 3 of ACPM, it should be ""k-cliques"" instead of ""3-cliques""

3. Some explanations of equations were given much later than they were referred.
For example, Eq.(3), (4), and (9).
It could be confusing when readers first read the equation
without knowing the notations first.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/19/2018,3:28,no
349,96,404,Dxx W,1,"(OVERALL EVALUATION) The poster tries to compare two eye tracking tools, Tobii x2-30 and Gazepoint GP3 HD. However, there are no researach questions, experiment settings, not any data analysis in the poster. The author should state clearly in what scenario to compare the two eye tracking tools, how to compare them, and what are the results.

This is not a research poster. It even does not use ACM format and has no abstract and keywords. I do not think it is ready for the conference poster.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/16/2018,20:14,no
350,96,92,Mixxxx Doxxxx,2,"(OVERALL EVALUATION) The proposal addresses a relevant topic but has some shortcomings:

1) Lack of contextualisation - examples of previous studies using eye tracking in the digital library domain.  . 
2) The work is at a very early stage based on the submitted abstract - it is hard to assess how advanced the comparison of the two trackers would be and what would be the benefits for the conference audience. While the comparison could still be forthcoming, one expects to see a more robust planning how two eye tracking devices would be compared. What would be the exact methodology? It is not entirely clear from this submission.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2/16/2018,22:07,no
351,97,91,Yixx Dxx,1,"(OVERALL EVALUATION) This short paper investigates coauthorship networks in Brazil and mines the interdisciplinary collaborations. At least two points are needed to address in this paper:
1.	More details on how Figure 1 was constructed should be provided, e.g., the layout algorithm, the percolation of the edge weight, etc.
2.	This paper lacks of some deeper discussions on their findings. For instance, what do these findings essentially mean and imply (e.g., your network-related findings)? Also, the authors should try to list more implications in their Conclusion section.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2/8/2018,18:17,no
353,97,282,Erxxx Nexxxx,2,"(OVERALL EVALUATION) The paper covers an interesting subject but it will need some improvements before it can be presented in the conference.

First of all it claims that interdisciplinary cooperation improves research However it needs to present a metric and an evaluation for that or at least has to cite/evaluate a paper where such an analysis has to be shown. Claims like that have no value if not substantiated. Such a claim is not really necessary for the study. I find one aspect you do not utilize sufficiently is your mention of the Mena-Chalco study and the differences to your study. Why not also include, like they do, ?€?novel?€? researchers as a second example to compare the over the time development in cooperation.

The clouds you show in Figure 1 need more explanations. What reflects the distance from the center? What is the black circle in one and the gray surrounding in the other? What is the placement of the 8 areas in the circle?

Has the path length anything to do with the quality of research? Is it different for different disciplines? We know that some have many co-authors others only a few. Just giving an average without a mean and even a distribution is of limited value.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) As a short paper it is acceptable but still my rather easy improvements should be enforced before it can be published.

===================================

Looking at my own evaluation I believe it is more in line with 'boarderline' or 'weak reject' if I assume there is not time enough to repair the problems I mention.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/17/2018,9:47,no
354,97,407,Jixx W,3,"(OVERALL EVALUATION) This paper deals with the research on how important interdisciplinary collaborations are on scientific coauthorship network. To support the idea, authors compare the relationships of interdisciplinary collaborations between global coauthorship network and interdisciplinary coauthorship network in eight major research areas. At the same time, authors give a good explanation of related works to strengthen the advantage of their research, that their work focuses on the importance of interdisciplinary collations rather than on the analysis of the actual coauthorships. The conclusion that the Brazilian coauthorship network grew and became more interdisciplinary is drawn through a clear analysis on the data.  

As the authors state, the research was carried out on mining a complete and robust scientific community, rather than specific groups in traditional studies. This idea is the primary contribution of this paper. The related works are truly convincing to support the advantage of the research. This paper gives a good view of author?€?s work. 

However, the conclusion is drawn so easy, since the topic is about ?€?how important?€??€?. I hope I could have seen more evidence on the weakness of some scenario with weak interdisciplinary collaborations. It seems authors focus more on illustrating the proportion among the research areas. It is likely showing facts, not reasoning. 

Also, there are some grammar and editorial problems. Such as,
?€??€?, which emphasizes how important are interdisciplinary collaborations in a more interconnected world.?€? Should be ?€??€?, which emphasize how important interdisciplinary collaborations are in a more interconnected world.?€?
?€?On the other hand, Mena-Chalco et al. analyze a network that include other kinds?€??€? should be ?€?On the other hand, Mena-Chalco et al. analyze a network that includes other kinds?€??€?
?€??€?in order to comparison?€??€? should be ?€??€?in order to compare?€??€?
And a serious mistake occurs in analysis section, that 11.6% should be the data of ?€?Health Sciences?€?, not of ?€?Agrarian Sciences?€?.
Besides, the Figure 2 has an obscure legend.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,Jing,Zhao,zhaojing.thtf@gmail.com,355,2/17/2018,3:11,no
355,97,407,Jixx W,4,"(OVERALL EVALUATION) This paper attempts to address how important interdisciplinary collaborations are on scientific coauthorship network. The authors compare the global coauthorship network and interdisciplinary coauthorship network in eight major research areas. The data is from the Lattes Platform which is a large academic network in Brazil, containing researchers' curricula vitae and research groups. The authors found that about 37.4% of all collaborations revealed in this network are interdisciplinary.  the Brazilian coauthorship network has a strong interdisciplinary component is drawn through a clear analysis of the data.  

As the authors claim, the research was carried out with the purpose of mining a complete and robust scientific community, rather than specific groups in traditional studies. However, the major contribution of the paper does not address the level of the importance of multidisciplinary collaboration. It merely shows the phenomenal facts that they seem to be strongly bonded from that particular dataset. The authors did not describe to any extent how authors of different disciplines are identified technically and how this connection shaped the scientific community and/or how the interdisciplinary connections (rather than intra-disciplinary connections) impact the scientific outcomes. The entire work is just analytical rather than algorithmic and there is a lack of insight into the drive and consequence of the analytical results. In addition, the authors did not perform any time serial analysis, but they claim in the abstract that the ""Brazilian coauthorship grew"". 


There are many editorial problems. Some examples are listed below:

Page 1. ""which emphasizes how important are interdisciplinary collaborations in a more interconnected world."" Should be ""which emphasize how important interdisciplinary collaborations are in a more interconnected world.""

Page 3. ""On the other hand, Mena-Chalco et al. analyze a network that includes other kinds?€?"" should be ""On the other hand, Mena-Chalco et al. analyze a network that includes other kinds?€?""

Page 3. ""in order to comparison?€?"" should be ""?€?in order to compare?€?""

Page 3, based on Figure 2, 11.6% should be the data of ""Health Sciences"", not of ""Agrarian Sciences"".","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/17/2018,3:17,no
357,98,275,Fedxxxxx Naxx,1,"(OVERALL EVALUATION) The paper presents a new system for extracting phrases answering the 5W1H questions from English news articles. The paper is very well written, the system is clearly described and the evaluation setting is well designed. 

While I have a few important remarks on the paper, which I listed below, I would start by saying that this research addresses a very interesting problem for the digital library community and therefore I encourage the authors to continue working on this, following what they state in the ""Discussion"" part of the paper.

Regarding event-detection:

There is a very large body of literature on event-detection in information retrieval (starting from the ""topic detection and tracking"" and ""first story detection"" tasks) and in NLP. While it is evident that the authors know some of these trends, I would strongly advise them to offer a more clear background on the different approaches presented in almost 20 years of research on the topic. Sprugnoli and Tonelli [1] recently published a very clear survey on event detection, that the authors could use as a starting point.

Regarding the system:

The main point of this paper is that the highly specialised systems already presented in the literature cannot be adopted in their research project as these are not publicly available. I completely agree with the authors that is important to address this problem, which is a very pressing issue involving the NLP and other application-oriented research areas, such as digital libraries. 
However, I find striking that also the authors of this paper are not directly sharing the code of their system to the reviewers. In particular, I find the sentence ""If you are interested in reviewing the code before the paper has been accepted, please send an email to felix.hamborg@uni-konstanz.de to retrieve the code and datasets."" unacceptable, given the anonymity of the peer review process. The authors should be very careful with this type of suggestions. Next time, just add a link to the code on GitHub.

Regarding the annotations:

I have found the creation of the dataset well defined and conducted. For the annotations of the output of the system in the 3-point scale, I would suggest the authors to follow the word-intrusion task, from the topic modelling literature [2], and add also some false examples, for instance answers to the ""Where"" question taken from another document, as a sanity check.

Regarding the evaluation:

While I agree with the authors that most of the previously published systems are not publicly available, I disagree on the fact that they are not described in sufficient details to re-implement them. In particular, the paper by Wang et al. (2010) clearly states how to address the ""How"" question using their approach:

""SRL parser returns all possible labeled results for each topic sentence. If a topic sentence is long enough to include a <Subject, Predicate, Object> triple, the entire clause from subject to object is selected.""

The authors could adopt SEMAFOR as a semantic role labeler: http://www.cs.cmu.edu/~ark/SEMAFOR/

In addition to this, simply reporting numbers in a table (as in Table 6) without any type of comparison with any type of baselines (even very naive ones, such as a random baseline) does not tell much to the reader. Given the fact that the authors know the literature on the topic, I would suggest them to a) re-implement an independent baseline for each step from other previous papers (e.g., a baseline for ""who"", a baseline for ""why"") or b) de-compose their system and evaluate each step; for instance what is the impact of the use of copulative conjunctions in the method extractor - does it really offer a boost in performance?

Moreover, it is not possible to compare numbers across papers, as the authors do in page 8, when the approaches are tested on different datasets or address different tasks. For example, Parton et al. (2009) not only does not address the ""how"" question, but the method is tailored for cross-lingual 5w detection.

Regarding the final output:

The authors remark that they address all 5W and 1H questions with their pipeline, however the performance on ""why"" and ""how"" are extremely poor compared to the others 4W questions. MAgP of 0.32 and 0.36 make their system not usable for any digital library or journalistic applications. I would highly recommend the authors to divide their work in two different projects:

a) make a 4W system available to the research community, which already seems to have really good performance
b) focus their effort on improving ""how"" and ""why""; results on this type of research could be very relevant for the NLP community.

--------


[1] Sprugnoli, Rachele, and Sara Tonelli. ""One, no one and one hundred thousand events: Defining and processing events in an inter-disciplinary perspective."" Natural Language Engineering 23.4 (2017): 485-506.

[2] Chang, Jonathan, et al. ""Reading tea leaves: How humans interpret topic models."" Advances in neural information processing systems. 2009.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I do not think the authors had bad intentions with the sentence:  ""If you are interested in reviewing the code before the paper has been accepted, please send an email to felix.hamborg@uni-konstanz.de to retrieve the code and datasets.""

However, suggesting the anonymous reviewer to get in touch with the authors outside easychair-JCDL2018 should be highly discouraged.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/19/2018,8:08,no
358,98,96,Antxxxx Doxxx,2,"(OVERALL EVALUATION) This paper presents a system for extracting the main event descriptors from news articles, aiming to get answers to the 5W (who, what, where, when, why) and the 1H (how) questions.
    
    The main announced contribution is the promised release of software (with source code) and manually annotated ground truth, in a field where much of the state of art is based on concealed code and datasets. However, this latter key claim of the paper is to some extent exagerated. While I'm not either aware of datasets that would address the 5W1H questions, there are several existing datasets that cover some of the questions, the most famous being the ones distributed within the message understanding conference series (MUC). The tasks on terrorist event detection in Latin America for instance very directly covered several of the W questions (MUC-3 and MUC-4). Also, a few domain-specific system are available, together with their data sets (for instance in epidemic event detection from news, over the who, where and when questions). Perhaps the reason why a good share of related work was missed by the authors is the early assumption that their 5W1H problem relates essentially to QA (beginning of section 2, while a lot of related work lies in the neighbouring field of information extraction (IE).

    While this tampers the motivation of the work, the effort to build 1) a generic pipeline 2) for any domain event detection and 3) to share it with the community all form a very worthwhile contribution. The system assumes as input prepared ""clean"" documents, which is in practice very difficult to obtain, both for online contemporary sources (the generic extraction of title, content and paragraph divisions is difficult in practice due to different formattings in every source, commercials, distribution on multiple pages, copyright notices, etc.) and for digitised newspaper (article reconstruction, OCR issues, classification of actual news articles versus novels, columns, advertisements, obituaries etc.). It also seems to assume that all articles contain an event, which is unrealistic (think of summaries, in-depth analysis, etc.). These issues are probably to be handled priorily to running the system.
    A big concern about the system is its efficiency. It includes several inefficient linguistic analysis tools (POS-tagging, full parsing, corerence resolution, NERD), to be applied to every single sentence in every article. It would be good to get some further background on this. This is hopefully something that can easily be tailored in the source code.  
    
    The evaluation is the biggest problem. There is a number of critical flaws in it. The biggest one is that the whole evaluation is run on a corpus of 60 news articles split into 6 categories (10 documents per category). This is enough to question any of the results that follow. In addition, the parameter learning section uses 100 articles to fix optimal parameters, determining 692 configurations, from which the best one was picked ""by hand"" due to ""correlated parameters"" (methods such as principal component analysis would generally be more appropriate to handle something like this). Oddly, the assessments used for parameter learning were binary, while the assessments for the evaluation were made on a 3-point scale.
    As mentioned earlier, comparative evaluation could have been performed using existing data sets, even if only for some of the W questions. With tiny data sets assessed on different scales, it is hard to trust any of the conclusions in and subsequent to sections 4 and 5. 
    
    The paper is clear and well written. There is some redondancy at the end of section 5 (with the end of section 4), about ICR results (ICR is not defined by the way - and why not use generalized Kappa?). Access to the data is inconvenient to the reviewers since one should send an email to the 1st author. As JCDL reviewers are usually anonymous, this is a problem (I understand the author wants to know who sees the source core at this point, but the data set would have been interesting to see, or even a sample of it)

    In summary, the scientific contribution of the paper appears too slim, only combining existing tools and lacking to provide sufficient evaluation. However, the software contribution presented appears very worthwhile for the community, provided a corpus of articles can be input in a appropriate shape. I fear this is not possible for JCDL 2018, but if it is, this paper would provide a very strong demonstration paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I don't dare to give this out as I'm a co-author, but the following points to a corpus with published evaluation over 3 W questions (2015):
    https://daniel.greyc.fr/public/index.php?a=AboutDAnIEL
    https://daniel.greyc.fr/public/corpus_daniel.tar.gz","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2/18/2018,14:57,no
361,98,419,Masxxxxxx Yosxxxxx,3,"(OVERALL EVALUATION) The authors have developed a publicly available 5W1H extraction system
for news articles named Giveme5W1H. The authors made an extensive
review of literature in this research field and integrated several
existing tools and software to yield better performance.
The contribution of the authors to the research community can be
acknowledged. However, the reviewer could not find strong original
technical contribution in the paper.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/20/2018,13:33,no
362,98,374,Domxxxxx Tkxxxx,4,"(OVERALL EVALUATION) The paper describes a system for extracting the answers to journalistic 5W1H questions from news articles. I believe this is an important topic, especially since such articles are still being published in an unstructured, non machine readable format. The described approach seems sensible and gives good results. Even though this is not the first attempt to solve this task, I agree with the authors that lack of published datasets and code is a serious issue which holds back future research and needs to be addressed. For these reasons, I believe this is a valuable paper, and along with the dataset and code will be very useful for the community.

Some smaller issues and questions:

1. In the abstract the authors report ""extraction precision"", referring to ""mean average generalized precision (MAgP)"" described on page 8. Since ""precision"" is an evaluation metric used widely in classification, I would suggest giving the full name of the metric in the abstract, so that the reader is not confused.

2. I would also rename section 2 into ""State of the art"" or similar. The current title suggests this section covers the new proposed approach.

3. Page 7, ""For 'when' candidates, we computed the difference in seconds between candidate and annotation. For 'where' candidates, we computed the distance in meters between both coordinates."" - I wonder whether this is an appropriate metric of similarity. For example, if the wrong time expression was extracted from the text, I doubt it matters whether this wrong time is (by random chance) closer or farther from the ""true"" time. It seems to me a more discrete metric (correct or not?) might be more appropriate. It would be good if the authors could comment on this issue in more details.

4. Evaluating described task seems like a separate challenge. The authors asked human assessors to judge the results of their approach and the results of those judgments are reported in the paper. One problem with this evaluation approach is that it will be difficult to compare potential future alternative extraction approaches to the results from the paper. Have the authors consider an alternative evaluation approach, such that the results are judged automatically by comparing them to ground truth?

5. It would be also good to add some comments on the impact of errors occurring during preprocessing (if any) on the final results.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/20/2018,18:08,no
363,99,291,Denxxxxx Pexxxx,1,"(OVERALL EVALUATION) The paper proposes a subject-based document retrieval approach based on a score that combines keywords in the document and terms in the query.

The topic is interesting for digital libraries. However, it has been much discussed in the literature over the last decades. A new proposal should provide clear evidence of superiority. However, the paper does not provide such evidence in a convincing way. The experimental evaluation is simple, only based on the opinion of users for 10 queries.

Other observations:
-- In the Introduction, the authors should present how the method was evaluated and the main results.
-- In Related Word, make clear the difference between the present work and that of the reference [10]. This section should also contrast the present work with those of literature, which was not done. In addition, the entire section was written in a single paragraph. It should be better organized by approach.
-- The text contains several typos, spellings, and incomplete sentences.
-- There are few recent bibliographical references, and an incomplete reference.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2/8/2018,13:58,no
364,99,79,Faxxx Crexxxx,2,"(OVERALL EVALUATION) This paper presents a scoring scheme for improving retrieval of relevant documents in a digital library context. For this purpose the authors use a collection of papers as their dataset. The idea is to rely on the categories or ""subjects"" that are manually associated with the documents by human experts. Such categorization is usually present in digital libraries. They combine their scoring system with that of BM25 in a linear interpolation. 

The idea is interesting in that they convert each document as well as query to a vector of subjects, where each element of the vector represents the corresponding probability of a subject. 

The evaluation is a case study presenting the ranking results of their approach as well as that of BM25 to 10 participants each looking at the results for 10 queries. The results show that in 83% of cases, the participants preferred the ranking results of the new approach over BM25.

There are however some concerns:

1. The only used baseline is BM25. Since the authors try to propose a semantic approach, including baselines such as LSA, LDA, or word2vec would have been appropriate. I would have been more positive about the paper if these baselines were included.

2. Usually the cosine similarity is preferred over the Euclidean distance in vector space. I wonder what is the reason behind not using it?

3. The equations should be carefully checked. For example, I don't understand why should there be a joint probability in Equation 6, though this does not directly interfere with the correctness of other equations. Are you making some assumptions about the data?


A few comments on the writing of the paper:
The paper is generally well written and organized. However, there are a number of typos and grammar mistakes in the paper. Also, there are some strange characters as a result of Latex output in the paper, e.g. in the related work section.

There is a comma in the title. Reference 1 starts with the year. The writing should be consistent. For example, you write ""database"" and also ""data-base"". 

Additionally, the literature review is almost a page and yet a single paragraph. I suggest breaking it down to paragraphs to improve the writing.

Over all, given the above mentioned points, as well as the relatedness of this paper to JCDL I recommend a week accept.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,Seyed Ali,Bahrainian,seyed.ali.bahreinian@usi.ch,356,2/12/2018,1:22,no
365,99,328,Raxx Schxxxx,3,"(OVERALL EVALUATION) The paper considers the problem of retrieving documents from a digital library. Instead of using title information only, the proposed method relies on user-defined keywords and subjects. This is not really a novel idea. The actual implementation represents documents in terms of a subject probability distribution derived from the user-defined keywords. Existing methods such as SVD follow a similar line, but do not rely on predefined subjects or keywords; the paper lacks a comparison to these existing methods.

The paper relies on user-defined keywords and subjects. It is not clear if such an annotation exists for every digital library, and which quality the annotation can have. Full-text search on the abstract may be a more promising approach than relying on keywords and subjects. The granularity of the set of subjects will also contribute to result quality. In general, the idea of using an intermediate subject layer is new, methods like SVD have done this for a long time without the need for an explicit subject definition. The proposed method represents a document with a subject vector, where the subject weights are generated based on the keywords assigned to the document; this is not exactly a probability distribution, but is computed in a similar way.

Either Equation (7) of the following explanation of ds^i_m is wrong; from the explanation, it seems that i (and the sum in Eq. 7) is not needed.

The final subject-based score is computed as the inverse of the distance of the document subject vector and the query subject vector; this is clearly not a proportional relationship. It is unclear why not a more standard cosine similarity is used. Algorithm 1 is not needed since it does not add anything new.
The paper proposes to linearly combine the subject-based score with a standard BM25-based score (where it is unclear on which part of the document this score is computed); the paper does not really explore in the experimental evaluation how the different parameters of the models should be set.

The experimental evaluation is not satisfactory for a full paper. The data set under consideration is not public and collected from a private site, so it would even be difficult to reconstruct the collection. It is unclear why it was necessary to classified the subjects into eight main subjects; this is not used anywhere.
It is fully unclear why the paper does not compute the combined score for all documents. There is no point in making performance optimization at a stage when result quality is evaluated; this should be a second step. There is no need to explain the example query right before Table 1 in such a detail; all computations are trivial.
The paper then proceeds to evaluate the proposed method. A first set of results is presented for a single query (!), this includes the result on the impact of the \alpha variable. The measure for the amount of changes is strange, the paper should have used a standard measure for comparing ranked lists. There is no point in such an analysis with a single query; in addition, simply considering the amount of change does not tell anything about result quality.
For the following experiments, it is unclear why \alpha=0.3 was chosen, especially if that tuning was done on a separate set of queries. For the actual test that follows, 10 queries were defined, seemingly with a title only; here, a more precise definition of the information need would have been required. A simple evaluation such as ""is ranking list A or B better"" seems like a hard decision if the information need is not known. It is unclear why not a more standard evaluation like precision or ndcg was used together with a per-document assessment procedure.","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2/12/2018,16:51,no
367,100,90,Yixx Dxx,1,"(OVERALL EVALUATION) This paper tries to diversify the citation contexts in academic literature for knowledge recommendation. I personally pretty much like this article. Nevertheless, I here provide several minor suggestions and hope the authors can consider: (1) In Methodology section, the authors should first use natural language to provide an overview of the given section for readers, and then provide mathematical notations. Finally, some examples should be provided. (2) More details should be provided in the section on evaluation. (3) More implications (not simply methodology-related implications) could be discussed in the Conclusion section.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,Yi,Bu,buyi@iu.edu,353,3/9/2018,4:56,no
368,100,242,Kxx L,2,"(OVERALL EVALUATION) The study aims to diversify citation contexts in order to improve the efficiency of applications based on citation contexts. Citation context is a well-studied topic in information science. To diversify citation contexts and provide a list of citation contexts with minimum overlaps helps to obtain a comprehensive view of how a work is cited. The study adopts search result diversification methods from information retrieval field to diversify citation contexts. The application of the methods seem reasonable. The diversified lists are then presented to users for assessment. The results suggest the diversified lists are favored by the users in terms readability, diversity and usefulness. I understand the paper focuses on introducing the diversification task for citation context. In future study, maybe some newer diversification methods may be used. Some of them have been cited in the current paper.
The structure of the paper is pretty clear. However, there are many details not clearly explained, in particular, the user study procedure: 
1.	Please specify which items from Table 2 measure Readability, Diversification and Usefulness, respectively. 
2.	Does each respondent read all four lists, or are they randomly assigned to one list? If they read all four lists, what is the sequence? How to control carry-over effects? Do they answer the open questions after reading each list, or after reading all lists? Where is the evaluation conducted? Do the subjects read in a lab environment, or they complete the tasks online?
3.	page 7, it?€?s not clear where ?€?unsure?€? comes from because earlier it describes ?€?3-point Likert scale ranging from agree (3) to disagree (1)?€?. Is there a fourth option ?€?unsure?€?? And then, ?€?we further changed the instrument in to binary measurement: bad or good?€?. However, the survey questions are more of agree or disagree questions, why use bad or good? This is not clear.
Although the paper is mostly readable, the writing needs to be improved. There are numerous grammar errors.
Minor:
1.	page 4, ?€??€?were cited between 15 and 100 times, i.e. each had 15 to 100 citation contexts, ?€??€?. Well, ?€?cited 15 and 100 times?€? does not necessarily mean 15 to 100 citation contexts as one paper can be cited multiple times in the full-text of the citing paper. 
2.	page 4, section 3.2.2, is average pairwise similarities between words used for similarity between citation context? This is not clear.
3.	page 4, below equation 1, ?€?where d_i is the citation context with highest score in one round of iterative selection.?€? d_i should be c_i.
4.	page 4, ?€?S is the re-ranked dataset?€?, re-ranked list?
5.	The paper seems to use correlation and similarity interchangeably. The two concepts are not exactly the same. Please use consistent terminology. 
6.	page 4, ?€?we set the same correlation degree function for sim_1 and sim_2?€?, what is the correlation degree function?
7.	What does D represent in equation 2? What is ???
8.	Which one is penalty coefficient? This is not explained.
9.	Something common as kappa score does not need an equation, just provide a reference. 
10.	Citation styles are not consistent.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/14/2018,20:27,no
369,100,322,Horxxxx Saxxxx,3,"(OVERALL EVALUATION) Given a paper which has been cited a considerable number of times, the problem addressed  by this paper is how to show the citation?€?s contexts which will cover the various aspects considered by the citing articles avoiding redundancy.

The paper describes a number of techniques to rank the set of citation contexts to try to reduce redundancy. Among the techniques used to measure similarity between contexts are one based on WordNet and another one based on semantic analysis. To re-rank the contexts marginal maximal relevance and a diversity score are proposed.  

The proposed approach is evaluated in a CiteSeerX dataset relying on human judgement who assessed how readable, diverse, and useful the list of citation contexts is.

The problem studied in this paper is relevant and appropriate for the conference. But the paper has a number of problems as far as I can tell.  First the similarity methods are not well explained: semantic analysis is not explained and the use of WordNet is unclear, after all you compare sentences and not just individual words. 

With respect to the re-ranking or diversification strategies, the formulas for MMR are incorrect or at least the variables you mention (d and S on page 4).
Besides, some of the evaluation metrics, although well known,  are not explained or referenced at all.

The paper also has some methodological problems (for example the evaluation setting; first use of a 3-point scale and then binary).

I also found some problems with English, specially in the  experimental section.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/14/2018,22:05,no
370,101,1,Trxxx Aaxxxx,1,"(OVERALL EVALUATION) This full paper presents the results from a quantitative study (survey) and a qualitative study (interviews) on how users create and organize associations/links between documents they keep and use in their own workspace. The paper is well structured, with good use of tables and figures, and is easy to read. The contribution is well aligned with the topics of the conference and will be of interest to the general conference participant. I particularly like that the authors have a systematic approach with scenarios and mechanisms, and that they emphasise the impact of the findings by listing design implications. 

My main concern, and the reason for my somewhat reserved score, is that I find the analysis to elaborate on and emphasise details, but the results do not really establish novel insight beyond saying what x% of the participants have done. The results could have been more contextualized, and I miss a systematic discussion of the validity of the results. The majority of participants are master and phd-students, and we can assume they have a specific study situation that not necessarily can be generalized to the broad community. I would also have liked to know the kind of research the participants were doing as I suspect there will be a difference between sciences and tasks.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/14/2018,9:41,no
371,101,51,Gexxxx Bucxxxx,2,"(OVERALL EVALUATION) Generally this is a good paper, but there are some limitations that it would be helpful to address in a final version.

One issue is that the scholarship (references) have been somewhat compressed to fit in more body text. As a result, some useful material is left out that would be really helpful to have here, and a broader context is absent that really should be present.

In particular, while PIM and Annotations are both developed subjects, the material presented here omits some of the more recent work done. Jennifer Pearson's work on annotations, while focussed on digital readers, would be pertinent here, or perhaps, more speculatively, also the work of Craig Tashman. Pearson noted that users often fold books into each other as a form of annotation, for example, and this is just one case of physical constructs that are overlooked here. Such issues are a necessary product of the research method, but they need to be discussed. Furthermore, how annotation works with multiple documents has had at least some coverage, again through Pearson, but also Masood Masoodian's work on enabling recall of filed documents drew on primary material.

Furthermore, to take one point, we are told that ""The use of annotations as well as highlights enables participants to establish associations at any level of granularity since participants are able to highlight and annotate any part of a document. Annotations and highlights also yield to di erent types of bidirectional and unidirectional associations"" - what's exactly meant here by 'granularity'? Also, if a part of a physical document falls beyond a single page, this isn't always possible either. There are implicit constraints weighing on the researchers' minds that need to be made more explicit. Granularity is mentioned later, in 4.2.1, but I'd recommend a bit of re-ordering here.

Overall, some editing will be required to save space, and give scope to explain the issues that are over-compressed, but some of the minor observations may usefully be dropped, and the reader given more guidance. Where prior research findings are confirmed, compression of the narrative is probably appropriate.

Overall, I like this paper but a lot of editing is needed to make it ship-shape.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,2/17/2018,7:24,no
373,101,166,S.M.xxxxxxxx Haxx,3,"(OVERALL EVALUATION) This paper presents a study on user behavior when associating information across physical and digital documents. *The paper is suitable for human-computer interaction (HCI) conferences.* 

The ideas presented in the paper are interesting, but the paper suffers from several weaknesses. 

1. The authors did not provide a clear description of the questionnaires they used for data collection. Hence, it is difficult to replicate the study to validate the results.

2. The authors employed the knowledge worker population (MS students, PhD students, and researchers holding PhD degrees) in their study. However, they did not provide a discussion about their research areas or domains of expertise.  Usually, computer science or engineering researchers are more proficient with advanced software features than others because of their educational training. Therefore, it was not clear whether or not the knowledge workers' areas of expertise imposed any bias in the study.

3. A limited number of document-linking software products were mentioned in the paper. However, many of these are available with advanced features. For example, the paper did not mention Mendeley. Moreover, authors may consider software such as Jupyter Notebook to expand the scope of their research.
 
4. The result section's description was too long. I think this section can be shortened because most of the results are understandable from Table 1.
  
5. Some of the design implications looked obvious (e.g., DI3: Documents side by side). Hence, they did not provide concrete research contribution. Moreover, authors did not specify any implementation approach for the design implications.

6. The references mentioned in the paper were old. I think the authors need to study recent related literature.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper may be considered for a short paper or a poster.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/23/2018,9:56,no
374,101,273,Uxx Muxxx,4,"(OVERALL EVALUATION) The authors present results of a user study on cross-document linking. Given that this topic has been researched extensively before and several findings are re-affirmation of previous findings in the context of cross-document linking, I recommend this paper be converted to a short paper.

relevance to JCDL
- topic of the paper, dealing with human perspective of cross-document linking is highly relevant to JCDL

novelty/originality, adequacy of references
Overall, the background work can be enhanced with more context and the authors should consider adding well-framed measurable research questions to develop contributions of the paper.

The authors present results from a user study focusing on cross-document linking. While I agree that a user study focusing on cross-document linking has not been done before, several findings are affirmation of findings from previous studies. ""The Need for a Linking Tool."" was researched in works as early as Ted Nelson's Xanadu. The main problems with digital annotations and linking, especially in scholarly, knowledge-based tasks include: 1) dealing with the lack of affordances provided by paper, 2) working across and managing multiple document formats, and 3) managing new information (annotations and links) in the context of existing information. Given that these problems have been established and researched before, the novelty of this research is unclear. 

The subsuming topic of document referencing and annotation has been extensively researched before, including:

Annotations
C. C. Marshall, ?€?Annotation: from paper books to the digital library,?€? in DL 97: Proceedings of the second ACM international conference on Digital libraries. New York, NY, USA: ACM Press, 1997, pp. 131?€?140.

A. J. B. Brush, ?€?Annotating digital documents for asynchronous collaboration,?€? Ph.D. dissertation, University of Washington, Department of Computer Science, 2002.

M. Winget, ?€?Annotation of musical scores: Interaction and use behaviours of performing musicians,?€? Ph.D. dissertation, SILS, UNC-Chapel Hill, 2006. [Online]. Available: http: //www.ischool.utexas.edu/???megan/Winget final-Rev2.pdf

M. Agosti, N. Ferro, and N. Orio, ?€?Annotating illuminated manuscripts: an effective tool for research and education,?€? in JCDL ?€?05: Proceedings of the 5th ACM/IEEE-CS Joint Conference on Digital Libraries. New York, NY, USA: ACM Press, 2005, pp. 121?€?130.

?€?A formal model of annotations of digital content,?€? Transactions on Information Systems (TOIS), vol. 26, no. 1, pp. 1?€?55, 2008.

Superimposed information
Uma Murthy, Lin Tzy Li, Eric Hallerman, Edward A. Fox, Manuel A. Perez-Quinones, Lois M. Delcambre, and Ricardo da S. Torres. 2011. Use of subimages in fish species identification: a qualitative study. In Proceedings of the 11th annual international ACM/IEEE joint conference on Digital libraries (JCDL '11). ACM, New York, NY, USA, 185-194.

Uma Murthy, Digital Libraries with Superimposed Information: Supporting Scholarly Tasks that Involve Fine Grain Information, Ph.D. dissertation, Virginia Tech, Department of Computer Science, 2011.

PIM
William Jones, Ammy J. Phuwanartnurak, Rajdeep Gill, and Harry Bruce. 2005. Don?€?t Take My Folders Away! Organizing Personal Information to Get Things Done. In Proceeding of CHI 2005. Portland, USA.

methodology and assessment/evaluation
The authors conducted a survey and follow-up interviews with participants. Most of the paper includes results form the survey, with some quotes from the interviews. This is good way to get holistic data on questions. 

style/quality of writing
While the authors provide detailed information in the paper, the style of writing can be more succinct to improve readability ad comprehension. In several instances, the authors use multiple setences to convey a single point. These can be combined to improve comprehension. For example: page 6, 4.2.2:

In this scenario, 62.6% of the participants tend to associate information. Most participants have adopted various mechanisms to create associations between pieces of information in different documents of the same document type
-->
62.6% of the participants have adopted various mechanisms to create associations between pieces of information in different documents of the same document type

There is also several repetitions of the phrase: ""it is worth noting"", which should be there for truly insightful results. 

Other nit picks:
Page 1, section 2:
There have been a number of studies in di erent domains re- vealing some interesting findings regarding the user behaviour when associating information within and across physical a
-->
rephrase. this seems a little odd considering you start your abstract claiming the opposite 

Page 3, section 3.1.1
Note that the scenario names and their abbreviations, which are also illustrated with an example in Figure 1, are extensively used in the remainder of this chapter.
-->
Note that the scenario names and their abbreviations, which are also illustrated with an example in Figure 1, are extensively used in the remainder of this paper.


Page 3, 3.2
While 97 participants have provided us their email addresses, we only selected 12 participants for an additional interview.
-->
Mention how you selceted these participants

Page 5, section 4.1
to associate information in both, scenario SP and MP.
-->
in both scenarios, SP and MP


Page 6, section 4.2.1
The contradiction between our finding and the previous study finding might be explained in different ways. In the study of O?€?Hara et al., participants were, for example, obliged to use a specific ?€?old?€? document viewer for reading documents. Some participants might not have been familiar with the annotation features o ered by Microsoft Word 6.0
-->
rephrase: you give one example, yet write that finding might be explained in different ways

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The authors present results of a user study on cross-document linking. Given that this topic has been researched extensively before and several findings are re-affirmation of previous findings in the context of cross-document linking, I recommend this paper be converted to a short paper.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/24/2018,17:16,no
377,102,23,Roxxxx Alxx,1,"(OVERALL EVALUATION) Most of this work is technically solid and the ultimate vision of nuanced techniques for extracting the claims made by research papers is worth pursuing. However, the paper is algorithm driven and we don?€?t get a seat-of-the-pants sense of what was really found and why that is of value I?€?d like to see some descriptive statistics comparing the documents marked as ?€?strong witnesses?€?? In addition, I?€?d like to see some specific examples of what the programs found as results in the sub-sections. What are examples of Author Commitment, of Claims, etc

The main point seems to be to show an example of how the ?€?Skyline Operator?€? analysis can be applied to text processing of scholarly material. The specific construct the authors evaluate they call ?€?strong witness?€?. To accomplish that, there?€?s a complex set of descriptions, relevant conceptual frameworks, of studies developing data sets, and then evaluations. 

To organize this review, I consider the paper section-by-section:

Introduction/Section 2 ?€? Could you provide a some examples of strong witness and what do you expect readers to do with a paper that is a strong witness. Wouldn?€?t a highly readable review paper be more useful to readers in most cases? 

The discussion of argument mining is interesting but since (as you note) much of that work doesn?€?t use techniques similar to yours, it's notclear why that discussion is relevant.

Section 3 ?€? While Section 2 set us up to focus on text processing, here we get two dense pages that covers everything from formal definitions, to linguistics, to cognitive science. Given the breadth of the paper, this is all relevant but I wonder if there?€?s some way to help the reader see how it all hangs together.

Section 4 ?€? This starts with the Research Questions. I would normally expect the Research Questions to be in the Introduction. They are too high-level to be in a section titled ?€?Experimental Setup?€?. Moreover, this section has more than the ?€?Setup?€?. It includes Results and Discussion. At the least, it could be renamed just ?€?Experiment?€? but I think it would be better to divide it into several sections ?€? or use numbered subsections.

We don?€?t any details about how the crowd-sourcing studies were done. One of two sentences would be helpful (was it with Mechanical Turk or something else?)

The main result of the paper is that 86% accuracy of the technique. (Presented just before Table 5 ?€? by the way, Table 5 is not useful and should be dropped.) This does sound ?€?very promising?€?. However, it seems a bit odd that the inter-rater reliability was modest (as reported right after Table 5). 

The question ?€?Will the knowledgebase be complete if it includes strong witnesses only??€? seems odd in comparison to the traditional view of libraries as holding ""all the existing documents that it should hold"". To a first approximation, I don't see why that is hard. It?€?s not at all clear to me that a collection should ?€?avoid redundancy?€? and the citation [41] doesn't seem very relevant to what question of what a library is. Rather, the authors seem to slip over into the question what makes a good knowledge base.

Section 5 ?€? After all your consideration of the nuances of the research, I don?€?t understand why triples should be the ultimate knowledge representation. They seem impoverished. Much better would  be some sort of rich semantics and direct representation. 

Minor Points:

In general, the writing is good but given the complexity, I think it would be helpful to impose a stronger top-down structure. Just adding spaces between paragraphs or indenting the paragraphs would make the conceptual structure easier to parse.

Keywords should be alphabetized.

In Section 3, the subsection headers are confusing an inconsistent: Defs 2&3 may be meaningful to you but won?€?t be for most readers. Why is Def 5 in parentheses? In addition, capitalization of the subsection headers is inconsistent.

In Section 4, it?€?s confusing to say ?€?in the following section?€? after presenting the RQs. I think you mean in the remainder of this section. Also, in the ?€?incomparable relations?€? section, it?€?s confusing to have a ?€?summary?€? halfway through.

There are few terms which I couldn?€?t follow at all. At the end of the first paragraph of the Introduction, what is ?€?facettation?€?? Also, in Research Question #1 (Section 6) what is ?€?incomparable?€?? Maybe it means a ?€?distinctive dimension?€? but I?€?m not too sure.

Given the current numbering, ?€?Algorithm 1?€? should be identified as Table 1. 

There are also some rather casual asides which are jarring in contrast to the rest of the paper. Is it really appropriate to say that argumentation mining ?€?is a current trend in the natural language community?€? (top of Section 2)? That point would be better made with a few citations. Similarly, in Section 3 we are told that Skyline queries ?€?has sparked a lot of interest for years?€?. And, just following that ?€?the field of economy?€? should be ?€?economics?€?. Then in Section 4 under ?€? Document collection?€? we are told that a technique is ?€?clever?€?. And, in that same sentence, it should be ?€?MESH?€? rather than ?€?Mesh?€?.

Throughout the document, there are many small word-choice decisions that could be could be improved. I'd suggest getting one or two friendly, native speakers of English to wordsmith the paper.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) After all the machinations of this research, I would like to see a very simple description of what was really found here. For instance, were there other (maybe simpler) correlates of ?€?strong witness?€?? What was the length of those papers compared to others? Were the authors of those papers particularly well cited in their other work? What about the quality of the journals in which those papers were published?

I gave the authors the benefit of the doubt about this because the techniques they used are plausible (though very complex). But, I also won't object to rejecting the paper because their case wasn't as tight as it should have been.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/24/2018,4:22,no
378,102,170,Braxxxx Hemxxxxx,2,"(OVERALL EVALUATION) The authors study scholarly claims in the  context of a digital library.  They use the idea of finding ?€?strong witnesses?€? of scientific claims to build high quality knowledge bases (the semantic data of claims associated with full articles). They suggest that a prior step of finding highly plausible and low bias claims is helpful (i.e. prior to standard extraction of triples (entity, relation, entity)).  They use the skyline method derived from Pareto dominance theory for this extraction.  They test it using a pubmed corpus and crowdsourced truth evaluations of the claims.  They find the results promising, in the strong witness claims being more convincing and covering the full set of claims.

The paper spends a large amount of time going into details about the Pareto semantics and skyline method, and explaining in detail how they applied their methodology.  This is helpful to readers wishing to understand the methodology, application and implementation.  It may be more than the general JCDL audience wishes to hear in the presentation. 

Other important considerations for discussion and coverage in the presentation:
Why is it important to have the most convincing claims identified?  In our research we find this very context dependent.  I.e. best claims depends on user and task.  So it can be difficult to identify a common set of ?€?best claims?€?
Similarly, the statement about ?€?secondly, the collection should avoid redundancy.?€?  Why is this important.  Why should not all claims be captured and recorded?   All that should be added is recognition (and ability to filter) by stronger/better etc claims.   When humans process information maybe it is helpful to just see highlights, but for machine processing why not just ?€?grade?€? claims while keeping all them.  There is also the temporal aspect (mentioned in limitations), in that grading may change based on new literature and new claims.
Maybe address better the issue of using crowdsourced participants for truth.   Who were the participants?  Evaluation of scientific evidence is something that takes experience and training to become accomplished at, and is often domain specific.   How do we know the crowdsourced subjects had this?   And also the low Kappa agreement (0.44). 

Generally a good paper, and interesting results.   There are limitations but most of these are covered in the Conclusions and Future Work section.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/19/2018,19:53,no
379,102,419,Masxxxxxx Yosxxxxx,3,"(OVERALL EVALUATION) The goal of the paper to find the skyline set is interesting. The paper employes many techniques in the literature as well as crowdsourcing.
The reviewer's main concern is that core technical contribution of the paper is unclear.
The authors model claims as a triple, namely a relation between two entities. However, this model is too naive to deal with actual scientific claims which are, in many cases, conditional ones. Examples of such conditional claims are ""X is effective for cancer Y for elder female patients"" and ""X is effective for Y if the laboratory test value of Z is under V."" In addition, relations are not simply binary but $n$-array in general. A discussion is required how the authors can extend their idea to such more general cases.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/20/2018,11:49,no
380,102,161,Kisxxxxx Haxxx,4,"(OVERALL EVALUATION) The paper presents an approach for finding strong ""witnesses"" or documents supporting a scientific claim from a collection of scientific papers in order to construct high-quality knowledge bases. The idea proposed is interesting, practical and relevant to JCDL. The literature review places the current work nicely compared to existing research on multiple domains such as credibility analysis and argumentation mining. The problem is clearly motivated in the introduction section.

However, I have a number of concerns regarding the current version of the manuscript.

Writing: The writing can be improved. Apart from many grammatical errors, the organization of the paper is also quite confusing. 

1) There are many terms introduced in the beginning of the paper, that are neither defined nor used again after introduction(e.g. consensual support, assertiveness of author).

2) Figure 1 illustrates the overall workflow and is helpful in understanding. However, section 3 which describes the approach doesn't describe their system in accordance with the figure, which makes it difficult to relate the components. Using a running example to demonstrate what each of the components is doing would have helped get the idea across better.

3) In section 3, many definitions and notations are introduced, but some of them are never used again. Some examples accompanying the definitions would have helped in understanding them. The notation table (Table 1) is of little help.

I am not sure what is meant by, ""Claim_g:  Dataset representation of \sum{Claims}"" in Table 1. Later in the definition 4, it is mentioned that ""Let Claim_g be an n-dimensional dataset that represents each g in groupDom( ??? ), where greater values are preferred"". These two are not same.

4) The concept of ""Pareto semantics"" is over-explained in the beginning of Section 3. Space could have been better used to demonstrate other important aspects of the work.


Methodology:

1) What is the role of relation_type in the method? In Figure 1, it is mentioned that there are three signals for claim characterization, but in Section 3 after Def. 5, the authors ""propose to model two core content-based properties"" and relation_type is not one of them.

2) In Definition 2, the relation set is defined as,

Let R_dom = {r_1, ?€? , r_n} be the set of relations that are incomparable. In other words, no relation in R_dom dominates any other in the set.

However, ""incomparable"" and ""non-dominating"" are not the same. Some relations can be mutually exclusive, for e.g., in the first experiment, four types of relation are used: beneficial, non-beneficial, no-effect and unknown. Here all the relations are mutually exclusive, i.e. a claim cannot be similar to the ""plausible claim"" if they have different relation types. Therefore, is it used as a filtering criterion before the similarity matching?

3) The use of cosine similarity between the topic distributions of two abstracts for computing agreement/disagreement between two claims is surprising. 

a) LDA is a generative model that is used to discover ""topics"" in a document. However, it can't give much sense of directionality or sentiment and is not directly employable for argumentation mining. For e.g., two abstracts mentioning ""Caffeine causes cancer"" and ""Caffeine doesn't cause cancer"" might have very similar topic distributions but are totally contradicting. 

b) Since the abstracts consist of many sentences with multiple claims, is it reasonable to assume that the topic similarity between them will correlate with that of the intended claims?


Evaluation:

1) What is the significance of the first experiment? Is the classifier used in the workflow for assessing relation type?

2) The paper aims to build a high-quality knowledge base, therefore, the evaluation of assessing witness quality using non-expert workers is inadequate. It should be validated by some people at least having some domain knowledge.

3) Additionally, the workers were not provided with additional resources while annotating the more convincing claim in a claim pair. This means that they just had to rely on the confidence of the author which may not always be indicative of the correctness of a claim [1].

[1] When confidence and competence collide: Effects on online decision-making; Liye Fu, Lillian Lee, Cristian Danescu-Niculescu-Mizil; Proceedings of WWW, 2017","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,Lahari,Poddar,writetolahari@gmail.com,375,2/22/2018,4:19,no
381,103,250,Byxxx Marxxxx,1,"(OVERALL EVALUATION) This paper may be useful in developing a new and useful technique with important medical implications. However, It does not represent a relevant contribution for this venue. I see no Digital Library content in it.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Just using a data set that is stored somewhere is not enough to make it appropriate for JCDL.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2/12/2018,19:58,no
382,103,71,Sauxxxx Chaxxxxxxx,2,"(OVERALL EVALUATION) This paper investigates the efficacy of Deep Learning based RNN classifiers to predict seizure in epileptic patients. It investigated the RNN and bi-directional RNN models, along with LSTM and GRU variants of the RNN. It claims to have the best results compared to the other methods. This was achieved using GRU.

The dataset used in the study was of a small scale, but its understandable that getting a large dataset for such studies is not always possible. 

Some recommendations : I would want the authors to describe their network (or encoder-decoder) architecture in a picture and add it to the paper. They have described it in words, but it would be more accessible to the reader if a picture is presented for the same. Also, I would want the authors to elaborate a bit on section 4.1, on the exploitation of the correlations in data. It would be useful to the readers to get an insight on the analysis performed.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I had come across a paper (https://arxiv.org/pdf/1706.03283.pdf) on the same topic which used an RNN and had the same accuracy number. I don't know what to make of it, but I just wanted to mention it.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/14/2018,4:01,no
384,103,308,Anixxxx Prxxx,3,"(OVERALL EVALUATION) Summary of the review: This paper proposes use of RNN (LSTM/GRU) architecture for seizure prediction in epileptic patients. The work explores an existing problem and dataset with an existing machine learning model and shows it to beat a prior baseline. The paper can be best described as a task paper on specific kind of signal processing showing RNN architecture to beat CNN architecture.  


Relevance to JCDL: Though the CPF of JCDL calls for contributions from health/medicine community, in my opinion, the contributions are sought in the intersection area of digital libraries and health. Such work may include architectures, applications, and deployments for digitization and management of medical records, scientific medical data management, medical communities and individual behavior modeling et al. 

At best the work under consideration can be termed as an automatic classification of medical data which is more of a pattern recognition/ medical application work. A social/user/document/information management angle is needed to make it more appropriate for JCDL.  


Novelty/Originality: Since [1] as cited by authors use the same problem and the dataset with a CNN model, the novelty of the work is using an RNN model instead. 

In a broad context, it can be seen as a natural signal classification (much like speech) using a sequential model (RNN vs CNN) which is a well-explored machine learning problem. [2][3]

In a focused context, it an experimental comparison of well-known architectures on a specific dataset[1]. However, more rigor is required in experimental setup to sustain the claims. (Please see next section of review for details)   


Assessment/Evaluation/Comparison: The whole methodology and result section miss important details to confirm the findings and the claims. For example ?€?GRU model outperforms all other models?€? Since the dataset is small and the number of parameters is not controlled out of LSTM, GRU, BLSTM, and BGRU, it might be an artifact that GRU does well. In other words, GRU performance might just be the result of fewer data meets fewer parameters. 

As compared to [1] the CNN results are much weaker, the author doesn?€?t explain this. Is this replicated CNN or authors pick it up from [1] directly (which seems less probable as [1] has a different setup)? Further [1] perform 10 fold validation, which is a better strategy in case of small dates as in the given scenario. [1] also reports Sensitivity, Specificity, and AUC which is de facto included in medical tasks results and/or dataset with high skewness. 
 
About Section Methodology:
preprocessing: Is the preprocessing applied to both the RNN and the CNN architectures? Is this preprocessing same as [1]? If No, How does this affect the baseline CNN performance as compared to the [1]? 

architecture: The implementation framework is not mentioned. The number of units in BSLTM is not mentioned.  Assuming it?€?s Keras-like wrapper library, one can extrapolate the hidden size of BLSTM is equal to twice of the hidden size of LSTM. This may affect the output and hence experiments with equal parameters or best parameter configuration for each model need to be done.  Claims without knowing the parameter and experimental setup cannot be validated.

Author mention using linear activation function for the FC layer. Do you mean rectilinear activation function? A linear activation function (y=alpha*x) is not a non-linearity and hence does not count as a legitimate ?€?layer?€?. 

model training: Missing details include the number of epoch/ early stopping details, details of validation split (note: test split is mentioned in pre-processing but no details of validation split are given) etc. Is the test and validation split same? If Yes, then the results will not be accurate.  

About Section Results: 
As mentioned in the indicative example of GRU vs BGRU, LSTM, BLSTM, the performance lack enough details to confirm the claims. Figure 4 can be removed (since the RNN model?€?s training behavior is well captured and is not the focus of the work) and replaced with a more granular experimental result and discussions. 


Style/quality of writing: The writing is good and overall the work is a pleasure to read. Different portions of the paper like motivation, background, technique etc are well weighted. As mentioned earlier as well imaged take up too much of the allocated space and can be safely removed (Figure 1, 2 and 4) or decreased in size (Figure 3, 4) to free up more space to include more results.  


Replicability: The experiments are generally replicable. The dataset is public and the experimental details are adequate to replicate the setup. However, more details might be needed to reproduce the results to the decimal places. Such details include framework, system seeds, no of epochs, initialization distribution of the parameters, etc. 
Other issues (as discussed in details in the above section) about the replicability against the baseline (weakness of baseline as compared to [1]) needs to be addressed.  


Adequacy of references: The references are adequate from a focused task paper point of view.


References:

[1] Acharya, U. R., Oh, S. L., Hagiwara, Y., Tan, J. H., and Adeli, H., ""Deep convolutional neural network for the automated detection and diagnosis of seizure using EEG signals,"" Comput Biol Med, Sep 27 2017.

[2] Abdel-Hamid, O., Mohamed, A. R., Jiang, H., & Penn, G. (2012, March). Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on (pp. 4277-4280). IEEE.

[3] Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. ""Speech recognition with deep recurrent neural networks."" Acoustics, speech and signal processing (icassp), 2013 ieee international conference on. IEEE, 2013.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2/15/2018,19:23,no
385,103,21,Suxxx Alxxx,4,"(OVERALL EVALUATION) This paper does an excellent job at defining the research problem, laying out the research design, reporting the results and interpreting those results for the reader.  However, the paper does not clearly identify the significance of the novelty of the approach and would benefit with noting why these results, which do not appear to be substantially better than the 96% accuracy achieved by [3], are important.  

With the understanding that there are substantial space limits in a short paper, it would still be beneficial to add a short paragraph that relates back to the problem outlined in the introduction by talking about the potential of the improved predictions for patients. It is important when our field is engaging in translational research that the outcome should be discussed in terms of how this will have practical applications and not just concentrate on summarizing the scientific/engineering accomplishment.  What does this level of accuracy improvement mean when applied to the problem you outline in the introduction? How can this be translated into useful technology? You mention it briefly when you note that it is faster to train and good for small datasets -- but what does that mean to the medical profession or for medical devices?","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/15/2018,20:26,no
386,104,342,Giaxxxxxx Silxxxx,1,"(OVERALL EVALUATION) The paper does not respect the layout rules and the suggested length. Some references are missing, e.g. in the conclusions, the authors write: ""by previous research for RDA-based bibliographic metadata^17"" what's 17? 

The conducted study is rather speculative, it is not clear how the dataset has been chosen, how the time interval has been selected, how the evaluation of changes has been conducted. Moreover, the discussion is rather syntactic and the take-home messages are not well stated.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/7/2018,10:17,no
387,104,319,Thxxxx Rixx,2,"(OVERALL EVALUATION) The authors of the paper present an analysis of the evolution of metadata change in authority metadata over time. To get a better understanding on how metadata is evolving in library catalogs is an interesting and useful question e.g. for data integration. However, the introduction would benefit about some ideas on how the results of the study can be used. The analysis itself is based on just two extracts from the OCLS database within a 22-month timeframe. For this timeframe, only observations can be conducted, but these are too few snapshots to draw conclusions. However, observations are interesting even though sometimes hard to read as often only MARC field numbers are given, which only understandable for insiders.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/7/2018,15:43,no
388,104,373,Alxx Thxxxx,3,"(OVERALL EVALUATION) This short paper's quantitative evaluation of edits to a large sample of MARC authority records yields some interesting findings about which new RDA-based MARC authority fields have been adopted most readily, but the paper's analysis of the possible contributing factors to these results is thin, and the paper feels especially short (are margins in the short paper template supposed to be so wide?). 

For example, the analysis does not mention relevant aspects such as the fact that Library of Congress staff implemented some large-scale bulk edits to authority records (which may or may not have affected their sample set). Nor do the authors consider the fact that not all approved MARC fields are expected or desired to be used uniformly--many fields are optional (left to cataloger judgement) or are for edge cases that may have been created in response to a handful of actual cataloged resources but are not widely used/needed.  

Copyediting note:
There are typos in the given URL to the MARC Edit tool","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/12/2018,22:26,no
389,105,109,Nixxxx Fexx,1,"(OVERALL EVALUATION) The paper conducts a study on exploratory search in DL by providing two contextual algorithms - content-based and session-based) for re-ranking documents in a personalized way for the user.

The paper carries out evaluation in a living labs settings relying on a real DL in the social sciences area. The adoption of a living labs setting is especially interesting in the field of DL, since it is more commonly used in IR and RecSys.

The evaluation methodology is sound and the results are interesting.

It would be nice to have access to the dataset both for reproducibility purposes and for further analyses.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,1/25/2018,17:27,no
390,105,263,Daxx Mcxx,4,"(OVERALL EVALUATION) This paper presents a study of the effect of contextualising search results in two different ways for exploratory search tasks on the rank of clicked suggested documents.

Despite, I suspect, being based on some good work, this paper has a number of significant problems:

The first problem is a definitional one: the authors speak about 'exploratory search on the level of browsing'; neither browsing nor exploratory search are defined clearly. Exploratory search is loosely defined as being more about learning than known item search, but this is a partial definition at best. Browsing is not defined at all, and in fact browsing and exploratory search are two different information activities (they may overlap, but browsing is an activity in its own right, not one contained within exploratory search or any other kind of search). It is therefore entirely unclear what the authors mean by 'exploratory search on the level of browsing'. Really, if the authors wish to discuss exploratory search they need to cite Marchionini's 2006 CACM paper that expressed and introduced the idea. Further the authors discuss the use of a living lab without ever defining it: this again has more than one definition; in HCI it is a laboratory as much like a home or office space as possible, in other fields it is a working piece of technology also used for research. While it is clear the authors are using the latter definition, they need to make this explicit.

The second major issue with this paper is its structure. The authors introduce a number of concepts, but fail to tie many of them together. It is clear that the study takes place in a living lab, but what that means for the results is never discussed. Equally, it is clear that some kind of user test took place, but what users actually did/how it was measured is not discussed. It is further clear that for some pat of this study, transaction log analysis was used, but not how the logs were gathered or analysed.

The statistical tests used in this paper are dubiously correct, particularly given that for some document sets the correct assignment of MFR would be infinity--users did not find a relevant document. Instead of marking MFR as infinity, these instances are discarded from the data before testing. Further, p should never be reported as 0; it is simply never the case that there is no probability that results arose from a random sample.

Finally, on an ethical note, the authors claim that one of the major critiques of their study should be that user relevance can not be compared with canonical relevance; this completely denies the user experience of their system as having value. If we do not believe user assessments of relevance, perhaps we should not conduct user testing at all--to call user judgement into question in this way is reprehensible. A much better critique would be that from log analysis it is still unclear how satisfied users were.

My review of this paper may read like I think it is irredeemable: this is not true. I believe this paper contains an interesting study that--analysed and written up properly--is likely to show the value of contextualisation at the document level for exploratory search. I strongly encourage the authors to address my concerns and submit their paper to JCDL next year.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2/17/2018,7:31,no
391,105,279,Micxxxx Nexxx,5,"(OVERALL EVALUATION) The authors describe three different ranking algorithms on a
production DL, Sowiport (http://sowiport.gesis.org/).  The ranking
algorithms can best be described as ""facets + implicit relevance
feedback"".  The baseline ranking algorithm just uses features of
the documents itself; that is your view and my view are the same
for the same query.  The second ranking algorithm ranks the documents
with feedback from a ""seed document"", which is based on an earlier
query by the user.  The third ranking algorithm takes more user
context into account: previous documents the user has encountered
as well as their previous queries.  If I'm reading the results correctly,
the second ranking algorithm (based only on the seed document) 
performs the best of the three algorithms.  

My observations about the paper:

1) The paper continually references a ""living lab"", which seems to
be just a ""production DL"".  On the one hand this is admirable because
the results reflect actual usage, but on the other hand we don't
have a lot of insight as to the kinds of users or tasks that are
happening in the DL.  Are these scholars searching in the DL for
their own research?  Or are these students who have to use Sowiport
for assignments?  This could influence the results.

2) Related to the above, they perform A/B/C testing with the three
ranking algorithms on the live DL and glean what they can from the
logs.  This has implications for some of the signals they list in
table 1: if Sowiport has full-text, then the user transferring to
Google Scholar or Google Books seems like a negative signal and not
a positive one (whereas ""bookmarking"" and ""exporting"" are more
clearly positive signals).

3) The production DL of Sowiport does limit the kinds of baselines
they can employ.  So while it is great to have ""real"" data, no other
techniques from the related work are employed; I appreciate that
adding 10+ additional ranking mechanisms would add more complexity.
Although, parts of the evaluation could be synthesized: additional
rankings could be partially evaluated to see if documents otherwise
skipped in the log data are moved up or down (cf. ""Skip Above and
Skip Next"").  Essentially the authors have shown ""personalization""
/ ""contextualization"" is better than not.

4) But is it clearly better?  Aside from not all the signals being
clearly positive (#2 above), the authors limit their results to
their own metric, ""mean first relevant"" (MFR) instead of conventional
metrics like MRR or NCDG.  The issue is MFR is defined by one of
the authors of this paper, in a 2017 tech report, which means it
has not been reviewed or accepted by the rest of the community.
MFR might turn out to be a great metric, but there is no reason to
not report additional metrics at the same time.  This paper has
three ranking algorithms, all of which they've defined (i.e., no
baselines from related work), and the main metric they use to
evaluate their work is of their own design as well.  That makes me
slightly uncomfortable.  And not that adding MRR to tables 3 & 4
would take that much space, but the authors do have approximately
1/4 of a page left blank.

5. Early on in the paper I was left confused between ""searching"",
""exploratory searching"", and ""browsing"".  I thought this was going 
to be about browsing, but when the layers were peeled back it 
was still largely about searching.  By the time I got to page 4, 
I realized browsing wasn't really the point of the paper and once
I let that go I was able to proceed without difficulty.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) review done a the request of Ed Fox. 

I could go 0 or 1 on this paper.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,3/1/2018,18:51,no
392,106,337,Axx Shxx,1,"(OVERALL EVALUATION) The study provides a novel simulated user evaluation approach to digital libraries, focusing on digital libraries as context.

The literature review provides a focused and good coverage of the different approaches to and criteria for user studies in the context of IT systems and digital libraries. In particular, the categorization of models into descriptive and conceptual, predictive and explanatory and mathematical is useful and well presented. A key issue with the literature review lies in the fact that there is not sufficient coverage of empirical user centred studies of digital libraries. This could have provided the authors with a nuanced understanding of the nature of digital libraries and their sophisticated architectures and users. 

The library-related tasks types listed seem to be very limited, particularly considering the 'looking for a book' category as other. This is a very limited set of task categories. 
The discussion and literature review about search tasks should have been part of the related work. this makes the paper read as fragmented. 

The user search behaviour categories listed in section 4.3.3 is very limited and may not necessarily be searching behaviours within digital libraries. In particular, given the complex nature of current digital libraries with multiple collections, search and browsing functionalities and visual user interfaces, these four categories seem very reductive. I am not convinced that this is a model that would work for users of digital libraries of different types. 


The software architecture section is very limited and does not provide a detailed account of each component. Given that the software aims to depict the simulation process, the technical information about mining user content, user behaviour, predictor and various data mining techniques should have clearer and more detailed description. 

The Future Work section does not need to be long and too descriptive. It feels like the authors ran short of content for the last part of the paper. 


There are some typos in the paper: (page 5) The terms 'proposed' and 'quantitative' in the Data Analysis section","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,1/30/2018,16:04,no
393,106,225,Clauxxxxxxx Kxx,2,"(OVERALL EVALUATION) The authors argue to use simulated users for digital library evaluation based on 
extraced user behavior given real logs. 


Basic Reporting
The paper is well written and easy to read. The state of the art is well written.
The describe framework seems to look at the right dimensions.

Experimental Design
The conducted user study, to identify library related tasks, search tasks, to measure level of expertise and to see behaviour based on logs was done with 40 participants. The outcome is
used to create the DL search model and a user model. 

Validity of Findings
The state of the art and the framework and the proposed model seem to be very
good, and I expected, that within this full paper, at least one simulated DL evaluation 
would have been described. But it was not, so the whole paper is a first idea to
do research in that direction. 
The idea of using simulated users on ""some"" DL tasks to be evaluated is good to lower
cost and to foster research and comparison. Maybe even a evaluation testbed can be created.

But the paper does not go beyond the idea.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I would propose to have this paper either as short paper or poster. As stated above, 
I find the idea for certain evaluation types worth looking at and could be discussed at a poster. Bates would argue against it.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2/16/2018,8:24,no
394,106,196,Samxxxx Jayxxxxxx,3,"(OVERALL EVALUATION) This paper presents a methodology for producing simulated users that can be implemented and used for evaluation of information retrieval systems.  The study formulates on a ground-truth data collection study from real users with genuine information needs conducting their searches. Authors present a data analytics framework to identify most influential information search behaviors and propose a searcher model and software architecture for future work including evaluation and validation framework. 

This is a very interesting work in the area of user modeling and interactive information retrieval. The presented work is innovative, and it is very good idea to exploit the user oriented approaches towards building user-centric use profiling paradigm to identify user interactions.  From this point of view the work can make a good contribution to JCDL community. 

My only concern is how the data analysis can justify the searcher model proposed in [49]. It is not well clear how the lessons learned in the study applies towards the simulated user searcher model. The user model U:[Q,E,B,C] represents only a generalized user profile but how each component reacts to user interactions in an interactive retrieval process is not explained in the current study. 

It is not clear from the Figure 2, what it means by the user class? Does this corresponds to certain user behavior or the search task or some other ad-hoc profiling technique? 

Minor formatting issues,
It would be great if authors can improve the readability of the Figure 1 and 2, may be zoom it and spread across the columns?","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/17/2018,2:03,no
396,107,360,Kazxxxxx Sugxxxx,1,"(OVERALL EVALUATION) The authors propose positive-unlabeled learning (PU-Learning)-based recommedation 
which improves existing supervised leaning approach to provide much more relevant 
scholarly paper recommendation. It is important to develop better recommendation system 
as the number of published papers are rapidly growing. So the topic is relevant to JCDL.   

The idea of employing PU-Learning with embedding vector in recommendation is interesting. 
But this paper has a lot of problems. The details are as follows:   

(1) Novelty/Originality
- According to Sec 2.1, the authors describe that 
""To tackle this problem, a Positive-Unlabeled Learning (PU-Learning) model is employed in this paper. 
Our model is derived from an A-EM (Augmented Expected Maximization) model [4], ...""

It seems that the authors just employ one of PU-Learning approaches, ""A-EM"" as it is. 
If not, they need to clearly describe the difference between their approach and original ""A-EM"". 


(2) Methodology
- The authors need to clearly define parameters so that they do not confuse readers.  
At ""(2) Random-walk Probability"" in Sec 2.2, the authors use capital letter ""P"" to denote each paper 
(P^{a}_{1}, ..., P^{a}_{n}). But, in Sec 2.1, the authors have already used ""P"" for ""positive instance set"". 

Similarly, in Sec 3 (just before Sec 4), ""A"" is used to denote ""an author"". But ""A"" has already been used 
as ""a set of the scholars (authors) in Sec 2.1 (right column in page 2). This should be also clearly distinguished. 

- It would be interesting to compare the authors' random walk probability with PageRank-like score. 

- According to Sec 2.1, the authors address binary classification task. So the authors should also try SVM as it is often reported that SVM gives better results in binary classification. 
 
Recently, random forest, which is an enhanced version of decision tree, is also a promising classifier.  
The authors also try random forest to enrich their experimental results. 
 

(3) Assessment/Evaluation/Comparison
- In Sec 4, it's not clear how gold-standard data is defined for each author while they describe their dataset. Does it depends on reliability score? 

- In Sec 4, the authors need more discussion on why proposed approach gives better results compared with 
other classifiers based on characteristics of them not just describing ""improvement"". 

- The authors claim statistical significance of PU-larning model. So in Table 2, it is better to clearly show it by marking with ""*"", and so on. 


(4) Style/Quality of Writing
The authors need to write the paper more carefully. 

- From structure point of view, in Sec 2.1, the authors need to define P, U, and RN first, 
and then show Algorithm 1 for better readbility. In addition, while Figure 1 is shown at 
the beginning of the paper, but it is first referred to in Sec 3. The authors should show 
it close to the reference, and then start to explain by using the paragraph 
""Consider a conference c ..."" 
described in ""(2) Random Walk Probability"". 

- The authors need to improve expressions a lot. The reviewer shows important ones only. 
Please double-check the whole paper. 

[Sec 2.1]
- 'positive' papers have direct ... => '*P*ositive' papers have direct ...

- Consider set of the scholars ... => Consider set of the scholars (authors) ... 
(This part should be written in the same way as ""scholars (authors)"" which first appear at ""Graphical Features"" in Sec 2.2.)

[""Textual"" at Sec 2.2]
- The value of each dimension in vector w_{i} ...
(If ""w_{i}"" is a vector,  it is better to use bold font. In this paragraph, it is difficult to distinguish vector and scalar.)

- word vector w_{a_{j}} for ... => *W*ord vector w_{a_{j}} for ...

[Just before ""(3) Strength of the connectivity"" at Sec 2.2]
- Higher the score more relevant the paper.  => *The* higher the score, *the* more relevant the paper. 

[""(3) Strength of the connectivity"" at Sec 2.2]
This generate four features ... => This generate*s* four features ...

[""(1) Doc2Vec"" at Sec 2.2]
- based on it?€?s text context [6]. => based on *its* text context [6].

- Similar to method explained in 2.2, ... => Similar to method explained in *Section 2.2*, ... 


(5) Replicability
- If the authors much more clearly describe their approach, researchers interested in this work 
would reproduce the authors' approach. 


(6) References
- The authors should cite the original meta-path paper: 
Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. 
""PathSim: Meta-Path-Based Top-K Similarity Search in Heterogeneous Information Networks""  
Proc. of the VLDB Endowment, 4(11):992?€?1003, 2011.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,3/5/2018,18:55,no
397,107,175,Draxxxxxx Herxxxxxxx,2,"(OVERALL EVALUATION) This paper looks at the task of recommending relevant publications to authors and proposes a new learning approach called positive-unlabeled learning (PU-Learning). The motivation for the approach is to be able to leverage datasets with a small number of positive examples (such as relevant papers), a large number of unlabeled examples, and no negative examples (e.g. papers which are known to not be relevant). 

The authors propose an iterative training method in which in each step the examples with the lowest probability of being relevant are added to the training set as ?€?Reliable Negative Instances?€?. As such the method resembles co-training with a single view of the data. The idea seems novel and can potentially provide useful in the domain of scholarly publication recommendation.

My main concern is regarding the evaluation. Firstly, paper recommendation is a fairly well studied problem, however, the authors did not reference any previous literature or compare their results to any state-of-the-art methods. This makes the evaluation less convincing. 

The experimental setup is based on existing citations to papers (i.e. if an author cited a publication, that publication is considered relevant) and the evaluation is performed by removing citations from the set, however, this has some limitations. One limitation is that the method may recommend publications which are more relevant than the publications removed from the set and which were not cited for example due to lack of space. 

It would also be useful if the authors could provide a more detailed analysis of the results to show whether their method identifies easy-to-recommend cases or whether it is able to provide meaningful recommendations for not-so-obvious links.

While the results in Table 2 show an improvement when using the proposed method, it would be worth sharing more detailed results such as precision and recall values which I think are relevant for this task.

It would also be useful to share some details about the setup used for the machine learning models.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/17/2018,8:22,no
399,107,230,Pexx Knxx,3,"(OVERALL EVALUATION) This paper proposes the use of an application of the Expectation Maximisation approach to the area of scholarly paper recommendations. More specifically, the paper deals with the problem of confidently identifying negative training examples in the set of unlabelled papers. The paper is clearly written. 

In terms of weaknesses. 
- I would argue that the paper is not fully aware of the state-of-the-art in academic recommender systems. Most of the successful academic recommender systems, especially those applied in production, do not use supervised machine learning approaches (as it is done in this paper), but largely benefit from algorithms such as collaborative filtering and approaches based on co-citation analysis. This is not necessarily a problem for the proposed method, as identifying negative examples is still important, but it should be reflected in the selection of evaluation algorithms presented in Table 2 (i.e. why all of the selected algorithms for evaluation are supervised when most of the systems currently in production are not?). 
Have a look at work, such as: 
a) J. D. West, I. Wesley-Smith and C. T. Bergstrom, ""A Recommendation System Based on Hierarchical Clustering of an Article-Level Citation Network,"" in IEEE Transactions on Big Data, vol. 2, no. 2, pp. 113-123, June 1 2016.
doi: 10.1109/TBDATA.2016.2541167
b) J. Beel, A. Aizawa, C. Breitinger and B. Gipp, ""Mr. DLib: Recommendations-as-a-Service (RaaS) for Academia,"" 2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL), Toronto, ON, 2017, pp. 1-2.
c) Hristakeva, Maya, et al. ""Building recommender systems for scholarly information."" Proceedings of the 1st Workshop on Scholarly Web Mining. ACM, 2017.
d) Knoth, P., Anastasiou, L., Charalampous, A., Cancellieri, M., Pearce, S., Pontika, N., & Bayer, V. (2017). Towards effective research recommender systems for repositories. arXiv preprint arXiv:1705.00578.

- ACM is not the world's largest open dataset of CS papers as claimed in Section 3. Please look at others, such as the datasets provided by CiteSeerX, Semantic Scholar and CORE. 
- This is the most important comment. The evaluation should focus on testing how well does the approach help in identifying Reliable Negative Instances. While this is the main/key contribution of the paper, evaluation of this aspect is missing. 
- It has been demonstrated that the results of off-line evaluations in (academic) recommender systems do not well correlate with online evaluations. Therefore, it is important that the reported recommendation methods are also tested in an online environment for CTR and related measures.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2/19/2018,13:33,no
401,108,23,Roxxxx Alxx,1,"(OVERALL EVALUATION) This paper explores linking names extracted from English legal documents dealing with the Irish Rebellion of 1641 to Wikipedia articles.  One major technical part of the (Section 3) deals with the corpus.  The second section major part uses the GERBIL annotation framework to compare the performance of several disambiguation modules.  

Overall, the performance is modest.  One partition of results focuses on ?€?Emerging Entities?€? (EE).  These are entities which appear in the text but not in the knowledgebase (Wikipedia).  When they are tagged as coming from Wikipedia, they are false positives.

Some of the disambiguation modules were unable to complete on this corpus and were dropped.  As shown in Table 1, credible scores are reported only for AGDISTIS.  But, further analysis shows that much of that result seems to be due to its rejecting most comparisons and that makes it do well on the EE cases but is not actually useful.

In short, there is no magic bullet.  This is, simply, a very difficult task.  I think the authors might want to consider how to build tools that are robust across different context.  Indeed,  future work might examine using a wider range of knowledge sources (e.g., historical texts) and more detailed, rich semantic, modeling of the historical context.  

Minor Points
What is the distinction between digital libraries and digital humanities?  Digital Libraries are mentioned among the Keywords but not again.  On the other hand, Digital Humanities is mentioned frequently in the article but not in the Keywords.    Also, Entity Linking is not needed in the Keywords since it is in the title.

I would prefer not to have abbreviations defined in the abstract.  The abstract is often used separately from the main text.

(Section 1) The use of the term ?€?generous?€? for a digital library interface doesn?€?t seem very useful.  I think this audience understands the relevance of the entity extraction task as a foundation for many different applications.

(Section 2.1)  Entity Linking is not primarily a Computer Science task, it?€?s more a task for computational linguistics.

(Section 4) For this audience, the description of the evaluation metrics could be greatly shortened.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) (The previous review I submitted for #108 was for a different paper, please delete that one)

I'd be more inclined to drop my score than to raise it.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/11/2018,5:48,no
402,108,286,Daxxx Nixxxx,2,"(OVERALL EVALUATION) This paper reports on using multiple systems for linking entities in historic texts.

The paper is well written and presented; and generally a pleasant read.

The end of Section 2 could benefit a brief summary of the main points from the related work. Possibly a little clarification of the features of ?€?REDEN?€? would help here as well.

Section 4: there seem to be human annotators and machine annotators, and in some text the reader has to do some extra work to be clear about which is being referred to. I suggest passing through all occurences to ensure there is clarity about these references.

5: define BAT framework

Understanding of the numerical results might be enhanced with some specific examples of both correct and incorrect annotations. Given the discussion in Section 3.1 of the difficulties in notability, time, language change etc. examples that illustrated these from the results would be a good way of connecting the discursive and numerical portions of the paper. In particular it seems as if the ability to provide explicit temporal parameters to a system should be useful in these tasks. Presumably none of these systems had that ability: is this one of the ?€?desirable properties of annotation systems for cultural heritage collections?€?? Likewise could a geographical ?€?probability field?€? be provided to more strongly weight terms from Ireland rather than from elsewhere? 

Overall this is well-written paper on a relevant topic that provides useful results for the digital heritage community.


Minor

3.1: ?€?incredible window?€?: maybe interesting, informative, fascinating ?€? but not incredible 

4: ?€?in order perform?€?

Numeric values in the results table should be vertically place-aligned.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/16/2018,3:28,no
403,108,318,Thxxxx Rixx,3,"(OVERALL EVALUATION) The authors of the paper investigate in their work the suitability of entity linking systems to early English documents. The introduction provides a quite generic motivation on why entity linking is important also for historic documents. However, a short summary about the challenges especially wrt. historic content in the context of modern knowledge bases would help the reader to shape the expectations. 

The background section gives an introduction into the topic of entity linking. It mentions the problem of emerging entities but more important for historic documents are evolving entities e.g. name changes of a city or person (e.g. the pope). This information is rarely well documented in knowledge bases or Wikipedia. Another challenge not mentioned is that knowledge bases are based on digital documentations (mostly Wikipedia) that has been created today. 

The corpus description presents a number challenges for the entity linking. The authors also clarify that they are only interested in the quality of entity linking and not in the extraction of entities from documents, which should be more emphasized in the introduction. The annotated corpus is a good contribution but it?€?s not clear if it will be made available for research. 

The experiment was performed just for the disambiguation but some EL systems need the context of the entity for the disambiguation and therefore parse the surrounding context. Therefore these systems run into the language challenge and perform worth as they would with modern English. 

The selection of annotators was based on the ability to fully execute the annotation task. No overview has been provided about capability, annotation approaches and used knowledge bases. Therefore it is hard to draw conclusions about the systems. 

Overall, the contribution of the paper is limited as mainly shows that today?€?s popular knowledge bases are not yet covering the historic entities in an appropriate way. This is not surprising and a basic assumption of many work dealing with historic content. Therefore conclusions of the quality approaches of the system cannot be drawn. Positive is the creation of the annotated corpus which will hopefully be available for research purposes.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/16/2018,14:35,no
405,109,408,Irxx Xx,1,"(OVERALL EVALUATION) This paper examines an extension of a model for information seeking in large-scale DLs, to demonstrate how contextual browsing supports serendipitous discovery. Through the use of Compage, a prototype for contextual browsing, the researchers assess three types of contextual browsing strategies via a simulation.

The research builds on the authors?€? prior model. The use of contextual browsing in understanding serendipity for large-scales DLs makes this a unique topic. While the idea of examining contextual browsing is very interesting and has great implications for the design and improvement of large-scale DLs, the hypotheses proposed in this study only tried to compare worksets produced by the strategies.  

The authors did not state or have clear research questions or research focus: whether to validate their model in relation to serendipity in context (test modes of contextual browsing or effect of serendipity or just compare the three strategies). If the authors attempted to compare the similarities of the strategies, their simulation design would be acceptable, but the results are predictable as they mentioned in the paper. The novelty of this paper is not high. 

If the authors tried to examine serendipity in context, then appropriate research questions and associated hypotheses need to be proposed. Contextual browsing is a very dynamic and complicated information seeking behavior.  Many factors including user (knowledge, motivation, interest, style, etc.) and system (organization of the information, content, layout, color, etc.) influence the way user browse information. User study is the best approach to illustrate the browsing approach and how serendipity occurs in the process. 

If the simulation study is conducted to investigate serendipity in context, then the design of the study needs to reflect the complexity of contextual browsing in reality. The researchers did not consider or describe the scenarios of contextual browsing for the three strategies that users normally go through. For example, for the active search, what is the information need? The authors only specified the data set. At the same time, the authors did not provide justification that DBPedia has the characteristics of a large-scale DL. Large-scale DLs consist of more diverse types of materials, metadata, and their associations. Finally, the simulation design needs to focus more on the browsing process than the relevant results generated by the strategies which is less useful to support serendipity in context. 

The current simulation study is more for the comparison of similarities, few information regarding elements of serendipity is identified. 

Markir et al suggested 7 recommendations for system design to support serendipity.  It would be helpful if the authors would focus on how to implement these recommendations and further test their usefulness and effectiveness. 

In addition, it seems that the authors did not use a standard way to propose hypotheses and report the statistical results.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) After reviewing other reviewers' comments, I made some changes for my comments. My main concern is that the authors did not state or have clear research questions or research focus. If the authors attempted to compare the similarities of the strategies, their simulation design would be acceptable, but the results are predictable as they mentioned in the paper. The novelty of this paper is not high. If the simulation study is conducted to investigate serendipity in context, then the design of the study needs to reflect the complexity of contextual browsing in reality. The current simulation study is more for the comparison of similarities, few information regarding elements of serendipity is identified.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/18/2018,19:45,no
407,109,182,Xixx H,2,"(OVERALL EVALUATION) This paper proposed a model of strategies for contextual browsing and realized some of the strategies in a software framework. The realization was empirically verified by a set of simulated experiments. The topic is highly relevant to JCDL. In fact, this paper is a follow up of previous JCDL papers contributed by the same research group. The proposed model is well grounded on influential theories of information seeking, with valuable extensions supported by sound explanations (Tables 1, 2 and Section 3).

The simulation experiments were also well designed, with two exemplar cases rooted in practical scenarios and with worksets of different sizes. The three contexts (Reset, Unprioritized, Prioritized) were representative and the statistical tests were carefully designed. The results is well explained with Figure 3 being a nice presentation of the simulation results. It would be more helpful if the results of statistical tests can be presented clearly.  

In general this paper presented a study with both theoretical and empirical roots. It contributes to the conceptualization of strategies of contextual browsing, proposes an implementation of these strategies based on Linked Data,and verifies both with a reasonably designed simulation experiment.

Here are some suggestions for further improvement of the paper: 

1. It could be helpful to further strengthen the connection of the simulation experiments back to the proposed model of contextual browsing strategies (Tables 1 and 2). They are connected, but it could further help the readers if the connections are explicitly stated. 

2. The columns in Table 1 were not explicitly introduced. They seemed to be different stages of contextual browsing, but a clear introduction of would be appreciated. 

3. At the end of Section 6, it would be helpful to provide a summary (in a table if possible) of the test results. As there were quite a number of combinations of contexts, worksets, replacement vs. narrowing, and tests, a summary would be helpful for the readers to follow.

4. A user study can be proposed for future work, to complement and strengthen the evaluation based on simulation.

5. There seems to be a incomplete sentence at end of section 5.1.2.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Thanks to other reviewers for their informative reviews! Upon reading them,I agree to reduce my score to 2. I agree that this paper did not do a very good job in connecting the theoretical part of modeling contextual browsing and the experiment part, but I think (or hope) this can be fixed by explicitly state that the simulation experiment aims to provide a proof of concept for several (not all) strategies in the proposed model. 

IMHO, simulation is an acceptable approach to verify proposed models/methods, when real-world empirical data are hard to obtain or not in the scale needed. Currently, linked data repositories have not accumulated a lot of user interactions, and thus I am okay with the strategy of simulation. Admittedly a user study should be conducted in the future to further evaluate the proposed model and software framework.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/18/2018,4:22,no
409,109,187,Antxxxx Isxx,3,"(OVERALL EVALUATION) Relevance to JCDL:
The paper is highly relevant to the the JCDL. Its general focus on navigation of large scale digital libraries will of course be of interest to the JCDL's audience; and the Compage framework in particular should excite interest for many.

Novelty/originality:
The paper is doing various things at once, not all of which harmonise entirely well with each other, so it is difficult to give a unified assessment. In addition, my confidence in several of these areas is low. In terms of the overall user framework adopted, the analysis of 'contextual browsing' is as far as I am aware novel, and potentially useful. The use of worksets as the basic unit of analysis is also interesting; if the concept itself is not new, its application is. On the other hand, the treatment of the RDF triple as a means of exploring connections felt a bit underdeveloped. The algorithm used for computing similarity in entity graphs is appropriate but basic and not novel (and above all not well explained, see later).

Methodology:
The methodology is somewhat confusing. Sections 1 through 3 concern serendipity, and how platforms might support it better; sections 4 through 6 concern workset similarity. At first glance, serendipity and similarity are distinct notions, or even in tension with each other, as the authors indeed seem to note at various points (for instance, with the occasional good hits the 'reset' strategy yields). The paper's claims that they are strongly related is unclear (for example ""We have described how contextual browsing requires a means of prioritisation to remain feasible"" in the conclusion). There is not much transition between section 3 and 4. Really, we seem to have two separate papers that have been zipped into one.

The analysis of the modes of contextual browsing is interesting, and certainly a good contribution of the paper. It is hard to follow however. Perhaps this is due to the fact that table 3 seeks to render what is essentially an assignment of behaviours that depends on 2 dimensions. But I was confused by the fact that the level that has all the target and profile specifications (last line) does not enable all serendipity behaviours enabled by previous levels. Behaviours 1 and 3 are missing: why?

Assessment/evaluation/comparison:
This is simply inadequate. Attempting to assess serendipity effects in an automated way and without reference to actual users seems dubious at best; furthermore, it is not really clear that this is what the authors' automated tests aim to measure anyway, as they are oriented entirely towards assessing similarity. Finally, the methodology of the automated tests is, as the authors admit ('perhaps a predictable outcome given that the prioritisation is also realised via similarity calculations'), somewhat circular. No real conclusions can be drawn from it beyond very basic sanity-checking.

Finally, there is little value of evaluating the approach that narrows worksets to the case of workset 2. Starting with 13 items, it is clear that the approach will have a non-standard behaviour. There could be some value in evaluating this approach for such small-size sets, but then the analysis should be much more granular: it cannot be at the same level as for bigger sets.


Style/quality of writing:
This is generally very good. There are however some strange choices made, which do not make reading easy, at time:
- the abstract does not introduce the problem, which is quite unusual
- the categories in table 1 (column titles) are not really introduced
- the explanation of Jaccard is not good. First in the text, when it refers to binary RDF tuples (RDF is made of triples). Then in the listing, when the notation g1[p,o] is left for the reader to understand, the loop in line 5 introduces an 's' variable that is not used in the loop itself, and 'matchScores' seems like a raw list of numbers, which is not indexed by entities or couples of entities. In 5.2 there is a mention of 'directed Jaccart prioritization' that hints that Jaccard similarity is not symmetrical (which it is). If it's not what is meant here, then what is meant?
- the text in 5.1.1 hints that 'Similarity views' are distinct from 'Entity connctions' but the caption of figure 1 refers to the similarities being the content of the Entity Connctions pane.
- the choice of reflecting the scales in figures 1 and 2 is not helpful. It might be graphically elegant, but that is offset by the reader having to struggle her way back to the scale on the left to figure what the similarity values are. Also, the choice of a vertical line that corresponds to 'unprioritized' is confusing. It would have been better to have, for each iteration, a 'cell' that clearly groups each of the settings.
- the discussion point on dataset modeling in the conclusion is worthwhile to make, really. But such a long explanation of what was observed does not fit a conclusion, especially when there is a discussion section where it would fit perfectly.

Minor comment: a word seems to have disappeared from the last sentence of section 5.1.1.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,Timothy,Hill,timothy.hill@europeana.eu,357,2/17/2018,14:41,no
411,109,172,Mauxxxx Henxxxxx,4,"(OVERALL EVALUATION) This paper builds on an earlier piece of work is well suited to JCDL and the wider library community. The simulation of serendipity browsing in large-scale collections is very interesting.

The article begins with contextual browsing and the various specifications, but there doesn't appear to be any discussion of the profiles of the users, which I think may have given a little more insight. And Table 1 requires some explanation.
More fundamentally, the paper doesn't appear to hang together, for example, the early sections on serendipity does not seem to sit well with the later worksets similarity. Perhaps this should be 2 short papers. 

The paper is well written, but I think that it suffers from a lack of coherence.

Minor problem - last sentence in 5.1.1 is not complete.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) First I must confess that my confidence in reviewing this paper is not high, and I am not familiar with the original work.","Overall evaluation: -1
Reviewer's confidence: 2
Recommend for best paper: no",-1,,,,,2/26/2018,8:48,no
412,109,88,Gioxxxx Maxxx,5,"(OVERALL EVALUATION) In this paper, the authors present a family of information-seeking strategies for contextual browsing. These strategies are used to support the traversal of contextual spaces.
The paper is well presented and it has an interesting statistical test analysis for the three main hypotheses.
There are a couple of things in the experimental part there are not completely clear to me, though.

1) ""We have chosen two seed worksets for our simulation: seed workset 1: a subset of 50 entities randomly selected from the category Roman Catholic theologians; and seed workset 2: the more specialised subcategory of Benedictine Writers, which contains 13 members."" Then you write ""One workset is produced per strategy for each iterative step""

It is not clear, at this point, what is a workset. It may be just a question of wording or rephrasing the concept.

2) Section 6.3 the value 1/5 has been chosen for some reason? Is this  20\% the result of some optimisation in other experiments?

Just a few more things:

- metadata[6] -> metadata [6]

- Figure 3, use the same range of values for y axis","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/19/2018,14:19,no
414,111,323,Horxxxx Saxxxx,1,"(OVERALL EVALUATION) The paper studies the still relevant problem of author name disambiguation in the context of digital libraries.  The novelty of the paper is on the combination of superficial and ?€?deep?€? (!) features which are used in a simple neural network to discriminate pairs of papers which contain the same surface author name.  The superficial features basically refer to string similarity measures such as comparing the list of authors of two publications and lengths of the resulting list of co-authors.  Comparison of publication titles is also used. The ?€?deeper?€? features make use of word embeddings trained on a DBLP dataset. These representations are    used to compare two high relevant tokens from the titles or combined using all token comparisons.  
The approach is evaluated with 5 different author names. Results appear to indicate better performance when deep features are used.
The paper is well written and relevant for the conference. The state of the art, which just concentrates on neural networks, lacks in my opinion background on the problem  and more classical works. Some aspects of the method are unclear, for example it is unclear how relevant tokens from the titles are used when top tokens are identical. It is also unclear how you compare the co-authors (token-based or named entity based).","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/20/2018,10:16,no
415,111,108,Andxxxxx Ferxxxx,2,"(OVERALL EVALUATION) In the paper, the author evaluates the utility of semantic features for AND task, using word embedding vectors of tokens selected from publication titles. The author uses the word embedding vectors trained on DBLP publication titles to add two semantic features.  The first feature is based on tokens with the highest IDFs and the second is based on the minimum distance between all pairs of tokens from titles of two publications.

The author evaluates experimentally the use of these two semantic features on five same-name groups from KISTI dataset. The results showed improvements in two groups.

The paper is well written, the problem addressed in the paper is a relevant and still challenging problem for the digital library academic community, but the contribution is unclear. What is the contribution compared with the previous paper ?€?Semantic Author Name Disambiguation with Word Embeddings?€??

Positive points:
- The author highlights the potential of the use of semantic features in the author name disambiguation task.

Negative points:
- The main negative point is about the contribution of the paper. It is not clear the contribution of this paper compared with the paper ?€?Semantic Author Name Disambiguation with Word Embeddings?€?.

- The related work does not discuss about AND techniques. In this section, the author should at least give an idea of the amount of work developed on task AND.

- The authors did not investigate the causes of improvement in ?€?J. Mitchell?€? and ?€?M. Chen?€? groups and no improvement in the other groups.

- The explanation about the two semantic features needs for improvements.

- The experimental evaluation is only initial.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/16/2018,13:21,no
416,111,58,P??xxxx Caxxx,3,"(OVERALL EVALUATION) This paper presents a feature engineering study for the task of author name disambiguation (AND).

Although the proposal could be useful to the community, I believe the work is too superficial to add any value to the existing literature. First, very few features were tested. Second, features were not studied individually and we are given no information of their prevalence in the dataset. Finally, it is not clear why use neural networks for this type of test when simpler classifiers, (i.e. classifiers where the impact of each feature is much more easy to interpret) are available.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/16/2018,15:27,no
417,111,407,Jixx W,4,"(OVERALL EVALUATION) This paper provides little novel contribution compared with the paper published in TPDL 2017 by the same author. In fact, in the TPDL 2017 paper, the authors used a more sophisticated method and careful comparison. For example, the NN contained multiple hidden layers instead of only 1. The title was broken into 2, 3,4, and 5 ngrams. The authors also compared DBLP, Microsoft Academic Graph data, and two w2v implementations. In the JCDL 2018 paper, the author crafted two simple ""semantic features"" by (1) converting the ""most significant"" tokens of two titles into vectors and calculate their cosine similarities and (2) calculating the cosine distances of the most semantic similar words in the title. In particular, the authors did not perform any comparison with the TPDL results though they come from very similar ideas. In fact, the TPDL results beat the JCDL results even in the best scenario. 

The paper ignores the state-of-the-art works on the author name disambiguations, such as Treeratpituk et al. (2009) and Clotta et al. (2007). These work address both aspects of name disambiguation: different people with the same surface name, and different surface name belonging to the same person. This paper only addresses the first one and only considers titles. The experiments are based on cleaned and completed metadata (DBLP) in only computer science. The proposed method will fail when the same author publishes in multiple disciplines, which becomes more common.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/24/2018,20:05,no
418,112,275,Fedxxxxx Naxx,1,"(OVERALL EVALUATION) The paper presents a method for detecting and distinguishing same-name entities in a knowledge base via hierarchical clustering. 

While reading the paper, I've encountered the following issues:

- It is hard for me to understand why this work should be relevant for the digital library community. I see that it could have implications for our works (for example through the use of entity linking), but the authors do not motivate why they submitted a knowledge-base paper to JCDL.

- I have found the description of the method in section 3 extremely difficult to follow. Instead of offering two different schema (Fig 1 and 2), it would be better to have a figure with a clear example.

- There is no comparison with other approaches for performing the task, apart from a very-similar previous approach presented by the same authors. The authors could, for example, experiment with approaches from the literature on detecting errors in relations in a KB (see [1] as a starting point).

- The adopted datasets are not clearly described, I would recommend the authors to present them in a different section and offer additional information, starting with a URL. For example, I imagine that the authors are referring to a different D-Lib (http://www.dlib.org/), as they say that most of the articles are in Chinese. I would additionally recommend the authors to also employ resources in English, in order to compare with approaches for similar tasks developed by the community [2].

 - Finally, I would recommend the authors -- if they do intend to re-submit this work to a digital library conference -- to use the 2 additional pages (the submitted paper is 8 pages long, while JCDL accepts full paper up to 10 pages) for showing useful applications of this technology in digital libraries.

For these reasons, I have to recommend rejection.

[1] Melo, Andr??, and Heiko Paulheim. ""Detection of Relation Assertion Errors in Knowledge Graphs."" Proceedings of the Knowledge Capture Conference. ACM, 2017.

[2] Gardner, Matt, and Tom Mitchell. ""Efficient and expressive knowledge base completion using subgraph feature extraction."" Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 2015.","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2/5/2018,14:33,no
419,112,374,Domxxxxx Tkxxxx,2,"(OVERALL EVALUATION) The paper describes a clustering method for detecting incorrectly merged entities in knowledge bases. In general, this is an interesting task and relevant to digital libraries. The research described in the paper is fairly novel. The paper is well written and easy to understand. I believe it would be a good contribution to JCDL.

The methodology is well-designed. I wonder if it would be possible to enhance proposed method with reasoning over the entities themselves, similarly as described in the introduction. For example, if we know that tripleA cannot belong to the same entity as tripleB (because of the year of birth and death inconsistency), maybe these triples should never be merged in clustering, regardless of their occurrence vectors?

I also noticed some problems with the evaluation. The authors use accuracy and recall to evaluate their approach, and the immediate question is why precision is not given. Also, F-score is calculated from accuracy and recall, which is not correct (F-score is a harmonic mean of precision and recall). I strongly suggest correcting these issues.

Minor issues:

1. ""Hierarchical clustering is performed on the set of description documents of triples of an instance."" I believe the clustering is performed on the set of triples, not documents.

2. The paper lacks the details about finding the mentions of instances and objects in the text. Is it based on equal matching only? Does it take into account variations in the names, such as ""I. Newton"" vs. ""Isaak Newton""? Also, does it process correctly referring to the instances by words such as ""he"" ""she"", etc. (coreference resolution)?

3. In section first 3.2 the occurrence vectors are defined as binary vectors, but in equations (2) and (3) it seems like the elements of the vectors are the number of occurrences. This is a bit confusing.

4. Equation (4) could be written compactly as V1 + V2, the sum of vectors is a well-defined operation.

5. Determination of the threshold of similarity?€?length ratio. The authors write ""An appropriate threshold should be determined by domain experts based on the characteristics of the corpus and the nature of the instances for inspection."" It might be worth mentioning that choosing the threshold could also be done using a validation corpus.

6. In the evaluation recall is calculated as ""number of detected same-name instances"" / ""..."" I think it should be ""number of _correctly_ detected same-name instances"".","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/12/2018,15:03,no
420,112,107,Andxxxxx Ferxxxx,4,"(OVERALL EVALUATION) In the paper, the authors propose an approach to detect entity mixture in knowledge bases, i.e. attributes of entities erroneously attributed to other entities. This is a relevant problem, can occur mainly due to the fact that many entities have the same name, and  can decrease the quality of services built on knowledge bases.

The proposed approach for identifying mixed entities is based on a collection of documents. The authors identify the documents from the collection, in which the pairs of instance and object names, obtained from each triplet, co-occur, creating occurrence vectors for each pair.

The authors perform a hierarchical clustering technique to group the vectors of the same entity. If the number of clusters is at least equals to 2, the authors assume the existing of entity mixture.

What is the effectiveness of the method in identifying/splitting the data of the mixed entities?

The authors should explain the division by the minimum length of the vectors in the similarity-length ratio.

The definition of the similarity-length threshold value is not intuitive. The value of this threshold is crucial for the correct implementation of the approach.

The authors must improve the related work.

Positive points:
- The authors present a simple approach for detecting mixture entities based on a corpus and a hierarchical clustering technique.
- The approach presented in the paper aims to check the quality of a knowledge base on all triples of an entity.


Negative points:
- The replicability of the experiments is not possible. The dataset is not available. 
- The abstract and introduction do not summarize the results of the paper.
- The definition of the similarity threshold value is not intuitive.
- Figure 1 needs for improvement.
- The corpus of documents and the entities/triples used in the experimental evaluation are not available.
- The authors do not check the resulting clusters for verifying the correct grouping of the vectors.
- There is no analysis of failure case.
- The authors did not take advantage of the set of pages available to carry out a better experimental evaluation of the proposed approach.
- The originality of the paper is not clear compared with the paper [31].

?€??€? organization owing to its structured representation?€??€? --> ?€?organization due to its structured representation?€?","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/16/2018,15:48,no
421,113,40,Joxx Borxxxx,1,"(OVERALL EVALUATION) This paper proposes a solution for a PID infrastructure as an extension of the already existing ""Handle System"" created by the CNRI and now managed by the DONE Foundation...

A knowledgeable reader can understand the general motivation for this problem, but the paper really never explains it in a way that can explain to a reader ""why this and this way""?

The related work is spread by the sections I and II (in fact, we can find more related work in I than in II...), and then the requirements for the solution that is proposed never are explained. This means we are presented with a proposal of a solution for a problem that we might be aware in general terms, but we never are explained in detail enough to make us judge if the solution is appropriate or not... 

Concluding, we can understand that the proposed solution can result in a working service, but we cannot conclude why is it better than any other existing solution (e.g., DOI, for example...), or what particular aspects of the problem is it intending to prov...

This seems to be work from the ""RDA PID Kernel Information WG"", but it never is clarified if there is any connection to the work of the ""RDA Data Citation WG""... I believe the results of the WG (and the state of the art in data citation and metadata for scientific data) should be taken in consideration to support this work! Maybe the authors had did it, and simply had ""jump"" to the solution without feeling the need to report that in the paper, but frankly speaking, in my understanding of the problem, the consolidation of those requirements are the true core challenge of this problem! I believe that once the community can reach a consensus on those functional issues, the technical solutions can be straightforward, considering the present state of the art of the technology...

Finally,the non-functional performance analysis can be relevant, but not necessarily a core issue to deserve so much ""space"" in the paper... it looks a bit naive claiming the  processing performance of an information entity only depends of its size, and not also of its structure (in simple terms, it is always faster to process one attribute with a 1KB value than 1k attributes with 1byte value each...)

Suggestions for revision of the text: 

(in abstract) ""The Persistent Identi???er (PID) is a globally ..."" should be ""A Persistent Identi???er (PID) is a globally ...""

(1st page, second paragraph) ""A PID must be globally unique - unlike people, who can both have the same name, there can be only one PID."" maybe should be ""A PID must be globally unique - unlike people, who can both have the same name, there can be only one WITH A SPECIFIC PID.""

(page 9, first paragraph): These two sentences ""are strange"": ""We compare the provenance view generated from a full provenance capture system. Then using the model of minimal provenance stored to the PID record, we compare.""

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This could be a poster... I'm confused with it as a paper... it seems more a report of a project, done in a context where some ""wider"" issues were taken in consideration, but that are here not explained in the paper to make us access it properly...","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/11/2018,20:04,no
424,113,426,Zhxxxx Zhxx,2,"(OVERALL EVALUATION) This paper studied the Persistent Identifier (PID) problem, and proposed to add PID Kernel information and profile and data typing into a PID system. The overall conclusion is that this proposal can help data sharing at affordable extra cost.

Many readers will not be very familiar with the PID system. 

I agree with other reviewers that this paper may lack one key piece, which is comparison with some alternative methods such as HTTP link headers and DOI, as suggested by other reviewers. Otherwise it is not convincing why the proposed method is superior.","Overall evaluation: 0
Reviewer's confidence: 2
Recommend for best paper: no",0,,,,,3/9/2018,5:33,no
425,113,105,Mohxxxx Maxxx,3,"(OVERALL EVALUATION) This paper aim to illustrate the performance implications of the addition of the PID kernel information and data typing to the PID resolution process.

The writing of the paper is poor and shallow. No clear justification of what is the importance of the work. The evaluation section is very weak. Saying that adding 20% (which is dramatically worse) is acceptable regarding the benefits added by the kernel information is not a clear, sound evidence. 

Abstract, Related work, Conclusion are very short. Some related papers are mentioned in different parts. They all should be put in one place (related work). 

You are measuring the response time and not using it to measure the enhanced PID without justification. 

What is the x-axis of Figure 7? What is the right part of figure 10 talking about?

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The paper would be more suitable for short paper submission.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/23/2018,11:56,no
426,113,75,Yixxxx Cxx,4,"(OVERALL EVALUATION) This paper proposes a good idea to add PID Kernel Information to a PID system and provides a real-world use case. The most important conclusion from this paper is that the addition of 20% service time performance cost and the PID Kernel Information record should be no larger than 500KB in size. However, it is not clear that how this 20% be set and would be better to have some references or more experiment results to backup this argument. 

Need more detailed explanation of the handshake diagrams. In Fig 1, according to the paper, the only difference between (a) and (b) is that (b) added a step to use PID to get retrieve information from Data Type Registry. Thus, in (b), should the PID: 11723/fpp/bar2 change to Handle: 11723/foo/bar2? Should the ?€?Resolved PID?€? be ?€?Resolved Handle with PID?€?? 

In Fig 2, it looks that the client also needs to change in order to handle the different format of the response. However, I don?€?t see the description about how to accommodate this change and what performance will be affected in the paper.

In Fig 3. It looks like the key-value pairs are ?€?Data Type?€?(key) and ?€?Data Value?€?(value). So what is Index field in the diagram?

In Fig 4. The name field is missing in ?€?Simple Data DTRecord?€?. Should ?€?Scope: Range:?€? change to ?€?Range:?€??

In Table I: There is no explanation on ?€?Instantiated Size?€? and ?€?Template Size?€?.  Which one is the size of ?€?type definition?€??

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I am not sure if this paper is relevance to JCDL 2018 theme.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/23/2018,13:30,no
427,114,243,Kxx L,1,"(OVERALL EVALUATION) The paper proposes a new method to rank scientific papers and venues that considers citation time interval, the venue, and the citing papers. Papers and venues are positioned in a heterogeneous network to iteratively update their scores. The method is evaluated on the AAN dataset and compared to several other ranking methods. The paper claims superior performance of the new method. 
In fact, considering the prestige of citing venues and citation time intervals is not new. The contribution of this work is to combine the factors in a heterogeneous network to mutually reinforce venue and paper scores. The paper should be more precise about its contribution.
My primary concern is on the experimental design and evaluations. The results provided in the paper do not fully support the claim:
For example, Table 1 only provides results from MR-Rank without comparisons to other methods. 
The use of mIF is not clearly explained. Is it calculated per year for each venue or across the years? If calculated per year, the total number of papers (e.g. ACL 2002) is a constant, and the number of citation can only go up each year, thus mIF can only go up. Comparing other methods to mIF may not be a good evaluation method.  
It?€?s not very clear what the purpose of Figure 2 is, because it does not compare the methods. I assume you can calculate other measures by year, and they would vary as well.
The paper performance is not convincing. It uses the correlation between earlier performance (1997 - 2002) and later performance (2003 - 2012) to evaluate the methods. The correlation just indicates the consistency of the methods. But consistency is not indicative of their accuracy. 
Therefore, from the results presented in the paper, it is hard to claim the better performance of the new method. If the authors intend to show superior performance, they should design better experiments. 
Some minor things:
1.	The first three equations in section 3.1 should be numbered as well.
2.	Is there any particular reason that the citations between venues do not consider the time interval weights?
3.	Equation 2 has the direction wrong, should be from citing j to cited i.
4.	Equation 4, W_vv(v_i, v_j) should be W_vv(v_j, v_i), because v_j is the citing, v_i is the cited. Vs(v_i) should probably should Vs(v_j).","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/2/2018,17:10,no
429,114,259,Phixxxx Mxx,2,"(OVERALL EVALUATION) This is a strong short paper with lots of material included. The idea of the paper is excellent:  introducing novel ranking techniques for scientific papers and venues. This is very relevant for JCDL. 

The MR-Rank approach (the main algorithm) and the results look very promising.

I wonder: How are completely new venues ranked where you have no PageRank as starting point? Is the MR approach capable to solve that issue.

Conclusion is too short. The authors should say something about their approach in a more heterogeneous setting. AAN is a narrow and small community. A snapshot of Web of Science would be an interesting bigger data set.

Minor points:

- SJR and SIF are not introduced
- Abbreviations of venues are not introduced (CL, ACL, ...)
- ""In this table, 5 out of 10 top-ranked papers (in bold) are published in top 6 venues (in bold)."" -> should be ""... are published in top _4_ venues (in bold)""?
- references can be shortened to get more space which is necessary to extend conclusions

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) A good short paper.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/21/2018,22:11,no
431,114,232,Albxxxx Laxxxx,3,"(OVERALL EVALUATION) This short paper proposes a ranking strategy specifically designed for scientific papers and venues. The proposed strategy is based on a heterogenous network that consists of three distinct sub-networks: a paper citation network, a venue citation network and a bipartite network that ties papers and venues. The paper is in general well structured, but requires a detailed proofreading to improve its text. Moreover, although the reported results seem promising, they require a more detailed discussion to support some specific claims.

Positive Aspects
- The paper addresses a topic that is relevant to JCDL.
- The paper is in general well structured.
- Results seem promising.

Negative Aspects
- The paper requires a detailed revision by a professional proofreader to improve its text.
- Even considering that this is a short paper, experimental results require a deeper discussion.
- Several important issues in the paper are left without an explanation.
- References have been misplaced in several parts of the text.
- The paper does not follow the ACM style adopted by JCDL.

Specific Comments

1. The paper title is misleading when refers to ?€?Heterogeneous Academic Networks?€?.  Why do you use the term ?€?heterogenous?€?? Although your network is composed of heterogeneous nodes, the term ?€?heterogeneous academic network?€? seems misleading in the context of this work (see https://en.wikipedia.org/wiki/Heterogeneous_network). Maybe ?€?multi-node?€? is a more appropriate term. Check this throughout the paper.

2. Some terms in the text are used interchangeably, which is not  appropriate in a scientific paper. For example, you use ?€?measure?€? with the meaning of ?€?metric?€?, but ""measure"" express the size, amount or degree of something  (e.g, in Sect 1, first paragraph: ?€?However, many such classical measures are based on ?€??€?. The word ?€?metrics?€? is the most appropriate in this case). It seems that you also use the words ?€?framework?€?, ?€?algorithm?€? and ?€?method?€? interchangeably throughout the paper (see, for instance, the title and the second subtitle of Sect 3, as well as the discussions in Sect 4.3, 4.4 and 5). It is better to adopt one of these three terms and use it consistently throughout of text.

3. References have been misplaced in several parts of the paper. For instance, in the third paragraph of Sect 2 (Related Work), Meng et al. is not reference [15]; in fact there is no reference Meng et al., but Meng and Kennedy [5]. Also there is no Yan and Ding [16], but Yan, Ding and Sugimoto [15]. Also, Tri-Rank is not reference [17], but [16]. Check the references throughout the paper (or use an automatic citation tool). 

4. Still regarding references, they should be cited in the same order they appear in the list of references. Thus, [11-13, 4] should be [4, 11-13]. Also, do not use a reference number as the subject of a sentence. For example, ?€?Among all the works ?€?, [17] is the most relevant to our work.?€? Replace this by ?€?Among all the works ?€?, that by Hassan and Getoor [17] is the most relevant to ours.?€? In this same paragraph there are other examples of this kind of construction that should be rephrased. In addition, according to the ACM paper style adopted by JCDL, references should be listed in alphabetic order, not in the order they are cited in the paper. 

5. It seems that the authors have not used the proper ACM template when preparing this manuscript. The following comments should be taken into consideration: 

a) Section titles should have all its words capitalized.
1 INTRODUCTION
2 RELATED WORK
3 THE MR-RANK FRAMEWORK
4 EXPERIMENTS
5 CONCLUSIONS
 
b) Subsection titles should have the first letter of each word capitalized. 
3.1 Preliminaries 
3.2 The Ranking Algorithm
4.1 Dataset and Settings

6. At the end of Sect 1, you refer to the dataset used in the experiments as the AAN dataset. However, reference [6] refers to it as the ACL anthology corpus. Check how this dataset should be named.

7. A brief  paragraph describing the paper structure should be added to the end of Sect 1.

8. When describing your algorithm (Sect 3.2), Rule 1 refers to AVE_Vs (v_k) as ?€?the average score that venue  v_k obtains in the last three years?€?. Why three years? Please explain this decision. Also, explain the role of the parameter alpha.

9. When describing equation (3) in Rule 2 (Sect 3.2), invert the order of the components (3) and (4) so that they can follow the same order they appear in (2).

10. For better reading, rephrase the last sentence of the paragraph that describes Algorithm 1 by removing the itemization as follows: ?€?More specifically, at each iteration step, we (1) update the scores of all the papers using Rule 1 and (2) update the scores of all the venues by using Rule 2.?€?

11. Sect 4.1 (Dataset and Settings) should justify the choice of the baselines. Although the choice of PageRank and HITS might be obvious, you should at least comment of the other ones, which seem to be variations of the proposed algorithm. Also, the parameter setup described at the end of this section also requires some explanation. You cannot just say that it is ?€?good for performance prediction of papers.?€?

12. Explanation in footnote 2 is not clear. Please, justify your choice.

13. Try to explain why HITS has obtained the smallest precision value. Just making such an observation is not enough!

14. Results expressed by the graphs in Sect. 4.2, 4.3 and 4.4 should be discussed in more details.

15. The paper requires a detailed proofreading in order to correct many typos and grammatical errors throughout the text. As a specific point, numbers from 1 to 9 should be written out. For instance,  the caption of Fig. 3 should be: ?€?Convergence rate of the six algorithms.?€?

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This short paper addresses a topic that is relevant to JCDL and presents some preliminary results very promising. However, it seems it has been written in a hurry and the authors had no time to proofread it properly. The text has many typos and grammatical errors, and the results are just superficially described. Besides, the paper does not properly follow the ACM template, which shows some carelessness from the authors. In view of that, I do not recommend the acceptance of this, even considering the promising results reported.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/25/2018,1:06,no
432,114,186,Bixx Inxxx,4,"(OVERALL EVALUATION) This short paper presents the MR-Rank (Mutual Reinforcement Ranking) method for ranking journals by simultaneously ranking papers and venues based on heterogeneous networks. Compared to previous work, the authors include the citation time interval and past performance of a venue in their ranking method, giving higher scores to papers with short citation intervals published in prestigious venues. This study provides insights that can help guide future research in journal ranking based on analyzing mutual reinforcement in heterogeneous networks.

Overall, I think this is a good study. Although I would have liked to see how MR-Rank compares to more contemporary ranking methods, especially those that also use mutual reinforcement, like Tri-Rank, which was mentioned in Related Work. 

Finally, it is unclear to me what sets this study apart from other recently published studies: 

D. Yu et al, ""A multiple-link, mutually reinforced journal-ranking model to measure the prestige of journals,"" Scientometrics, vol. 111, (1), pp. 521-542, 2017.

X. Jiang et al, ""Exploiting heterogeneous scientific literature networks to combat ranking bias: Evidence from the computational linguistics area: Exploiting Heterogeneous Scientific Literature Networks for Fighting against Ranking Bias: Evidence from the Computational Linguistics Area,"" Journal of the Association for Information Science and Technology, vol. 67, (7), pp. 1679-1702, 2016.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/23/2018,4:38,no
438,115,411,Zhxxx Xx,1,"(OVERALL EVALUATION) This paper developed a method to identify and quantify changes in the playback of archived web pages. The results are interesting and useful, but I have a number of reservations.

The results show none of the discovered differences is malicious, and all replaying changes seem reasonable. The value of the paper, instead of being about the trust of individual archive, would be either on the limitation of the current archival crawler method (in comparison to, e.g., the screenshot method, which does not suffer this problem but has its own set of limitations), or the unreasonable expectation of static replaying from crawled mementos. In this sense, Michael's evil archive is but a remote possibility not yet materialized. Trust and third-party certification is indeed necessary, but this paper has not shed light on how to do so. It only tells us how not to do so, which is obvious.

Also, it's not clear if the selection of URI-Rs and URI-Ms is sufficiently random to accurately represent the archival quality of each archive. Also, since many changes are caused by the way webpages are constructed, an archive's selection of URI-Rs may affect the percentage of changing reconstructions.

It's not clear why Merkle tree is necessary or advantageous in detecting changes. What are the differences from simply using ""diff""?

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I would agree to accept the paper as a short paper focusing on the experiments and results, but limit extrapolating the results to the trust issue.","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2/19/2018,0:25,no
439,115,396,Nicxxxxx Woxx,2,"(OVERALL EVALUATION) The authors outline the issues in adapting cryptographic hashing methods to web archives. The paper does a good job framing the importance of the need for trust measures and tamper detection for web archives in terms of legal and civic contexts. It presupposes a bit of knowledge from the reader regarding the HTTP and Memento Protocols, although most basic concepts like URI-Rs & URI-Ms, etc. are defined early on.

The methodology is a bit hard to follow at times. For example, in 3.1 Table 1 tells us the total URI-Rs collected from each source; however, there?€?s an ordering pattern to the list of URI-Rs, e.g., Moz then Memento damage URI-Rs then alternating patterns for the HTTP Archive and WAHR (10 URI-Rs and switch back and forth between the two collections until a limit of 10000 was hit). Is table 1 necessary? Would a table detailing a composition the URI-Rs actually used (e.g. the 10,000) make more sense instead?

The titles for tables 3-5 are fairly vague. I understand from the text that table 3 represents new URI-Rs extracted from the mementos in Table 2. Table 4 includes URI-Rs from a list provided directly by the archival institutions and Table 5 is generated from the authors?€? Python script. As a bit of reinforcement, the table titles should reflect something of the URI-Rs origins.
  
I?€?m not clear on why 200 URI-Rs was selected as the minimum threshold for including smaller archives in the comparison? Is that a robust enough number? It?€?s also not clear how the URI-Ms are distributed temporally for each archive. Would that have impacted the final ChangeM scores? Some archives may skew more recent than others and would likely have more JavaScript issues. 

It is novel in there?€?s a detailed discussion on how fixity is complicated within web archives and there have been no studies the consistency of hashes of mementos and no comparisons of such across different web archives that I?€?ve come across. The conclusion would be stronger if the authors could speak a little more toward possible applications of their findings and ways forward.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/15/2018,18:58,no
441,115,367,Nicxxxxx Taxxx,3,"(OVERALL EVALUATION) Authors retrieved and hashed a set of composite mementos from multiple web archives on a repeat basis to assess the stability of their fixity. They found that almost 20% of composite mementos did not have a stable hash across the successive measurements, due to changes in HTTP status code, HTTP entity, TimeMap, HTTP response headers, and dynamic values generated by JavaScript.

Authors correctly observe that the reliability of web archives is presently largely based on institutional reputation, and this is likewise the case for web archives as applied in litigation. In the legal context, this has started to give way to a concern with provenance - e.g., can Internet Archive attest to the conditions of the generation of all subsets of archived web data made accessible through the Wayback Machine? There are at least a few more web archive-specific considerations - e.g., completeness and temporal coherence - that likely pose more conspicuous reliability concerns to courts than non-repeatability of hashes for composite mementos. Nonetheless, authors have highlighted a legitimate and novel reliability measure worth taking into account.

I think the article could benefit from further discussion of the importance of the findings. The results suggest the hash value for a composite memento may not be a reliable indicator of the reliability of that memento or of its parent web archive, since that value may vary due to any number of apparently common non-malicious and unpredictable changes. Considering the litigation context, the study is therefore principally useful in affirming that discrepancies in repeated hash checks of a composite memento cannot be taken for granted to mean that the information or web archive itself has been corrupted. That is a conclusion some ways away from having a new tool to detect malicious alteration of archived web resources.

It strikes me that if the devised method for hashing of composite mementos is to be useful for assessing reliability, web archive systems need to be better architected to minimize or mitigate these copious instances of false positives. Do specific recommendations for the enhancement of web archive systems follow from this research? If so, it would be great to see those unpacked.

A few additional questions for the authors:
* Could cached responses from a CDX server API serve some role in evaluating memento fixity over time? What about identifying identical mementos stored in different web archives?
* Why do you suppose there was the degree of observed variability in cumulative change across web archives? It did not seem to correlate with the web archive replay system employed.

A couple of quibbles:
1: ""We introduced some requirements to be fulfilled in order to generate repeatable hashes...(1) A generated hash must be repeatable"".
This seems like circular logic.

2: ""OpenWayback will replay the content of any selected archived web page in the browser.""
It might be worth qualifying that only those mementos in the index for that particular OpenWayback instance may be replayed.

A couple of copyedits needed:
5: ""The last column 'Transformation' in the table indicates different type of modification performed by the archive.""
7: ""This is indicted by a change in the 'Location' HTTP header.""","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/18/2018,23:12,no
442,115,354,Vexxxx Srixxxxxx,4,"(OVERALL EVALUATION) The paper talks about an interesting approach to identifying and quantifying changes to archived webpages. The authors retrieve webpages from various archives for experimentation, and after suitable pre-processing, develop and evaluate their algorithms on this dataset. The idea of using Merkel trees to detect for signature changes in this context is unique. The authors also do a good job of surveying related literature. The paper also covers various other tools in the domain of web archiving and retrieval in sufficient detail. 

My concern however is regarding the message of the paper. A lot of work has been done by the authors, but it's not clear what's to be gained. The idea that archived webpages (aka mementos) can render differently at different time points is certainly well known. The authors' experiments merely confirm this belief. Although in addition the authors also provide a few insights in terms of the types of errors that can be expected in this scenario, this is really not really substantial in terms of insight or utility. Additionally, the authors propose a statistic (called 'Change') that purportedly quantifies the changes in the mementos over time. It's not clear how or why this is useful, and what insights it is actually providing beyond what's already known. The authors could perhaps discuss their results and conclusions in greater detail.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I wouldn't mind if others think this should be accepted. There are quite a few useful things here (esp. the literature review and pointers to web archiving tools) that readers are likely to find useful.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/24/2018,17:11,no
443,116,27,Omxx Alxxx,1,"(OVERALL EVALUATION) The authors describe a longitudinal study that was designed to get insights about retrievability of links over time. The study is performed in the context of archiving links for a story. The experiment consisted on issuing 7 queries daily on Google over a period of 7 months. Several statistics are presented, and the authors provide recommendations for building collections using search engine data. Methods are described in detail, so it is possible to replicate results.

The problem is very relevant to JCDL and the overall approach based on SERP scraping is interesting. Having said that, the focus of the paper changes along sections making it a difficult to understand what exactly is the problem that the authors are trying to solve. Sometimes, it looks like a study on Google ranking whereas in other sections is about news stories only.

The feedback below is mostly about the methods and the overall execution.

At first, the problem shares a similar structure to TDT (Topic Detection and Tracking) where the goal is to segment a stream of data and to identify new events. One can think of a variation where Google is providing the stream and the goal of the scraper is to segment the urls. There is no mention to TDT on related work.

The authors mention that a search engine is biased because it provides the most recent documents. Recency is a ranking feature by design. The ranking of search results is affected by behavioral factors like click data and queries. If the authors want to study the same topic over time, they need to study also how the topic evolves (topic tracking). One can enter the 7 queries on Google trends and have a clear view of the spikes, stories, and (more importantly) related queries. It is a bit surprising that the authors did not use trends as input signal on the scraper.

The difference between general and news is a bit unclear in the methodology. The intent for a news vertical is clear: the latest news on a topic. A generic SERP blends content from various sources as satisfying a user intent is more difficult.

Links are retrievable using the right query terms. Expecting the same ranking for the same query-url pair when the query is informational is probably not a realistic assumption. There is news content published daily, ranking changes, and query re-formulation (from query logs) evolves.

In summary: interesting idea with lots of potential. There are issues with the overall methodology.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/13/2018,19:39,no
445,116,372,Alxx Thxxxx,2,"(OVERALL EVALUATION) This is a solid research paper on the topic of the temporal variability of search engine result pages and its relation to identifying seed URLs for event-based or thematic web collections. The writing is clear and precise, the illustrative figures and tables can be dense but are understandable, and the framing of the research questions in the context of related work is very good. The work is apropos for JCDL, and the takeaway finding that SERP-based collection building should start soon after an event and be repeated persistently afterward due to the variability of top search results is useful for web curators, even if the conceit of basing thematic web collections solely on the first five pages of SERPs is less likely to be followed in practice. The future work suggestion of assessing the effectiveness of SERPs for non-news based collections is welcome.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/19/2018,19:02,no
446,116,144,Vixxx Gxx,3,"(OVERALL EVALUATION) Presents a collection building approach that extracts URLs from Search Engine Result Pages. The authors limit their approach to news collections and results returned by Google. 

It identifies the need to capture results from different stages of events, starting from the earliest possible instant. The paper quantifies the rate at which search results appear and move around Google's ranked lists for a small pre-selected set of queries over a 7 month period.

The research work is completely specific to a single search engine's (Google's) ranking approach. Can the presented predictive models be applied to other search engines? How generalizable is it? There's no discussion of the types of queries: navigational vs informational and the possible effect on the presented model. What was the breakdown of the results based on their content or their source websites? Do results from traditional news websites do a better job of retaining their ranking? 

The researchers limit their results to the top few SERP. So when they claim that certain results are no longer discoverable, it must be noted that they're not available in the top N pages.

The paper does a decent job of starting from an understood assumption of the ephemeral nature of search results and quantifying it.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/17/2018,0:55,no
447,117,113,Schxxxxx Fx,1,"(OVERALL EVALUATION) The study is interesting as it attempt to understand the interactions among the Study Team, TWU libraries and its services using STIN modelling.  However, the paper lacks sufficient description and clarity in a number of areas.  

Section 1.2:  How is the membership of the TDL related to the study? What are the existing library services and what are the new services that are introduced and observed as part of the modelling?  In other words, the scope of the study and boundaries are not clear.   The scenarios and states of the situation needs to be clearly defined.  

Section 2: The STIN strategy outlines 8 elements ?€? these must be reported clearly in the contest of the study.  The last paragraph ?€?Using the aforementioned elements?€? ?€? is simply too scant and cursory for a piece of scholarly writing.  ?€?One portion of the STIN related to the University?€?s data repository?€? ?€? there is a need to show an overall diagram or model to clearly show the various elements at least for this repository and the interactions you are modelling.  

Section 3:  What is the basis for data analysis used to derive the findings?  Some examples of what is collected, the time period of which they are collected and documented, how the data is analysed, and conclusions drawn are highly desirable.  i.e.  The methodology is basically lacking in the write up.    

What are some of the future work or extensions of the modelling are you planning to do? Is there scope for the work to be reported as a full paper?  In its present form, the paper lacks sufficient scholarly merit and contribution.","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2/6/2018,7:39,no
448,117,334,Yx Sxx,2,"(OVERALL EVALUATION) This paper intends to ""describe how Socio-Technical Interaction Network (STIN) modeling can be applied to understand the development of digital library services at TWU"" and to ""illustrate the development of influences between the DFW Schizophrenia Study Team, the TWU Libraries, and its institutional and data repository services.""  But the two and half page short text dedicated majorly to the brief introduction of the DFW study team and TWU libraries as well as the STIN model.  There is the lack of description on how STIN modeling was applied, how analysis was conducted, what concrete results were drawn, and what significant conclusions have been made. 

In its results section, which is less than 1/5 of the overall text, it claims through the analysis ""librarians were able to identify and articulate specific legal, political, and internal challenge areas."" However, what specific legal, political and internal challenge areas have been identified"" and how? Please do articulate. Also, how did ""an analysis of the fourth, fifth, and sixth elements of the STIN framework resulted in prioritizing issues related to technical knowledge and data control""? Again, there is no description of how analysis was done and there is no detail of concrete results provided here, except for some vague and general statements.

In the one sentence conclusion section, the authors stated using a STIN analysis .... was effective in creating a dynamic assessment of the social, economic, and political influences on the data repository. Again, what social, economic, and political influences have you assessed? And how do you show or conclude that this analysis was effective?

In addition, what are the research questions? What are the objectives of this study? Why is it significant to conduct this work? How is it linked to existing literature? And how does it advance the state of knowledge in this field? These all need to be addressed in a research. 

Overall this paper serves mostly as an initial background introduction of the project. I suggest the authors put efforts into the overall development and rigorous analysis of this project.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/13/2018,18:59,no
449,117,87,Pexxx Daxx,3,"(OVERALL EVALUATION) This paper discusses developments of library services at Texas Woman?€?s University, particularly the impact of a partnership with researchers studying schizophrenia on the development of digital resources, particularly in relation to data. It presents Socio-Technical Interaction Networks, originally developed in social informatics, as a theoretical lens for understanding these developments. 

While a potentially interesting approach, this paper has a number of shortcomings. Although short papers are intended for preliminary results, the research project presented does not seem sufficiently mature and the paper is substantially underdeveloped.

Although the pdf document is four pages long, the authors have not used the JCDL template: if the submission was put in this template, it would be substantially shorter than the four-page limit for short papers. This additional space could be used to:
?€?	Include a section on Background/Related Work. There is a rich literature in Library and Information Science on the developments in digital library services, particularly in relation to research data management, that should be referenced. For instance, see:
Ray, J. M. (2014), Research Data Management: Practical Strategies for Information Professionals. West Lafayette: Purdue University Press;
Cox, A. M., Kennan, M. A., Lyon, L., & Pinfield, S. (2017). Developments in research data management in academic libraries: Towards an understanding of research data service maturity. Journal of the Association for Information Science and Technology, 68(9), 2182?€?2200. https://doi.org/10.1002/asi.23781;
Pinfield, S., Cox, A. M., & Smith, J. (2014). Research data management and libraries: relationships, activities, drivers and influences. PloS One, 9(12), e114734.
?€?	Develop the findings and conclusions further. At the moment, the Results section is only two short paragraphs in length.

I would recommend resubmission as a poster.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/16/2018,23:05,no
450,118,318,Thxxxx Rixx,1,"(OVERALL EVALUATION) The authors of the paper present an analysis on the reelection of history in Twitter. The work is well motivated and even provides ideas on how to reuse the results in applications. The introduction also provides a summary of the contributions. However, contribution (1) of ?€?being the first?€? is not a real contribution. 

The dataset has been created by collecting related tweets for nearly one year and using manually selected hashtags. In addition, co-occurring hashtags have been added to the set of relevant hashtags. The complete list of hashtags is shown in table 6, however this table is the result of the analysis of the whole dataset. Under the assumption that the streaming API has been used, it looks strange that the analysis result influences the dataset creation. Therefore the collection process needs more details. Another weekness is the missing user information. It would be good to know how many users are talking about history and the distribution of tweets among the users. 

The analysis of the dataset is discussed in detail in section 4 and 5. However, the temporal analysis focuses a lot on WWI and WWII but there are a number other peaks visible in Fig.1. It would be interesting know more about the other events that are in the collective memory. Especially as other events are mentioned in the entity analysis it would be nice to build a link between them.

The paper finishes with very good discussion on the limitations of the applied method. Overall a very interesting paper that provides first insights into the topic.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/10/2018,14:54,no
451,118,178,Hexxx Hocxxxx,2,"(OVERALL EVALUATION) This is a well-structured paper including very interesting analysis of history-related content on Twitter. The analysis is sound and comprehensive, including all the necessary components. The research questions which were raised in the paper and guided the analysis were answered very well. It also covered potential applications and limitations, pointing to future work. One can question or debate about specific details of some of the analysis, but this does not change the overall quality of the paper. 

It would be interesting to use the approach described in the paper to study and compare the same history event, as it unfolds now, and some time in the future, e.g. on 5 or 10 year commemoration, to understand how these may be portraited differently.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2/16/2018,17:16,no
452,118,182,Xixx H,3,"(OVERALL EVALUATION) This paper describes a study on microblogs referring to history. It is related to digital history, particularly from the perspective of the masses (social media). It is an interesting topic, and as the authors argued, few studies have been focusing on this topic. The authors appear to have expertise in history-related information studies. As one of the first efforts in studying collective memory of public history, this study contributes to a new research direction in the broad field of information science. This valuable contribution is related to digital libraries, but the authors should make this connection more explicit, to better fit to the themes of JCDL.

As acknowledged by the authors, the reported study is mainly exploratory, with much room for future work. It is appreciated that the authors acknowledged and elaborated limitations of the study in section 6.1. These are potentially helpful for researchers with similar interests. In this regard, it would be very helpful if the dataset is made available for other researchers to continue exploring on related topics. This would significantly add up values of this paper! 

There are several technical issues that need to be fixed before publication:

In section 4.2.2, Figure 6 is said to be a plot of conditional probabilities, but what's presented seems joint probabilities instead, as each cell represents probability of co-occurrence of the two types of entities referenced by the row and the column. If it was conditional probability, then clarification is needed on which variable (the row or the column) is the condition, and the plot would not have been symmetric. 

In section 5.5, Figure 11 is said to ""place hashtags according to their entropy values calculated over the distributions of contained entities and mentioned temporal expressions"". To be honest, this description is not clear: how the entropy measures were defined or calculated? It would be helpful if an equation can be presented, like equation 1 for probability distribution. Also, entropy by definition has a value between 0 and 1, but the axes in Figure 11 are all from 0 to 8 (or 7). Are these typos or something I missed? It would also be helpful to go one step further to explain what it means to have a higher/lower value of entropy.    
 
For Figures 1 and 10 (the line plots), the lines are differentiated by colors, which is okay for e-versions, but might be impossible to tell on printed version in grey scale. Perhaps adding markers on the lines could help. 

In summary, this paper is publishable, given that 1) a better connection to the themes of JCDL is added; 2) the dataset is made available; 3) the above technical issues are resolved. All these are doable within a short period of time, and thus I recommend accepting this paper.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/18/2018,3:54,no
454,119,387,Anxxx Vexxxx,1,"(OVERALL EVALUATION) I very much enjoyed this well written article. I think it quite relevant to the JCDL community, it covers the prior art well and presents an intersting and complementary approach to the existing methods for predicting citation counts.    

There are nevertheless some improvements / modifications that I would suggest prior to final acceptance.  One - mentioned in the abstract and also later in the body of the article has to do with the classifier criterion, namely

""whether or not an article will receive at least the the median citation count of a collection of scholarly articles""

Without any further details, it isn't at all clear to which collection the authors refer nor to *when* the median citation count is evaluated.  

I assume the authors are referring to the collection of 122,653 randomly chosen articles mentioned earlier. If that is the case then the total number of citations for each article obtained from Google Scholar is going to depend on the date of publication.

Perhaps this collection should be segmented into publication periods (maybe Winter / Spring / Summer / Fall of a given year) and the median calculated for each segment of each year in which the articles were published.  Then the same would need to be done for the citation counts for each article to get a uniform measure of ""cited @ T"".

Another has to do with the concept of ""reachability"" in list of 12 features enumerated on p.2 :

""the number of unique hashtags used in tweets that are related to an article,
which indicates the reachability of an article in general terms""

The third feature listed, i.e. the

""number of different mentions used by users in their tweets""

is also unclear.  It could mean the total number of unique users which mention the article in their tweets or it could mean the number of unique tweets originated by all users, not including re-tweets by other users.  Or perhaps it means something else.

the legend, axes and title of Figure 2 is in very small font and hard to read.

A few grammatical suggestions

""Number of countries an article is mentioned from"" -> ""Number of countries from which an article is mentioned""

""Number of platforms an article is mentioned on"" -> ""Number of platforms on which an article is mentioned""

Conclusion

While I think the ability to establish early predictors based on Altmetrics data is interesting and worthy of study, I take issue with the authors' conclusion:

"".... predicting the scholarly impact of research at an early stage would save the scholarly community, research agencies, and policy makers crucial time and thereby accelerate overall research progress.""

Suppose the authors had used a dataset of features among the population of criminals in the criminal justice system that could predict the likelihood of recidivism. This would be worthy of note but not because:

""... predicting the [likelihood that a convict would re-offend] at an early stage would save the [criminal justice system], [penal] agencies, and [law] makers crucial time and thereby accelerate the overall rate at which [convicts are re-arrested].""","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/13/2018,23:15,no
455,119,197,Samxxxx Jayxxxxxx,2,"(OVERALL EVALUATION) This paper presents a study on using social media features to predict weather a research paper will receive at least median citation count. Authors build classification models using 12 non-traditional features (9 article-level metrics and 3 context features) instead of established scholarly metrics such as citations, impact factor immediacy index, h-index etc. The dataset includes random sample of articles extracted from social media mentions of various research articles from Altmetric.com and citation count scrapped from Google Scholar. 

Authors state that the random sample consists of 122,653 articles, what are the date ranges for these publications? What it entails to use the citation count available from Google Scholar for each date range? How the outcome going to change if we identify the average citation count for each date range and then use it to generate multiple classes? I can imagine there would be issues related to unbalance classes, but it would be great to have some comparison from this end. It is no clear how the LDA is used in the current study, what was the topic number, what features used in it, what similarity method, based on the document-topic or topic-terms or some other method? Also, it would be great if you can include some description to explain how the weight calculation is done.

It would greatly improve the readability of the paper if authors can provide some examples for their feature set such as the content features from a certain tweet and/or Almetric values for a certain article?","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/15/2018,14:43,no
456,119,150,Swxxxx Gotxxxxx,3,"(OVERALL EVALUATION) 1.	The paper is relevant to JCDL. 
2.	The paper talks about many problems - quality, relevance of articles in the intro para 2. But the main focus is only o study the impact of the article. The authors should explain this clearly and not bring in many components of tasks.
3.	The related work specifies many almetrics in prediction accuracy  the authors didnt use or mention why they didnt consider them in the study.
4.	Some papers are not relevant or its is not clearly explained. Eg [16]
5.	Why 7 is chosen as threshold? What is the rationale?
6.	The results are for all the features in the table 1 and table 2. It is good to show what happened with the combinations? This will tell us which features are undermining the results.
7.	The literature review also shows some articles that evaluates against a baseline [19]. What are your results against the baseline approaches. How significant is adding the social media mentions in the prediction accuracy?","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2/16/2018,0:03,no
458,120,244,Clixxxxx Lyxx,1,"(OVERALL EVALUATION) This is an interesting exploratory paper. The deluge of data towards the end is poorly synthesized and quite hard to follow, but the high level findings are very useful (for example section 3.2). 

The discussion in section 3 and elsewhere on data preparation and cleaning is very, very important and helps to make clear that this is a somewhat artificial exploration of proof of concept, but that actual operational implementation of this at scale is very very problematic. One particular point that needs a bit of brief exploration here is whether having papers in some other format that PDF (Computer Science widely uses TeX, and in other areas Word is very commonplace) would help with the text processing flow. 

I would speculate that this works best for very focused journals; I suspect it would be ineffective for something like Science or even CACM. Perhaps worth a comment?

There are some formatting problems with the paper. The version I downloaded didn't include any author information on the front page, for example. This clearly needs cleaned up. Also, there are frequent english language problems, mostly misuses of language with are very distracting but can be figured out. I don't want to spend a great deal of time enumerating all of these here, but you should have a native english speaker read this paper and do some light copy-editing. It would make a big difference. 

Finally I have a problem in that the paper totally ignores the implications of what the authors have learned, and what might happen if they succeed. For example, it seems very likely that the kinds of algorithms and criteria described here would strongly bias against really creative, ""out of the box"" papers that for example import methods or discoveries from one area into another area. Instead, they tend to promote conformity, and more articles of the sort that have been published, and hence, very safe, incremental work. This really has got to be addressed. I would happily trade off some of the very detailed experimental results for this discussion if space is an issue.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This  is a worthwile paper with some interesting results in it. However, I would particularly draw attention to my final paragraph of comments.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/14/2018,3:19,no
459,120,260,Roxxxx Hx,2,"(OVERALL EVALUATION) This work is an exploratory study using AI methods to automate reviewer support for journal peer review. The authors attempt to decipher the possible reasons for rejection based on real data from the publisher Elsevier specific to their computer science disciplinary journals. This approach integrates two aspects of rejection: i) a paper being rejected because of out of scope and ii) a paper rejected due to poor quality. Much was accomplished here in feature extraction from the data set being reviewed. Overall this is an interesting paper but its merits will be hard to understand as long as the associated data set can not be shared for verification. More of an applied concept that could benefit specific journal publishers rather than support those researching in the field. 

The lack of data reproducibility makes this seem out of scope for JCDL.

Could this model be leveraged for use with specific society proceedings reviews for better informed weeding prior to double blind peer review?

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This is an interesting experiment that does offer some insight but by utilizing closed data sources it slants the research towards a more applied question that is specific to the data that was utilized. Thus in my opinion is not a good candidate for the scope of JCDL.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/16/2018,19:43,no
461,120,61,Vitxxxx Casxxxx,3,"(OVERALL EVALUATION) The paper presents an interesting analysis of the data (provided by Elsevier) about submission of papers for publication (in several different journals). The data includes percentages and reasons why a paper was rejected right away, or was retained and sent to reviewers for evaluation. The authors have identified and defined thirty criteria according to which a paper would be rejected or forwarded to reviewers. The analysis of the data appears sound and well thought, the amount of data on which the identification of the relevant criteria has been based is big enough, so this paper can definitely be of interest in the landscape of the peer review system. 
The negative part of the paper (which makes its acceptance doubtful) is the use of the thirty identified criteria to implement a supervised binary classifier that will ""decide"" whether a submitted paper should be accepted (sent to reviewers) or rejected. The main reason for being negative is that some of the criteria are reflecting a human ?€?bias?€? (e.g. the institution of the first author, a name with a big impact factor among the authors) and are not related to the actual content and value of the submitted paper. It would be unfair and unethical to consolidate this bias in a ?€?machine?€?. 
While a ""tool"" like this could be useful for authors as an aid to improve the quality of their papers, I see many dangers in the indiscriminate use of the tool (as presented in the paper) on the part of journal editors. Even worst, I would be afraid of the use of such a tool based on ""deep learning"" (as suggested in the conclusions) as at that point it would be impossible to understand why a paper would be accepted or rejected, and the tool would behave following the (bad) human bias. 
Finally, the paper is very well organized and the topics are well explained, but it might benefit from a review of the style and grammar of English.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) After the long discussion with Andi he finally convinced me that that danger of presenting an AI approach based on biased criteria is bigger than the benefit of alerting people about the possibility of having biased classifiers (in a peer review system).","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/24/2018,18:21,no
462,120,312,Andxxxx Raxxx,4,"(OVERALL EVALUATION) The paper reports on an approach to build a machine learning model to act as a pre-processing step in easing editor?€?s decision whether to forward a paper on reviewers or reject it without review process (desk reject). The paper is based on a very detailed study of accepted and rejected papers in several journals provided by Elsevier. Massive work has been invested in preprocessing the data, leading to a solid analysis of the aspects influencing desk rejects.

If the study had stopped at this analysis, I would have strongly favored an acceptance. However, the authors progressed by using the knowledge gained by observing the decision process (and its artifacts) as they are in place now to devise a system to assist future decision, showing a surprising ignorance of ethical issues associated with their design decisions. The most striking example of this is the fact that the authors themselves acknowledge that many desk reject decisions are strongly influenced by an author?€?s profile or an institutions reputation (quoting a single exception to the contrary as evidence that it is not all bad). As an observation, this is a revealing insight. The fact that one statistics journal was found to consist almost exclusively of papers of a closed circle of repeat authors should strike an alarming signal. Again, this is an extremely valuable finding of the paper definitely worse brining to public attention.

However, using these features subsequently in automated decision making reveals a complete lack of understanding of ethical aspects in automated decision making! The fact that an attribute is indicative of a decision does not automatically mean that this attribute should be (or, in the present case: may be) used for automated decision making. To make the situation a bit more obvious: there have been studies showing that female authors find it harder to get accepted than male authors in studies where the same paper was submitted under different author names (unfortunately I do not have the reference at hand, but even if it were a fictional example, it helps shed light on the situation) The fact that gender is (or if it would be) a strong predictor for rejecting submission obviously does not mean that any algorithm should be using this for any kind of decision making! Yet, the authors propose just this, further enforcing the fallacies of human-based decision making. Such an approach is rather shocking. I strongly recommend the authors to consult the ACM Joint statement on Algorithmic Accountability and Transparency as an initial starting point to the complexities of ethical implications of bias in machine learning and the wealth of work discussing attribute usage for decision making, both explicitly (such as ethnicity) as well as implicitly (such as ethnicity being encoded in (i.e. correlating highly with) the zip-code of the home address). 

A methodological flaw might also be present, although it is not clear from the paper whether this aspect was considered: the paper does not report in detail whether or how time aspects were considered: the study was done ex-post. Thus, citation numbers will have changed between the time a paper was submitted and accepted/rejected and the citation numbers to the references as they are now. How did you ensure you got correct citation numbers as they were at a certain point (more precisely: the submission point) in the past? It may not have a huge impact, but it is something that should be considered to avoid overestimating a machine learning algorithms performance ?€? which frequently happens when analyzing time series data in a non-time series manner.

Some minor confusion is created by conflicting numbers in the paper: the introduction states that 5500 papers have been analyzed, Fig, 1 provides numbers on 500 papers, and Sec 2 states that 5000 papers have been analyzed.

The arrow for the mapping of conference names should either point in the opposite direction (from full name to acronym) or the sentence above should be inverted (mapping acronyms to the full names)

Figure 2: why did you decide to use different journals for depicting certain tendencies? Are these tendencies present across all journals, or are these the most striking ones? A more solid representation would be provided by presenting the averages plus variances across these lines.
Please provide labels for all equations in the paper, not just for selected ones.
The unlabeled equation computing the KW_score is using the symbol for a cross product where it actually is a scalar product.

A minor issue that might raise a controversy: did Elsevier seek the consent of authors of rejected paper to make both their rejected papers, their identities as well as the reviews available to the analysts? (Although I admit I consider this really only a minor issue).

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) A massive violation of ethical aspects in designing machine learning algorithms, and a potential methodological flaw in analyzing time series data, thus likely overestimating performance.
The machine learning part needs to be completely re-done prior to being presented avoiding the unethical use of attributes in proposed decision making. alternatively, the classifier part could be removed, publishing only the findings of the study.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2/17/2018,10:35,no
463,121,109,Nixxxx Fexx,1,"(OVERALL EVALUATION) The paper studies the variation in document-to-document similarity in 10 US news sites, where similarity is either cosine similarity or a weighted Jaccard similarity, and shows that when some relevant events happen -- namely, presidential elections, veteran day, and thanksgiving -- the similarity between sites increases.

The research contribution of the paper seems very limited. The selected events are very broad and coarse-grained, so very easy (and not very interesting) to target; moreover, some bias may have introduced since the way in which HTML documents are parsed has been specifically adapted for the case of presidential elections. So, what would happen with somehow ""smaller"" events?

Moreover, showing an increase in similarity, does not provide any hints on how this could be used to develop algorithms to detected specific events, which would have been a potentially original contribution of the paper.

Finally, the paper seems to be written in a hurry and the presentation quality is not very satisfactory.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) It is 9 pages long - possibly an indicator of being written in a rush.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,1/25/2018,17:21,no
464,121,316,Brxxxx Rexx,2,"(OVERALL EVALUATION) I recommend this paper be accepted for JCDL. It applies traditional information retrieval measures of similarity to a new and novel context (analysis of news websites and how they present stories). The topic is current and likely to be of high interest to conference attendees. 

My only critique of this paper is that the authors do not do a sufficient job of explaining how their findings can be applied to a larger context. If you obtained results X from experiment Y utilizing methodology Z, what does that tell you about the larger world A? Does this research say (or is trying to say) anything novel about  American news organizations, or is the novelty simply in the methodology employed? I am not sure the paper answers this particular question, and this gives it more the feeling of a short paper or work in progress than an actual, finished research project.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/16/2018,21:38,no
465,121,46,Corxxxx Brexxxxxx,3,"(OVERALL EVALUATION) The paper examines how significant events impact the similarity of news stories across ten U.S. news websites. 

The contribution of the paper is twofold. First, the paper introduces a parser for extracting headline and link information from ten popular U.S. new websites. The parser is unique in that it is capable of extracting what the authors term 'hero stories', i.e. the most prominently featured new stories on the landing page of a news website. 

Second, the paper presents an evaluation using archived web pages (mementos) for the ten chosen U.S. news sites throughout the month of November 2016 (U.S. election). The authors demonstrate that the similarity of news topics indeed converges across the 10 examined news sites during a significant event, i.e. the 2016 presidential elections. As a side discovery, the authors find that half of the news sites alter their document representation, e.g. change naming conventions of CSS selectors, during significant national events like the U.S. presidential election. 

Both contributions are of high relevance to the JCDL audience. 

Evaluation: News similarity was measured using weighted Jaccard similarity coefficient and cosine similarity, two widely accepted similarity measures. The authors find that for both measures, similarity scores for the top stories (k = 1, 3 and 10) for the 10 examined sites are higher shortly before the election, highest on the day of the election, and then quickly decrease after the election as newspapers focus on more diverse topics. This finding itself is rather intuitive. However, the presented work is novel in that it is the first empirical demonstration of this phenomenon. Additionally, the authors show that the weighted Jaccard-Overlap method generally has a higher similarity score than Cosine similarity, and this difference becomes especially noticeable on the day of the event. The evaluation is sound and highly reproducible given that the authors have made their dataset publicly available. A minor weakness is that the stories on the chosen news websites could not always be extracted at exactly the same time each day for all 10 news websites, since the frequency of memento creation varied widely depending on the website. E.g. For the month of November 2016, only 470 mementos existed for the Chicago Tribune, while 3,560 mementos existed for The Washington Post.

Impact: I find this work especially interesting for members of the JCDL community. The openly available 'top news selector' parser will be valuable to other researchers eager to explore their own questions. One such research question that comes to my mind is examining how media bias could be identified using novel measures, such as the display duration of 'hero stories', or the intra-site similarity of 'hero stories' over longer periods of time for certain news website. E.g. higher similarity of hero stories over time could point to a news site trying to push an agenda, or at least featuring less diverse or balanced news coverage.

Style & Clarity: Overall, the article is well structured, and the methodology is easy to follow. A minor point is that the paper should specify earlier, i.e. on the 1st page, how exactly 'top' new stories are defined/ are being determined in the analysis. This will avoid ambiguity, since other common criteria to determine 'top news stories' include most read, most shared, duration displayed, etc. It would also be nice to know how the parser deals with sites where the content is not labeled with obvious CSS selectors, yet the choice of 'hero' stories is not straightforward: E.g. if 2 articles are of equal size, yet one uses a larger font size while the other displays a large image, etc. Deciding on the order of 'hero' articles in such a case would be arguably quite subjective.

Ambiguities / style issues / typos:
- proof for: missing punctuation marks / strange occurrence of question marks
- p. 1: ambiguous: topics not consistent among other sites --> different topics?
- p. 1.: unclear: similarity 'in news websites' --> Maybe: 'similarity among the topics reported on by' news websites
- p. 1: unclear: 'peak similarity values to peak'
- p. 1: ..for the top-k (k = 1,3,10) news stories --> 'top stories' how defined? introduce method earlier
- similarity measurements --> similarity measures
- p. 2: evaluate the similarity for a given day --> evaluate the similarity among the topics being reported on by news sites
- p. 2: unclear why is the entire second paragraph in 'Related work' is in parenthesis? 
...","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/17/2018,0:00,no
466,121,8,Ghxxxx Abxxxx,4,"(OVERALL EVALUATION) The paper describes an approach to extract news stories from 10 news web sites and use two algorithms to compute similarity measures. The paper shows that, within the month they looked at the data, the news stories similarity is dependent on the temporal distance from the major events during that month. The weighted Jaccard-Overlap method generally has a higher similarity score than Cosine similarity. 

The contribution of the paper is minimal since there is nothing new in the approach. Applying the tools to the news domain is new, however, the findings are not novel. We expect that when we get closer to a major event or holiday the news will be more related to that event which makes the similarity higher. 
It is useful to know that the Jaccard-overlap has better performance than cosine similarity, but again this is not a novel result.

The contribution could have been better if this approach was used to make interesting inferences from the similarity changes. For example comparing the contents of a news website to a baseline similarity and trying to predict future events based on the similarity distance.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/24/2018,8:01,no
467,122,428,Maxx ????xxx,1,"(OVERALL EVALUATION) This paper presents a meta-study of 13 digital reading reports published recently by different Chinese agencies. Since China is such a big market, the results of such a study would be very interesting to current and potential providers of e-resources.
Unfortunately the authors encountered a huge problem: the studies analysed differ substantially in methodology, sample, questions and approach in general. It was therefore vry difficult to generalize the results, which is evident in a very brief discussion and conclusion, which also ends abruptly.
The paper is also not in the required format.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/6/2018,10:27,no
468,122,10,Maxx Agxx,2,"(OVERALL EVALUATION) 13 reports on China?€?s digital reading published within the year 2016 are collected and most of them are briefly and only partially introduced, but the present paper fails to show any originality or new findings about the digital reading, due to the lack of analytical point of view. Three reports (nos. 2, 6, and 8 in table 1) are mentioned only in table 1 without any further description of the results. 

Tables aren?€?t well organized. ?€?3Conclusion and discussion section?€? doesn?€?t seem to have a clear connection with the previous sections of the article.

Secondary analysis of reports of China?€?s digital reading for a longer period rather than only a single year from fresh perspectives might lead to useful observation of trends in China.","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2/13/2018,13:57,no
469,122,333,Yx Sxx,3,"(OVERALL EVALUATION) This paper tries to analyze the trend of digital reading in China through a collection of digital reading reports generated and released by different agencies around 2016. It brings together these existing reports that greatly vary in their nature, scope, methods, and goals, and then sometimes uses findings from only one source report to make generalized claims in term of the trend in China. 
 
For example, under section ""2.2 Reader"" the authors claimed they ""can summarize the characteristics of readers through the analysis of the four reports"" (Page 5), but in fact several of their ""summaries"" have come only from one source report (e.g. copyright awareness), as is evident from Table 2. 

Thus, more efforts are needed to prove the validity and reliability of the claims and to improve the research rigor and methodological soundness of this work.

Overall, this paper is more like an inventory of a few existing reports, rather than a research. I suggest the authors take it from here and further design and develop a complete and rigorous research to inform and advance the knowledge of this field. 

Finally, there are a lot of grammatical errors and language problems throughout the text.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/13/2018,19:35,no
471,123,247,Gaxx Marxxxxxxx,1,"(OVERALL EVALUATION) This paper presents a collaborative filtering strategy that applies a neural network model to discovering relationships in biomedical literature.  The main innovation in this paper is adding an ?€?uncertainty constraint?€? to a multi-layer NN technique.  The idea is that this addition provides a computed assessment of the value of discovered associations to predict future associations.  The addition computes the weighted sum of errors due to classification and regression.  The two alternative NN techniques are run on Medline abstracts with typical cleaning/filtering applied.  Three metrics are used (ranking, classification, and F1) to compare the two NN techniques to rankings with 6 other systems, classification with one other system (factorization machine), and F1 scores with factorization machine runs.  The results demonstrate that the NN techniques perform slightly better than a factorization machine method and the other techniques in the case of ranking.  The NN technique with uncertainty constraint applied does a little better that the NN in all cases.  One important advantage of the NN approach over the factorization machine approach is that there is less computational cost for the NN models.  Additionally is scales well for very large collections.
A few questions/suggestions for the authors include:  How domain dependent is the uncertainty model (would training data in a financial or other space perform similarly)? 
You omitted terms that map to more than one UMLS entry.  This seems sensible but for discovering new relationships that are latent in the literature, these terms might actually act as latent ?€?bridges?€? rather than stop words.  It seems like this is what Swanson originally did to find paths in mutually disjoint literatures.
In section 4.5 you note that uncertainty loss plays a small role and should not be overused.  Is this an overfitting problem? 
In the conclusions, your future work might consider weighting figure/table captions more than text.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/21/2018,17:12,no
473,123,388,Kaxxx Verxxxx,2,"(OVERALL EVALUATION) This is a nice study on the topic of literature-based discovery, exploring both the contribution of ""global"" models for establishing association between entities, extensions of these using neural representations, and the incorporation of a model of uncertainty. The authors have introduced an approach using neural models, and with the contribution of incorporation of the uncertainty model. The presented results are compared with relatively weak baselines and hence do not fully support the broad claims.

I am not an expert on existing methods for this, although the authors seem to have done a reasonably good job of identifying and contrasting prior approaches. I note this relatively new review paper which is not cited (https://www.ncbi.nlm.nih.gov/pubmed/28838802) but reviews various methods for evaluating (the authors use the ""time-slicing"" method, predicting things one year out from a training set; the authors should relate the framing of the task in this way to previous methods) as well as detailing prior approaches; this reference should be taken into consideration. In particular, the evaluation metrics could be compared here, although I think the authors have done a good job of justifying their choices.

The authors should probably reference MetaMap as an alternative term matching approach; also see Funk et al (https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-15-59) for evidence that exact matching might not be so good.

The authors should indicate how the basic performance of their uncertainty model compares to the state-of-the-art on the CoNLL-2010 data.

There are some minor grammatical errors in the paper, some examples:
""methods considers"" -> ""methods consider""
""if a pair .. appear"" -> "".. appears""
""few if not none"" -> ""few or even no""
""longest term to shorted term"" -> ""longest term to shortest term""
""Same as"" -> ""As for""

Some other minor issues:
- ""abstract full-text data"" -> ""abstract text data""
- I wasn't sure what ""arbitrary-order"" referred to specifically at the bottom of p 1 (right col).
- e-2 is stated to be high certainty, with an uncertainty value of 0.999; e-1 lower certainty with an uncertainty value of 0.0006: do you mean ""certainty"" value rather than ""uncertainty""? What is the range of (un)certainty values?
- there are a few citations missing (""kim citation"", ""bio citation"")","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/20/2018,21:42,no
475,123,358,Kazxxxxx Sugxxxx,3,"(OVERALL EVALUATION) Existing works in literature based discovery use only local-information and do not take the effect of ""uncertainty"" into account.  To solve these problems, the authors propose neural collaborative 
filtering to discover biomedical knowledge. 

But this paper has a lot of problems. The details are as follows:


(1) Novelty/Originality
- It seems that the authors just apply neural collaborative filtering [13] and convolutional neural network for sentence classification [18]. If not, the authors need to clearly claim their novelty. 


(2) Methodology
- It is nice to conduct experiments on publicly available datasets. But the authors need to optimize the number of layers in their ""MLP-CF"" and ""MLP-CF-uncertain"" approach. While the authors have already shown experimental results on impact of \alpha, they also need to show the results on impact of the number of layers. 

In addition, as pointed in ""(1) Novelty/Originality"", the authors should develop original approach. 


(3) Assessment/Evaluation/Comparison
- It is nice that the authors compare their approach with several baselines. But they are relatively 
weak ones. The authors employ neural-based collaborative filtering approach, so they need to 
compare their approach with some variants of neural-based collaborative filtering. 

- In Sec 4.4, the authors claim that 
""we can see that our models perform better than Factorization Machine with considerable margin."" ?€€
regarding the F-1 scores in Table 3. 

But the authors need to perform statistical test to support the imporvement with considerable margin. 

- It is better to perform microscopic analysis as well as discussion on overall scores. 

- In abstract, the authors describe that 
""our MLP-CF model performs comparably with strong baseline Factorization Machine with much faster performance.""

But the authors do not show experimental results on time complexity. 


(4) Style/Quality of Writing
The reviewer would like the authors to correct or improve the followings. 
The reviewer only show important ones only. Please carefully check your paper. 

[Sec 1]
""uncertinty"" effec => ""uncertinty"" effec*t*


[Sec 2.1]
- subsection title, ""Knowledge discovery"" => ""Knowledge Discovery""
(It is necessary to be consistent with other subsections.)

- the local-information based LBD may perform not as well, ...
=> the local-information based LBD may *not* perform not *so* well, ...


[""Multi-perceptron layers"" Sec 3.1]
If the authors use ""l_{i}"" to denote each fully-connected layer instead of just ""i"", 
Equations (3), (4), and (5) would be more intuitive.   


[""Ranking AUC"" at Sec 4.4]
- ... is show in Table 1. => ... is show*n* in Table 1.

- Second, Jaccard's coefficients and BITOLA performs ...
=> Second, Jaccard's coefficients and BITOLA *perform* ...


[""Classification AUC"" at Sec 4.4]
- compared to that in Table 1. => compared to *those* in Table 1.


[Others]
- In Tables 1, 2, and 3, the authors should use ""MLP-CF"" and ""MLP-CF-uncertain"" 
in the same phrase as the body of the paper not ""MLP-CF"" and ""MLP_Uncertain"", respectively. 


(5) Replicability
- As the reviewer pointed out in ""(1) Novelty/Originality"", the authors' approach largely 
relies on exiting approaches. In addition, the authors conduct experiments in publicly 
available datasets. From these point of view, this work has high replicability. 


(6) References
- In ""References"", the authors often skip venues ([6]), page numbers ([7], [13], [14], [15], [16]). 
While the authors cite three arXiv papers, it is highly possible that they have been published in 
official international conferences. So the authors need to cite the official published papers. 
Especially, [18] has been published in EMNLP2014.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,3/5/2018,18:35,no
476,123,57,P??xxxx Caxxx,4,"(OVERALL EVALUATION) This paper presents a neural network model to infer associations between words, in order to discover new knowledge from scientific literature.

The paper is generally well written and is easy to read, although it could use a minor review. The models proposed seem to be effective, even though they are quite simple compared to what is commonly seen today in NLP. The methodology used seems to be sound. Results, however are not very convincing. The proposed model is not much better than FMs and, although the authors argue it is faster, there are no experiments to show this.","Overall evaluation: 0
Reviewer's confidence: 2
Recommend for best paper: no",0,,,,,2/16/2018,15:19,no
477,124,56,Klexxxx B??xx,1,"(OVERALL EVALUATION) This paper does not level up to the standards of this conference. The innovation is unclear. The quality of writing (English) is not sufficient; important points also remain unclear because of poor English. There is no discussion of related work even though it would be necessary. (I understand that this is a short paper, but this is not OK.) Finally, there is no evaluation. (Please define your expectations and your quality measures upfront etc.)

More specifically:
- ""in Academic Contexts"": What is specific to academia? If there is something, the article should be more specific explaining this.
- ""it could be approximately considered ... in academic contexts"" -- I do not understand this.
- ""formulating precise query"" -- This needs to be more rigid. What exactly does 'precise' mean here? How do you intend to quantify it.
- ""so as to expand query"" -- Query expansion is not new, what is your innovation?
- ""we perform semantic space projection ... through word embeddings."" -- I do not understand this.
- ""At last we remove ... the interpretation of domain experts"" -- I  totally do not understand: Are you proposing to have a human in the loop who does the work? If so, why is this innovative?



English (just some examples from the first page):
- the number of scientific literature
- by which can obtain the relevant results
- other irrelevant literature, which lower the precision ratio
- and are lack of an understanding of the whole picture
- word embeddings generated using neural network
- for formulating precise query
- identify the literature which semantic features vary significantly","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,1/29/2018,8:05,no
478,124,276,Fedxxxxx Naxx,2,"(OVERALL EVALUATION) The paper describes the adoption of word embeddings for supporting query expansion in academic context. This work does not provide to the reader essential information, for instance:

1) on which dataset the word embeddings have been created? In the proceedings of  WOSP 2016 and 2017 (a workshop on mining scientific publications hosted at JCDL) a few papers have used Core (http://core.kmi.open.ac.uk/) to generate ""academic"" word embeddings.

2) how was the quality of the word embeddings evaluated? There are many datasets for measuring for example word relatedness and checking whether the produced embeddings are able to capture it.

In addition to this, there is no comparison with other approaches for query expansion, e.g. through the use of a relevant models or topic models.

Finally, the authors do not present any quantitative evaluation concerning the usefulness of word embeddings for the task. 

I would recommend the authors to focus on a quantitative evaluation of the described use case of word embeddings in the scientific domain, in comparison with other baselines on a dataset which consists of a sufficient numbers of query (i.e., more than the single query ""deep learning"" presented in this work).","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,1/31/2018,15:30,no
479,124,89,Gioxxxx Maxxx,3,"(OVERALL EVALUATION) The paper presents an a query reformulation approach based on word embeddings.
The paper focuses on a very interesting problem but fails in the experimental part. In particular, when in Section 4 the description of the dataset, the query and the evaluation is very vague.

- when you start with the query ""deep learning"" and run the Word2Vec algorithm, what is the dataset? How many documents? You mention SCI-E but it is not clear how you used it.
- ""the corresponding literature of these terms is analysed by domain experts"" How many experts analysed the literature? How many documents? 
- ""The total number of retrieval results is increased by 98%"", who decided what portion of articles were relevant for the initial query and the expanded query?
- have you tried only *one* query? How can you generalise the result of your method with only one test?
Moreover, the query you use is very specific and you will rarely find a non-relevant document. I would suggest to 1) increase the number of query, 2) generate query that are more difficult to see whether the embeddings can truly recover the initial polysemy/synonymy problem.

Please find hereby some other suggestion to improve the paper.

- Section 2 is too short and should be included in the experimental section.

- ""The higher the degree [...] the result is better"" -> The better the result.

- Section 3.2, ""we could solve the problem"" -> ""we can mitigate/solve/improve the results ...""

- Section 4, ""Deep learning technology ..."" I would suggest to write this sentence in the introduction, not in the experimental results section.
- ""we take the literature retrieval on deep learning..."", check English.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/14/2018,16:05,no
480,125,379,Giaxxxx Tsaxxxx,1,"(OVERALL EVALUATION) The current short paper discusses the use of the multiple diagram navigation method to large collections of digital documents. MDN is considered as additional structure layer and the authors of this contribution are attempting to exploit this structure in order to improve retrieval rates in digital collections. 

The idea is interesting, but, unless I have overlooked it, I could not find the way of constructing such a diagram. How is its structure produced and what is its relevance/similarity to the one of the collection? This is crucial information that would help understanding of the method. I can understand the option of applying IR measures (see energy spreading), but as a graph based construction, wouldn't be more convenient and effective to explore network based methods and metrics? In a sense these look to me as multilayer networks (or interconnected networks). Several chapters in http://www.springer.com/gp/book/9783319239453 might look useful, either for this work, or for future ones. I have also a concern regarding the number of lists and categories used in every domain that the authors used. I believe that that the high number of items in the ""languages"" category affected the results. 

Technically, Figure 1 should be closer to the first point of reference. All figures and tables are contain dense information and lowers readability (also captions too long; maybe in the text). The literature, given the lenght of the paper, is adequate and well placed.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,1/30/2018,19:40,no
481,125,383,Douxxxx Tuxxxx,2,"(OVERALL EVALUATION) This is interesting work on a topic relevant to JCDL, utilising diagrams for exploratory search.  Spreading activation is used as a form of query or topic expansion.  The work is at a fairly early stage and the paper reports on a pilot study of their algorithm on Wikipedia pages and a limited empirical study of 12 diagrams. This is fine for a short paper.

The paper has 4 pages with 9 references, a little under-referenced though appropriate enoiugh for a short paper. The work might be grounded more in DL literature and representations perhaps. I was reminded generally of Shneiderman's discussion of information visualization and dynamic query methods - some of this literature might be useful as a source of ideas. I wasn't totally convinced by the rationale for the diagram model with its Items and Attributes (the distinction wasn't quite clear) - is this based on some particular work? Are any of the common DL metadata element sets relevant?

In any full paper, I would like to see details of the actual spreading activation algorithm. On p2, it was unclear what was meant by 'latest'. 

The evaluation was perhaps a little detailed on one specific issue for a short paper? Is the issue of reciprocal vs non-reciprocal particularly critical in general? 

I would have liked to see a little more discussion of the generalisability of the approach. Does the format specific nature of diagrams restrict generalisation?

I would like (in a full paper) to see more discussion of the limitations arising from the manual treatment presumably 
necessary to annotate specific elements of diagrams. Or is there scope for automatic approaches to certain types of diagrams?

There is a significant amount of work discussed for a short paper - it could make a useful contribution to JCDL.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I thought the approach was a little ad-hoc but would have no objection to accepting the paper if other reviews are stronger accepts.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/2/2018,17:27,no
483,125,47,Corxxxx Brexxxxxx,3,"(OVERALL EVALUATION) The paper builds upon the concept of multiple diagram navigation (MDN), previously introduced by the first author. Broadly speaking, the aim of MDN is to provide users of digital libraries with a richer DL exploration experience by seamlessly supporting the navigation between diagram elements and content (i.e. the documents contained in the collection). The authors note that a shortcoming of their previously proposed 'diagram-to-content' queries is that only a very small fraction of the full document collection is accessible from diagrams. 

This limitation is addressed in this paper by extending diagram-to-content queries to also reach related documents not directly connected to the diagrams. The authors use the Wikipedia hyperlink graph and internal diagram structures to realize such a 'diagram-influenced ranking' of Wikipedia pages. In an evaluation, various settings for their ranking algorithm are tested using 12 diagrams from 6 diverse domains. The paper determines a set of optimal parameters for their introduced algorithm and concludes that the diagram used in a query has a strong influence on rankings. 

One weakness of the evaluation is that it had not been made sufficiently clear according to which criteria the similarity rating (Rtng, 1-3) was determined. Since labeling was performed by a graduate student, the criteria for similarity should at least be made public to allow for some degree of reproducibility.

Overall, the paper is well-written and the evaluation methodology has been clearly presented, aside from the weakness I pointed out above. 

The scope of the contribution is generally suitable for the short paper format, yet, it is important that the authors also include a discussion on the limitations of their evaluation as well as the transferability of their findings. Especially: How will other researchers and developers benefit from the proposed approach? How could it further be improved? These points have not been addressed. Despite the space restriction, it would also be interesting to hear what the authors propose as future work.
 
In summary, the paper's topic and findings are of interest to the JCDL community, which is why I recommend it for publication with modifications.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/17/2018,16:47,no
484,127,24,Roxxxx Alxx,1,"(OVERALL EVALUATION) Eye-tracking data has been used to predict user interest for complex information tasks such as interaction with digital libraries.   This paper proposes to improve the utility of eye tracking data for those complex tasks.

Specifically, the authors develop an ?€?oculomotor plant features?€? model.  This is based on a detailed analysis of the way that different eye muscles affect saccades (quick eye transitions between fixations).  They report data from previous studies on following a ?€?jumping point?€? on the screen to validate the model.  They then propose that the correction should be useful for measuring user interest in information tasks.

I very much like the idea of trying to refine saccade data, however, it?€?s not clear how useful this correction will actually be for high-level tasks.  To be considered for this conference, we would need an evaluation of the technique using those tasks.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/10/2018,12:05,no
486,127,95,Mixxxx Dobrexxxxxxxxxxx,2,"(OVERALL EVALUATION) While the paper could be of interest to the communities working on human-computer interaction with its contributions to eye movement analysis, it does not make a strong connection to the use of eye tracking within the digital library domain. There were earlier studies involving eye tracking within the evaluation of digital library interfaces (the earliest one which I am aware of is https://link.springer.com/chapter/10.1007/978-3-642-15464-5_69). 

Is there anything specific in the digital library interfaces which requires adjustments of eye tracking methodology and techniques? How exactly an improved eye tracking method is of benefit to the digital library community? What visualisations of eye tracking data are most useful within the evaluations of digital library interfaces? All these questions would be of great interest to the digital library community but may be when the authors continue with the experiments they could select suitable examples and present in some of the future digital libraries conferences.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I revised the scoring. I am all to promote more eye tracking. I hope that if accepted the paper will be revised to make a better connection to the application of eye tracking in DLs.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/21/2018,17:20,no
487,127,403,Dxx W,3,"(OVERALL EVALUATION) This study proposed a foundation using the eye movements for user interest modeling. The topic of this study is of relevance to the conference. Modeling the user?€?s interest is valuable in the information retrieval. The authors used the eye movements and their OPF values to identify the relevance of resource content. 
  The author gave a detailed introduction for the reason of choosing of oculomotor plant features (OPF), as well as the content of the specific features. 
  But there are still some questions that should answer. In section 4.1, the authors introduced the selection of participants. Why did they choose 12 subjects randomly from dataset out of 22 participants? Why did not choose all dataset in this study? Whether the scale of samples will affect the modeling results? 
  In section 4.1, the sentence ?€?We utilized velocity-based model for eye movement classification with thresholds 700/s (based on previous studies).?€? There should be reference.
  The authors mentioned that the dataset in this study was also used in their previous researches, are there some relationships between the current findings and them?
  The authors expressed their dataset was not base on the same information task, how did they avoid the influence from other factors on participants?€? behaviors?
  There are still some limits about this study. The sample of participants in this study was small, and the implication of authors?€? findings should be more clear.
  Finally, this paper is interesting and related to the conference, and we encourage this study to be accepted by the conference.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/16/2018,14:35,no
489,128,414,Wexxxx X,1,"(OVERALL EVALUATION) This paper presents preliminary work on identifying author confidence in biomedical articles. The core idea is to predict a confidence score by natural language processing and authors previous publication. The paper includes a proposed architecture and preliminary results from the sentiment analysis; average word per sentence and frequencies of medical terms are used for 10,000 articles from open archives initiative. While the topic is interesting and challenging,  the submission is not ready for publication for the following reasons: 

-  Only results from a subset of proposed features are presented in the paper. With three presented features, it is not clear how they will be integrated and used together to predict confidence. 

- The paper lacks technical details. For example, it is not clear how the sentiment analysis is carried out, such as language model used, and the scale of scoring. It is also not clear how to convert sentiment analysis score into confidence. If using a general model, a strong negative of a certain fact may not mean lacking confidence. The context of the text must be considered. 

- A major weakness of the paper is lack of evaluation. It is not clear how the presented results can be evaluated for their relevance and effectiveness with regard to author confidence. The presented results, therefore, don't carry many contributions beyond a basic sentiment analysis and word frequency analysis.  

- The paper also includes a number of presentation issues. For example, Figure 2 seems missing from the article. it is not clear if it is completely an omission, or just a mislabeling of figures. There are also incomplete sentences.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,3/8/2018,20:24,no
490,128,84,Daxxxx Haxxx,2,"(OVERALL EVALUATION) This paper regards the identification of authors confidence. It is an interesting topic and fits very well with the conference. The paper is well written and easy to follow.

The authors claimed that they could automatically identify the level of confidence by analysing some article features: (a) the sentiment polarity of the text; (b) the average number of words per sentence; (c) the medical frequency terms. In order to show that these features are related to the author's confidence, the authors show the distribution of each feature. 

However, they did not show how the difference of each feature is related to the confidence of the author. For example, if some article has negative polarity, this may not be related to the author's confidence. 

In order to show that they can automatically identify the author's confidence, a suggestion is to label a sample of authors according to its confidence. After that, they are able to present the distribution of each feature and relating them to the author's confidence. Using correlation metrics should be interesting for this analysis.

The study is promising, however, the authors should improve their analysis in order to show the feasibility of the proposed features in this task.","Overall evaluation: -1
Reviewer's confidence: 5
Recommend for best paper: no",-1,,,,,2/16/2018,15:19,no
491,128,422,Bxx Y,3,"(OVERALL EVALUATION) This paper is poorly written and actually unfinished. It is not clear how ""confidence"" is measured, and what linguistic patterns indicate high or low confidence.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2/17/2018,4:13,no
493,130,244,Clixxxxx Lyxx,1,"(OVERALL EVALUATION) This is an interesting paper, though the detail in the middle sections is overwhelming and I could not follow it (I am generally knowledgable on this topic but not a deep expert or carrying out active research). Most of the writing is clear, and it does a good job of framing the overall state of the art and the issues. THere are a very small number of little errors (page 8 refers to a ""thesis"" instead of a paper, for example; there's a paragraph mid-page on the right column of page 6 that seems like it got a bit mangled somehow.)

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The questions around this paper are strategic rather than about the details of the content. I am not sure that JCDL is the right place for a very detailed paper on this, though the subject is very important. There are more specialized venues. 

I wonder whether most JCDL attendees will be knowledgable or willing to wade through the very detailed material in here. There's some argument that JCDL and the authors would be better served by a short, high-level paper for the broad JCDL audience and then submission of this full paper to a more specialized venue. I would invite other reviewers to share their thoughts on this as I am not expert in this area by any means, and certainly not at this level of detail.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/14/2018,3:27,no
494,130,77,Timxxxx Cxx,2,"(OVERALL EVALUATION) This is a well-written paper about important and interesting work wholly in scope for JCDL. Submission makes a well-argued case for the benefits of using presentation and content markup (in combination) to represent mathematical equations in DLs. Concrete examples are cited and well explained to bolster these arguments, though the paper stops short of presenting an analysis of full-fledged use cases drawn from systematic interviews of research mathematicians. The authors argue effectively that MathML provides the necessary affordances, which I think is a reasonable argument, though I was looking for (and did not find) an acknowledgement of the limitations of MathML for some complex mathematics. (In contrast the restricted set of functions and the simpler grammars of current CAS formats was mentioned.) While the uptake of content (semantic) MathML by authors and publishers has been underwhelming, this does not have to preclude its use by DLs if adequate conversion tools can be developed. For this reason the test collection and well-done quantitative analysis of conversion tools described in this paper are welcome. The author's innovative conversion approach that makes use of the text surrounding an equation also resonates with suggestions offered of late by research mathematicians working with librarians on strategies for making mathematics papers discoverable at a more granular level (e.g., at the level of equations, lemmas, sequences, etc.). The sources used for constructing the test collection of equations were well chosen and the GUI for creating the gold standard markup of these equations looks to be appropriate for purpose. The authors have extended the state of the art in this area of research. (The size of the test collection will need to be increased over time, of course.) The references cited (a little heavy on self-citation) were generally good and on point, though given the recent resurgence of interest in this topic among research mathematicians, there may be a few, newer, additional sources. (Also, the [Wat+14] appears to be missing editor names, i.e., S. M. Watt, et al.) While this paper is to be lauded for breaking new ground in this kind of research, the analyses presented are tightly tied to MathML and the conclusion/future work discussion fell a little flat. It would (I think) have been appropriate to acknowledge other, ongoing efforts of the research mathematics community to look beyond MathML, such as were on display at the February 2016 Semantic Representation of Mathematical Knowledge Workshop (http://www.fields.utoronto.ca/programs/scientific/15-16/semantic/) and in other endeavors associated with the nascent International Mathematical Knowledge Trust (http://imkt.org/). Overall, however, this is a strong submission and I have no hesitation recommending it be accepted for JCDL 2018.","Overall evaluation: 3
Reviewer's confidence: 5
Recommend for best paper: yes",3,,,,,2/16/2018,23:28,no
496,130,223,Maxxxx Khxxx,3,"(OVERALL EVALUATION) This paper tackles the problem of representing mathematical formulae, which are abundant in scientific papers and digital libraries in general. The authors motivate the work by outlining the importance of constructing powerful parser to parse and represent mathematical formulae in order to be able to perform any meaningful retrieval on them. Since labeled datasets are usually the catalyst of many research papers, the authors create a dataset of more than 300 mathematical formulae and manually annotate the literals within each formula. After that, many open source and commercial mathematical parsers are benchmarked on this golden dataset, and their performance is reported using tree edit distance from the gold parse. Finally, the authors explored incorporating the context of the formula to achieve better performance.

The motivation of this work is clear, and it?€?s indeed very relevant to JCDL. However, the flow of the paper is not smooth. Perhaps because the authors tried to include so much material in it, that they ended up not giving each part its fair share. Ideally, this paper could be split into 2 separate papers: one for the data collection and benchmarking, and a second paper for showing how including context is beneficial. 

Section 3.2 on building gold standard is especially not easy to follow. It could use some re-writing and explanation. Part of the issue is that the authors are explaining special cases for parsing on certain gold examples, but they refer to these formulae by their number (gold id 230) instead of listing the formula itself and explaining the issue. You might want to choose using couple of running examples to help in explaining the idea. Since constructing the gold standard is such a big deal, the authors could have dedicated the entire paper just for that, and made sure the process is very clear.

The evaluation of section 5 is not very clear to me as well. I expected it to be compared to other solutions described in Figure 3 or 4. But the authors picked 10 functions for evaluation. I?€?m not clear why was the evaluation was limited to this set, and how much improvement did they find in comparison to other approaches.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I'm not an expert on this topic. So my review should be considered as an average JCDL attendee who would read such paper.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/16/2018,23:48,no
497,131,170,Braxxxx Hemxxxxx,1,"(OVERALL EVALUATION) Strengths of the paper:  examines ways to improve determining similar articles to an existing article.  This is done under fairly specific conditions (identify similar cited article given one out of a set of citations from same sentence).  Advances in techniques are given, and very well detailed.  Application is sound for this situation. 

Weaknesses of paper:  This is a fairly narrow topic for JCDL audience.   So it may not be of as much interest to general audience.   The paper goes into great detail on techniques and methods, which is nice.  However, this may make the narrow topic even less engaging.  The interesting result of the paper, that their improved techniques (1&2) performed better than current standards (fastText, LINE) should get much more coverage, and explanation.   Especially to make more engaging for readers less familiar with the topic area.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,1/24/2018,23:25,no
498,131,249,Byxxx Marxxxx,2,"(OVERALL EVALUATION) This is a tantalizing paper. I read it several times and still want to look at it yet again.

I think it provides meaningful evidence that leveraging co-citations (that is multiple citations in a single list associated with a single sentence in a paper) can be leveraged to better identify other potentially relevant papers. Of course an array of additional information needs to be mixed in to annotate the co-citation graph.

The paper needs a careful edit (e.g., search for citatio. )

There were many technical components and many alternative computations so not all of the pieces were sufficiently documented even though more words were allowable for this venue. But actually, I don't mind that too much because the I see the contribution of the paper as definition of a somewhat innovative and seemingly useful analysis technique to help researchers better leverage digital libraries of research articles. The evidence here is encouraging even though it is not yet strongly supportive of the technique's value: just the kind of thing we should want in JCDL.

The authors should look over the abstract. I am not sure that ECCP is allows ""automatic evaluation of paper retrieval systems"". Rather the work seems to describe an approach for identifying promising papers to cite.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I was puzzled by a couple of things on this paper. Including  format and length. 
I'm thinking it was a rush to get it out the door for submission?
Yet I am intrigued.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/16/2018,1:11,no
499,131,18,Haxxx Alxxxx,3,"(OVERALL EVALUATION) - This paper proposes a co-citation prediction that considers the context of the citations.
- The proposed system takes as input: paper p1 + the context of the co-citation.  
- The system?€?s output is predictions of additional citations that could be cited with the citation p1.
- The underlying assumption of this study is as follows: papers that share a citation context are related. However, this needs a proof. Papers certainly exist that share a citation context yet are more dissimilar than similar or refer to very disparate topics. It would be useful if the authors were to cite a reference to this assumption or at least provide some statistics from their dataset to show that such citation behaviors are prevalent. 
- Page 3: In Section 2.2, this sentence is repeated: ?€?ACL Anthology, an open-access repository of journal, conference, and workshop papers in computational linguistics/natural language processing.?€?
- Page 3: Footnotes 2 and 3 are similar and should be combined. 
- Page 3: ?€?Since the average citation number of enumerated co-citations is 2.38, except for the first paper, we predict an average of 1.38 hidden papers for one case.?€? How was the 1.38 figure determined?
- Page 3: ?€?From the collected papers, citation and enumerated co-citation were detected by regular expressions, e.g. (Author1 YEAR1; Author2 YEAR2) or [1][2].?€? What does [1][2] refer to? An explanation should be offered here. 
- Page 5: The paper refers to a label or section called ?€?BACKGROUND.?€? However, on the website referenced the corresponding heading is ?€?INTRODUCTION?€?: https://www.nlm.nih.gov/bsd/policy/structured_abstracts.html. 
- There are some redundancies in the paper. Several concepts or ideas are repeated in more than once place. The paper includes several sections pertaining to motivation: 1.2 Goal, 1.3 Contribution, and 2.1 Advantages and they can be combined. Nanba [18] is cited 4 times. First in Section 1.1 and after 2 lines another citation for the same reason (i.e., three types of citation functions). Nanba [18] is also cited in Section 3.2, and again in Section 3.4. Another example is ?€?functional components (OBJECTIVE, METHOD, and RESULT),?€? which are introduced and explained in more than one section. 
- One of the main issues is that the results section (5.4) is very brief (10 lines). Additionally, the paper does not offer any discussion of the results such that any conclusion is left to the reader.
- Several related works should be mentioned and their results compared with the results reported in this paper. These works include the following: 
- He, Q., Pei, J., Kifer, D., Mitra, P., & Giles, L. (2010, April). Context-aware citation recommendation. In Proceedings of the 19th international conference on World wide web (pp. 421-430). ACM.
- Caragea, C., Silvescu, A., Mitra, P., & Giles, C. L. (2013, July). Can't see the forest for the trees?: a citation recommendation system. In Proceedings of the 13th ACM/IEEE-CS joint conference on Digital libraries (pp. 111-114). ACM.
- He, Q., Kifer, D., Pei, J., Mitra, P., & Giles, C. L. (2011, February). Citation recommendation without author supervision. In Proceedings of the fourth ACM international conference on Web search and data mining (pp. 755-764). ACM.
- Tang, J., & Zhang, J. (2009, April). A discriminative approach to topic-based citation recommendation. In Pacific-Asia Conference on Knowledge Discovery and Data Mining (pp. 572-579). Springer, Berlin, Heidelberg.
- It would be interesting if the authors were to compare their results from the text classification with results reported in papers that focus on building a citation recommendation system.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/16/2018,1:56,no
500,132,3,Trxxx Aaxxxx,1,"(OVERALL EVALUATION) This poster abstract presents a tool for annotating videos, developed to support personalization of learning videos. The abstract is well presented and is very relevant for the conference. The emphasis in the presentation is on the features of the tool, and the research related to this is only briefly described in future work. As such, it is more like a demonstration. 
However, this is a poster that I would have liked to visit at the conference.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/15/2018,9:35,no
501,132,63,Vitxxxx Casxxxx,2,"(OVERALL EVALUATION) The idea of a tool (ViDeX) for annotating videos is an interesting one, and definitely opens new perspectives for the use of videos in teaching and self-learning. Assuming that this contribution will be accepted, it would be interesting to present, at the time of the conference, the results of the use of the tool in undergraduate classes. 
It might be useful to provide also some comments about the state of the art in this field (are there other similar tools? how do they compare?) and the information whether the tool is open-source and freely available or if it is a proprietary one.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/15/2018,12:08,no
502,133,42,Joxx Borxxxx,1,"(OVERALL EVALUATION) This submission is for a poster about a community managed repository of data in the domain of proteins.

It is an interesting poster, fitting well in this conference.

English is OK but could be improved (please ask a proofread)
Authors should avoid to use ""quantitative statements"" when that rigor is not necessary and the numbers can not be proved, as for example when in this paper is it ""...scientists invest almost 90% of their efforts in the collection of data..."" 
On the other side, also expressions with fuzzy rigor should be avoid, as for example ""The system also focuses on plenty of contextual information...""","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2/13/2018,19:38,no
503,133,339,Sanxxxxx Sixxx,2,"(OVERALL EVALUATION) This paper proposes a community-driven data curation system to enhance data understandability and reusability. The system focuses on linking relevant literature to the data as use contextual information to help users understand the data. I think this could be a very useful system to the research community. Although there are only a handful of data repositories available, metadata are missing in many which makes them unfit for reuse. This system could be an important step towards eliminating this.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/16/2018,5:51,no
504,133,92,Mixxxx Doxxxx,3,(OVERALL EVALUATION) The topic of community driven data curation is interesting. As presented the poster proposal addresses mostly generic data curation issues - if accepted it should provide a stronger focus on the community components.,"Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/18/2018,19:38,no
506,134,48,Juxxxx Bruxxxx,1,"(OVERALL EVALUATION) This paper is in the incorrect format, the figures are illegible, and the methods are not described in sufficient detail to warrant acceptance.","Overall evaluation: -3
Reviewer's confidence: 3
Recommend for best paper: no",-3,,,,,2/13/2018,3:16,no
507,134,343,Giaxxxxxx Silxxxx,2,"(OVERALL EVALUATION) The layout of the paper is not the default one.
The paper is not well-written and the problem should be presented more clearly. 
It's not stated where the questionnaire was submitted to which users and in which context. 
The results are not contextualized.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2/14/2018,9:50,no
508,135,48,Juxxxx Bruxxxx,1,(OVERALL EVALUATION) This paper is in the incorrect format,"Overall evaluation: -3
Reviewer's confidence: 3
Recommend for best paper: no",-3,,,,,2/13/2018,3:20,no
509,135,348,Joxx Smxx,2,"(OVERALL EVALUATION) This poster looks at the effectiveness of official library WeChat accounts to successfully promote (Chinese) academic library services. The authors took data from over 5000 records, used Tableau to visualize the results, and made recommendations to improve WeChat usefulness for promoting academic mobile library services. The content is a bit thin and lacks sufficient discussion. The authors used a non-JCDL format for producing their submission; if compiled against the correct template, there is probably another full page of space that could be used to clarify the data they gathered and their analysis of the results. Even for a poster, there is a lot left unexplained. 

A minor point: The word ""tableau"" should be capitalized when referring to the Tableau product. Also, Table 1 could use an explanatory caption even though the headings are mentioned in the preceding paragraphs.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/14/2018,21:25,no
510,136,3,Trxxx Aaxxxx,1,"(OVERALL EVALUATION) This poster abstract presents findings from a study on what researchers look at when inspecting academic search results. The topic is relevant for the conference and the paper is reasonably well structured and presented. The actual results are not particularly ground breaking, but the study is well performed and presented. Suitable as a poster, although not a very competitive one.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/14/2018,15:57,no
511,136,37,Joxxxx Bxx,2,"(OVERALL EVALUATION) The authors did a survey of some dozens of researchers and students to find out what elements (title, abstract, ...) they look at when searching for research articles. They present the results (e.g. ~90% of the participants responded to use the title) but, and this is my main criticism, they do not discuss the results in relation to related work. The results themselves are rather not surprising and it would be interesting to know if and how the results differ from what other researchers have found already. In addition, I have some doubts regarding the methodology: it seems to me that doing research on log files, or doing a real user study would lead to more meaningful results than asking researchers what document fields they consider in search (or more precisely, what they believe to use). 

I recommend to reject this poster this year but I see potential in it. Hence, I encourage the authors to compare their results with results from related work and re-submit to JCDL next year, or to another conference such as TPDL or ICADL.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/15/2018,9:37,no
512,136,339,Sanxxxxx Sixxx,3,"(OVERALL EVALUATION) I am not sure of the proposal. Traditional paper search engines like Google scholar, Microsoft Academic search do look into content of the paper to judge whether it is relevant or not. If the authors are proposing they want a ""faceted"" paper recommendation system, they could look into this paper - Chakraborty, T, et al. ""Ferosa: A faceted recommendation system for scientific articles."" PAKDD, 2016. 
Also there is no discussion on how they plan to implement such a system.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/16/2018,6:07,no
513,137,48,Juxxxx Bruxxxx,1,"(OVERALL EVALUATION) This poster explored the performance consequence of creating WARC files from a python script retrieiving content from an in-memory cache as opposed to running transactional archiving software on a web server. These WARC files are stored on the web server where they are presumably retrieved or sent to an archive on a periodic basis.

Transactional archiving captures all versions of content while this method will possibly miss versions of content that change multiple times in between the python script running. This poster should cite the trade-offs between the performance gains of their proposed method and the impact on archive completeness as possible future work. Additionally, this poster should mention a method for transferring the WARC files to an archive as this may have a significant performance impact; transactional archival software often provides methods of accessing archived content, and the python script merely creates a WARC.","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,Jory,Morrison,jtmorrison@mitre.org,358,2/15/2018,20:47,no
514,137,48,Juxxxx Bruxxxx,2,"(OVERALL EVALUATION) This poster expressed the difficulty web archiving crawlers face in achieving a high crawl frequency (to minimize missing pages) at a low cost. In order to archive high value web resources, they proposed a server-side system of preserving content that does not inter with the primary HTTP transactions of the server. Web content is archived by a process that periodically adds content from an in-memory cache system to a WARC file on the local file system. The authors explained the system architecture and archiving process in detail. This work is relevant to the web archiving community.

The following steps can be taken to improve the poster:

Semantic suggestions:
1. Emphasize target user of the system: I believe the target users are web site owners who wish to preserve their content as opposed to public web archives that wish to preserve third-party web pages, since the usage of the system requires some intervention of the site owner.

2. Since the proposed system attempts to increase web archive coverage, a comparison of missing page rates in a web archive that adopts the system and a web archive that adopts a standard crawler seems appropriate. Similarly, express the incentives for site owners or archivist or system administrators to adopt the system by showing how the results affect their objectives.

Technical suggestions:
1. The Abstract should be included as the first section in the poster. Figure 1 can be shrunk to half page width to permit space for the abstract if the following steps are taken: 
1.1. Reduce padding inside text boxes
1.2. Reduce length of arrows
	
2. The numbering in the system architecture description in section 2.1 should match the component numbers in Figure 1.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,Alexander,Nwala,anwala@cs.odu.edu,329,2/16/2018,19:59,no
515,137,111,Nixxxx Fexx,3,"(OVERALL EVALUATION) The paper describes a python deamon which interacts with Memcached to constantly check for new content in the cache and store it to a local WARC file to seamlessly archive changes in web sites. The paper conducts a load simulation using Greek wikipedia data to show the scalability of the proposed approach.

The paper provides just an incremental improvement with respect to previous approaches and works only in the case of architectures relying on memcached - which is in any case very broadly adopted.

One question that is left open: what happens of all these locally stored WARC files? How are they contributed to centerla web archives?","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/18/2018,22:42,no
516,137,68,Boxxx Caxxxx,4,(OVERALL EVALUATION) The poster submission describes an approach to archiving materials while minimizing the burden on the web server. The paper is clearly written and reports good results. It should make a good poster.,"Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/23/2018,13:13,no
517,138,48,Juxxxx Bruxxxx,1,(OVERALL EVALUATION) Incorrect format and only an abstract.,"Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2/13/2018,3:05,no
518,138,3,Trxxx Aaxxxx,2,"(OVERALL EVALUATION) This poster abstract presents the methodology applied in a pilot project for cataloging and indexing the special collection at the University of Florida. Although the proposed workflow potentially is of interest to the conference, the submitted abstract is simply to short to justify acceptance (only two paragraphs). A poster at JCDL includes both a printed 2 page abstract that needs to pass the peer review and an accompanying actual poster that will be presented at the conference. This contribution does not meet the first requirement.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/14/2018,15:12,no
519,139,343,Giaxxxxxx Silxxxx,1,"(OVERALL EVALUATION) Nice work, with extensive results for a poster.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/14/2018,10:27,no
520,139,48,Juxxxx Bruxxxx,2,"(OVERALL EVALUATION) This paper introduces the concept of using prior knowledge from a corpus to leverage its vocabulary to computationally determine the topic of a document. The methodology appears sound but the practicality of the approach is not demonstrated. A practical example of using the methods, particularly as they compare to the conventional methods enumerated, would appeal more to the JCDL crowd. I anticipate pratical examples being shown on the poster itself.

The final (incomplete) sentences (And the effectiveness...) leaves something to be desired: It essentially states, ""We saw it was effective, therefore it must be."" A more rigorous and thorough evaluation of the approach (if given more space) would make the approach more convincing.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The paper seems to have merit though the subject area is a bit out of my wheelhouse. This poster may spark interest in JCDL attendees from the IR community.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,Mat,Kelly,mkelly@cs.odu.edu,53,2/14/2018,23:26,no
521,139,348,Joxx Smxx,3,"(OVERALL EVALUATION) Traditional key phrase extraction, used to determine the subject area and research focus of one or more documents, is typically accomplished using TF-IDF and/or TextRank. These methods depend on information found within the documents themselves rather than on those found in similar documents. In this paper, the authors describe using set(s) of domain-specific key phrases together with TF-IDF and TextRank as a basis for determining key-phrases in a set of (new) documents. They contend that applying the ""prior knowledge"" key phrases as part of a 3-step analysis produces a more accurate set of extracted key phrases from the document(s). 

The idea of developing domain-specific key phrases as a reference set is not new. However, according to the authors what is new about their approach is the process of using such a set together with other traditional analytical tools as a means to improve classification of papers. 

As a poster, the paper has enough merits to warrant presentation at JCDL. There are some shortcomings, including a weakly written conclusion. It would be perhaps better to shorten the length of the Abstract section and provide a better discussion in the Conclusion. 

Examples of errors that need correction:

1. ""Anonymous Authors"" is displayed rather than the names of the actual authors.
2. The Copyright section is also incorrect. The authors need to recompile the paper using the proper JCDL template.
3. In section 3, ""performance on Insepc"" --> performance on Inspec
4. There are numerous grammatical errors such as mismatched singular-plural noun-verb (and other) combinations that should be cleaned up but which don't greatly hinder reading the paper.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/15/2018,13:51,no
522,140,343,Giaxxxxxx Silxxxx,1,"(OVERALL EVALUATION) The is no abstract. 
This is a nice work and it will be interesting to be discussed at the conference.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/14/2018,9:54,no
523,140,92,Mixxxx Doxxxx,2,"(OVERALL EVALUATION) The focus of this proposal is on modelling footsteps (indeed the conclusions section only highlights this aspect of the work). The proposal discusses the use of datasets in this process but the work with datasets and the various transformations are not explained so that the participants in the ocnference could understand what data-related lessons had been learned from this task. Methodologically, there are unclear aspects, how the sizes of the datasets had been selected - are 162 footsteps sufficient for training? What experiments had been done after the training? 

Some abbreviations are not expanded (HVAC). 

While there might be interesting research data management aspects around this subject domain, the logic of the text is not helping to understand what is the RDM angle of interest to the DL community.","Overall evaluation: -2
Reviewer's confidence: 2
Recommend for best paper: no",-2,,,,,2/18/2018,11:34,no
524,140,68,Boxxx Caxxxx,3,"(OVERALL EVALUATION) The poster proposal describes a data collection and analysis project related to footsteps in a building on a campus.  The initial description calls the project a digital library, though it is not clear how it becomes a library.  What is the interface and who will access the data in the collection.  The description of the processing is clear and couched in terms of acquiring metadata.  The project is funded by IMLS.  The library connection could be clearer, but the limited space in the poster proposal is used to describe the capture and processing of the data.  The poster should make the library connection clearer.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) My default on posters is to accept as long as there is something worthwhile.  This may be a bit of a stretch as a digital library poster, but it is nice work.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/20/2018,16:13,no
525,141,42,Joxx Borxxxx,1,"(OVERALL EVALUATION) This submission for a poster describes a solution to annotate paintings, motivated by a case to annotate wall paintings. For that, the authors make reference to a specific ontology created for this particular project, and make relevant references to the state of the art.

The subject fits very well in this conference!

However, the authors might not well understood the requirement for the submission, as it comprises a 2 pages in the proper ACM template, but with only one full section ""Abstract"" for the core text (and also Keywords, Acknowledgements and References).
If this submission is accepted, authos should be instructed to provide a new text, structured as a proper ""paper"", and not only with an abstract (even if for only 2 pages... in fact, reading the present abstract we can guess the relevance of the work, but also feel the need of fundamental information for a proper assessment, which is why I only classify it as ""border line""...)","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2/13/2018,19:46,no
526,141,343,Giaxxxxxx Silxxxx,2,"(OVERALL EVALUATION) The paper is hard to follow and the work is not well-contextualized. The work presents an ontology and a tool for annotations. There are no screenshots of the tool and it is hard to assess its value; the authors had one more page available to add useful details, but they reported only very high level information. 
In this form the paper is not well-suited for JCDL.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/14/2018,10:32,no
527,141,339,Sanxxxxx Sixxx,3,(OVERALL EVALUATION) I am not sure how this proposal is going to help the community. This might be because the authors have not proved enough detail about the model. It would also be better if the authors could develop a system based on the ontology model to assist the annotators.,"Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/16/2018,6:32,no
528,142,42,Joxx Borxxxx,1,"(OVERALL EVALUATION) This poster describes the first actions and lessons of a project aiming to understand how owner of digital collections perceive and measure the reuse of their digital objects.

It is a very interesting topic, and in some way innovative in its purpose.

If this submission is accepted, the authors should be instructed to provide a proper ""paper"", with a section for ""References"" (no references at all are provided in the present submission...)","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/13/2018,19:50,no
529,142,343,Giaxxxxxx Silxxxx,2,"(OVERALL EVALUATION) The paper reports an interesting, yet very preliminary, study about the reuse of digital resources. The topic is of interest for JCDL. I suggest to the author to present the results with plots and tables in the poster. It'd be interesting to ask to practitioners how they cite the resources they re-use or if they consider the problem of credit attribution when they re-use resources.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/14/2018,10:37,no
531,142,92,Mixxxx Doxxxx,3,"(OVERALL EVALUATION) The topic of this proposal is relevant to the conference and it presents initial results which would be interesting to discuss with the wider community. The proposals needs another language revision (e.g. there is repletion in ?€?The remaining 302 surveys had at least at least one answer beyond the IRB agreement.?€?). 

The reason why I gave the borderline mark for this proposal is that there are three major shortcomings (the first is purely related to how this proposal is structured, the second and the third are issues with the study design): 

1) The lack of connection to any other previous work on re-use and previous results. Relevant literature has to be referenced; the topic of reuse is visible at least from 2009 at ECDL/TPDL conferences.

2) The lack of connection ?€? and this is visible from the survey as well  - with the *rights* for the use of the collection. Different rights would influence the re-use patterns but this is a topic not addressed in the survey. 

3) In what is a short survey the focus changes ?€? for example question  (9) ?€?What are the two greatest barriers to assessing digital libraries??€? is more generic (digital libraries is a huge domain to address). It is not surprising that the respondents who arrived that far in the survey would be mentioning re-use as a barrier ?€? after all this is what they were asked previously about and it is what sticks to their short term memory. I personally would not trust any ranking made this way since the high appearance of the reuse topic is introduced by the survey protocol.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Great topic, poor research design.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/23/2018,10:37,no
532,143,48,Juxxxx Bruxxxx,1,"(OVERALL EVALUATION) How Do People Read eBooks on Mobile Devices features novel ideas but poor execution in both technical writing and research. The paper seeks to understand how reading on mobile devices affects reading habits among men and women given contrasting ""genres"": Fiction and Professional. While indicating that 16 books total were selected (8 fiction, 8 professional) there is no discussion as to how these books were selected, or even indicating all of the books selected (the paper provides the titles of 4 fiction and 2 Professional books). In addition, there is no mention of screen-size differences or the devices used to read the eBooks beyond being a ""mobile reading device"". Without a concrete definition of ""mobile reading device"", the results (which focus on time per page) are less useful as characters per page can vary between screen-size, font size, column-width, etc. If this isn't an issue within BaiduRead (the only app used within the study), it should be explicitly stated to make results more credible. 

The paper is also poorly styled and features several poorly structured sentences. The text of the first paragraph on the second page is split between two columns, then broken by figures, then returning to the original styling. The bibliography also appears to have been improperly compiled. The text for the figures is also small and difficult to read. Finally, the formatting of the paper is insufficient to be considered for publication at JDCL.","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,Collin,McRae,cmcrae@mitre.org,359,2/13/2018,21:54,no
535,143,3,Trxxx Aaxxxx,2,"(OVERALL EVALUATION) This poster abstract presents a log-analysis from eBook reading on mobile devices. The topic will be of interest to the conference. A main objection is the lack of comparisons that is needed to determine what is specific for mobile devices. How does this e.g. compare with reading on a large screen (at least for scientific reading that is a relevant comparison). Do screen size of device matter etc?

Another objection to this paper is the formatting. The author section does not follow the ACM format, the references are strange. On page 2, the text on the top is split over to paragraphs etc. I also find that some sentences needs to be corrected.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) NB! This is an interesting and relevant poster, but the quality of the abstract does not meet the requirements. 
The final version needs to be checked before it finally can be accepted.

I revised my score to weak reject, so that I follow the other reviewers strategy on rejection because of bad formatting.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/27/2018,8:50,no
536,143,348,Joxx Smxx,3,"(OVERALL EVALUATION) The authors use web log data from one of China's largest e-book apps to analyze user interactions with their mobile devices, comparing reading activities of 8 ""leisure"" e-books against those of 8 ""professional"" e-books. From this analysis they conclude that there are several enhancements to the e-book UI that would benefit users.

Some of the claims seem dubious to me. For example, that the data reflect a user's psychological status, his hyper-attentiveness (or lack), and cultural background. It also wasn't clear how the particular subset of each category was determined (why this set of 8 and not some other set?). Some characteristics are perhaps less debatable -- page reading duration, page jumping, reading schedules. I'm not convinced that these necessarily lead to the conclusions the authors derive.

I rate this as a weak accept, not just for the issues noted above, but also because the paper is awkward to read with numerous sentences written in a non-standard phrase-order (Example: ""It's also found that the page reading duration of professional books is higher than that of fictions""). But the data gathering method and analytical approach may be of interest to the JCDL community.

Some items for correction:

1. The wrong template has been used. The paper needs to be resubmitted using the JCDL template
2. Author names have embedded icon-links. This needs to be corrected.
3. In the Abstract: ""hyper attention"" --> hyper-attention
4. In the CSS Concepts: Remove exclamation point,  ""Human-centered computing!"" --> human-centered computing (HCC)","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/15/2018,13:10,no
538,143,129,Ricxxxx Fuxxx,4,"(OVERALL EVALUATION) The paper presents large-scale data from Baidu about reading patterns with BaiduRead.  This, by itself, makes this an interesting report.  However there are a number of factors that weaken the paper:
- The study probably relates primarily to China due to its basis in Baidu.  The title/paper should indicate this rather than trying to represent a broader population.
- Are the books in Chinese or English or goth?  If both, does the language make a difference in reading patterns?
- What are the expected demographics of the readers?
- How flexible is the app in terms of page size, orientation, font size, etc., and what variation are readers using?
- The figures leave many unanswered questions.  One that struck me in particular was Figure 7.  The X axis is not labeled.  I assume that this refers to hours, but what day is the starting point (Sunday, Monday, something else?).  The weekend effect doesn't really jump out to me in this diagram.
- The formatting makes the paper hard to understand.  The paper must be reformatted into the JCDL conference format.  Items such as the distribution of text across columns at the top of page 2 are not correct for the format.  The references need to be fixed.

The authors may wish to consult previous proceedings of the JCDL conference, since reading on mobile devices is a topic that has been reported on.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/26/2018,18:47,no
539,144,343,Giaxxxxxx Silxxxx,1,"(OVERALL EVALUATION) This is a poster and not a poster-paper- It does not respect the layout rules and it is not fair to the other authors. For future submissions, I suggest to the authors to read the submission instructions prior to submission in order to meet the rules of the conference.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2/14/2018,10:39,no
540,144,63,Vitxxxx Casxxxx,2,"(OVERALL EVALUATION) The poster presents an interesting analysis of patent data, to identify possibly emerging cross-technology and cross-industry application fields. The methodology used (cluster analysis of co-occurrence matrix) is well known, and the interest of the poster is really in the conclusions drawn from the analysis of about 80000 patent data. 
It seems to be a very narrow topic that could be of interest to a small minority of attendees at JCDL 2018. It might be better to present the poster at a more appropriate event.","Overall evaluation: -1
Reviewer's confidence: 2
Recommend for best paper: no",-1,,,,,2/15/2018,12:09,no
541,144,339,Sanxxxxx Sixxx,3,(OVERALL EVALUATION) The authors have submitted a poster instead of a 2-page proposal and hence have not put in details of their proposal which makes it hard to understand what they wish to do. I am also not sure what they wish to achieve or demonstrate from their analysis.,"Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/16/2018,6:37,no
542,145,343,Giaxxxxxx Silxxxx,1,(OVERALL EVALUATION) The work is of interest to the JCDL community and it is well suited for a poster presentation.,"Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2/14/2018,10:40,no
543,145,139,Danxxxx Gxx,2,"(OVERALL EVALUATION) This poster paper describes a connection between a pair of documents which are cited by two other documents in a similar citation context. The construct of this paper is clear and the method is presented in detail, giving information about the technologies used. However, a few comments have to be keep:
-	The method is shown to have yielded relatively good results, but it seems very similar with the method presented in the article Rough co-citation as a measure of relationship to expand co-citation networks for scientific paper searches by the present author.
-	Proofreading is necessary to correct some minor grammatical (p. 1 ?€?A key for improving?€?) 
-	The following phrases could be considered plagiarized. Therefore, you have to reformulate the following sentences: ?€?a rough co-citation relationship may be weaker than the original co-citation relationship, because a rough co-citation relationship is determined by the citations in two separate documents. However, rough co-citation linkages may yield new related documents that are not identified by the original co-citation?€? (http://onlinelibrary.wiley.com/doi/10.1002/pra2.2016.14505301131/full)
or ?€?The aim of this study is to clarify whether ?€?rough co-citation?€? can improve the performance of co-citation clustering; a rough co-citation relationship is a linkage between a pair of documents cited by two other documents in a similar citation context.?€? (http://onlinelibrary.wiley.com/doi/10.1002/pra2.2016.14505301131/full
-       Also, the Figure 1 is very similar with the Figure 1 from the previous article cited (see:http://onlinelibrary.wiley.com/enhanced/figures/doi/10.1002/pra2.2016.14505301131#figure-viewer-pra214505301131-fig-0001).

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Some paragraphs in this poster-paper are similar to other published in Eto, M. (2016). Rough co-citation as a measure of relationship to expand co-citation networks for scientific paper searches in Proceedings of the Association for Information Science and Technology, Volume 53, Issue 1, pp. 1-4: ?€?a rough co-citation relationship may be weaker than the original co-citation relationship, because a rough co-citation relationship is determined by the citations in two separate documents. However, rough co-citation linkages may yield new related documents that are not identified by the original co-citation?€? (http://onlinelibrary.wiley.com/doi/10.1002/pra2.2016.14505301131/full)
or ?€?The aim of this study is to clarify whether ?€?rough co-citation?€? can improve the performance of co-citation clustering; a rough co-citation relationship is a linkage between a pair of documents cited by two other documents in a similar citation context.?€? (http://onlinelibrary.wiley.com/doi/10.1002/pra2.2016.14505301131/full
Actually, the same author presented already this research, the title was changed... Even, the Figure 1 is similar with the Figure published in the previous cited paper (see http://onlinelibrary.wiley.com/doi/10.1002/pra2.2016.14505301131/full#pra214505301131-fig-0001).","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2/14/2018,15:35,no
544,145,339,Sanxxxxx Sixxx,3,"(OVERALL EVALUATION) This paper discusses co-citation clustering using rough co citation. The author does not specify why this clustering is even required. Also the standard way of determining how good a clustering is through purity, NMI or ARI. I am also not sure what is the ground truth.","Overall evaluation: -1
Reviewer's confidence: 4
Recommend for best paper: no",-1,,,,,2/16/2018,10:45,no
545,145,92,Mixxxx Doxxxx,4,"(OVERALL EVALUATION) The proposal introduces a new co-citation clustering technique (the author introduced the term rough co-citation in their earlier work) which outperforms the baseline technique of co-citation. The paper is clearly structured, written and illustrated.","Overall evaluation: 2
Reviewer's confidence: 2
Recommend for best paper: no",2,,,,,2/20/2018,20:27,no
546,146,343,Giaxxxxxx Silxxxx,1,(OVERALL EVALUATION) This is a nice poster presenting a tool for helping user searching documents in a DL.,"Overall evaluation: 3
Reviewer's confidence: 5
Recommend for best paper: no",3,,,,,2/14/2018,10:52,no
547,146,3,Trxxx Aaxxxx,2,"(OVERALL EVALUATION) This poster abstract presents a proxy service that manipulates the query before sending it to a digital library, to solve specific errors or limitations that are known for specific services. A well presented abstract and a poster that will be found interesting at the conference, although the realism of having such a proxy service can be questioned.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/15/2018,13:23,no
548,146,404,Dxx W,3,"(OVERALL EVALUATION) The study proposes a bespoke proxying technique for Digital Library Environment. The topic is interesting and suitable for the conference. However, how to implement the known-item search and word wrap should be stated more clearly. Moreover, the test results should be given to evaluate the technique.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/16/2018,21:11,no
549,147,343,Giaxxxxxx Silxxxx,1,"(OVERALL EVALUATION) Nice poster paper presenting strategies for semantic disambiguation in a DL. The paper is well-written and can be of interest for the DL community,","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2/14/2018,10:53,no
552,147,139,Danxxxx Gxx,2,"(OVERALL EVALUATION) This paper presents a novel approach to word sense disambiguation based on semantic association. The methodology is effective and can improve the state of the art. The content of this paper is clear and well described, if we take into account the reduced space allocated to writing a poster-paper. However, some comments should be considered:
-	Even that the method have various applications, it will be very useful if the author will solve the issue of proper names (see Khurana, 2017) and multi-word expressions (see Timothy Baldwin's works). 
-	Proofreading is necessary in order to correct some minor language mistakes (p. 2 ?€?need to excluded?€?) and, also, the authors have to reformulate the following sentence, which is used too often: ?€?The extended abstract introduces?€? (see Abstract, Introduction, Conclusion). 
Finally, the paper has good practical implications, particularly since the authors have made available to the public their test results for a set of 100 out-of-copyright volumes from the HTDL (https://git.io/vN9lE).","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2/14/2018,16:59,no
553,147,404,Dxx W,3,"(OVERALL EVALUATION) This poster introduces a seeding algorithm and explores seeding strategies for volume-based semantic disambiguation in digital libraries based on the Wikipedia linking structure. The topic is novel and interesting, and the algorithm is clear. If the author can give some test results of the algorithm, it would be better.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/16/2018,21:01,no
554,150,343,Giaxxxxxx Silxxxx,1,(OVERALL EVALUATION) This paper presenting the architectural details of a system for searching OCR'd documents within a DL. The search interface is of interest and it can generate discussion at the conference.,"Overall evaluation: 3
Reviewer's confidence: 5
Recommend for best paper: no",3,,,,,2/14/2018,10:57,no
555,150,3,Trxxx Aaxxxx,2,"(OVERALL EVALUATION) This poster describes the implementation of a large scale digital library with fine grain access (text tokens from a large set of OCR documents). The context of the contribution is the need to support work-set creation, although this is somewhat poorly followed in the main part of the paper where the emphasis is on the implementation and configuration of the system and most of these aspects relate to search in general. 
However, I find the contribution relevant for the conference as it demonstrates the implementation and setup of a large scale digital library for fine grained information quite well.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/16/2018,14:54,no
557,151,42,Joxx Borxxxx,1,"(OVERALL EVALUATION) This poster proposes to report the finding of a bibliographic search (that resulted in 42 articles) on the subject ""smart library"" (or that has been perceived ass related to that...).

This text gives us a sense of ""too naive / too speculative"" on the subject, as it look everything we can understand that ""has digital on it"" can be tagged of ""smart...""... this might be a first step for a future interesting work on the historical view of how this subject has been present in the literature, but what we can read in the present proposal is not very motivating... We are aware that ""the library"" as a concept is facing a huge pressure for transformation, and this kind of work could be a contribution to that, but we are not sure of that by what we can read in the present proposal... We see the potential, but... border line!","Overall evaluation: -2
Reviewer's confidence: 5
Recommend for best paper: no",-2,,,,,2/23/2018,10:28,no
558,151,343,Giaxxxxxx Silxxxx,2,"(OVERALL EVALUATION) The format of the paper does not fully respect the conference rules. Nevertheless, the paper is not well-suited for poster presentation. This is a very brief and somewhat superficial survey of papers discussing the topic of ""smart libraries"".
Cited papers are not properly referenced in the text and there is no way to understand what papers were selected for the proposed analysis.","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2/14/2018,11:01,no
559,151,404,Dxx W,3,"(OVERALL EVALUATION) This paper using meta-synthesis method to analyze qualitative research studies on Smart Libraries. Three time periods have been identified to present the transition of technology occurring to meet specific needs. The topic of the paper is interesting, novel, and relevant to the conference. However, I am wondering how to find related papers to review. Only using ?€?smart library,?€? ?€?mobile services,?€? ?€?digital library,?€? and
?€?internet of things?€? probably is not enough because some literatures are talking about smart library but without the term ""smart library"", for example, the topic about information recommendation of the library. Therefore, I think the authors need to describe the definition and scope of the smart library, and then they can choose coresponding terms to search literatures. ""METHODOLOGY"" should be section 2, not section 3. And research questions are redundant in both ""INTRODUCTION"" and ""METHODOLOGY"".","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/18/2018,1:43,no
560,152,48,Juxxxx Bruxxxx,1,"(OVERALL EVALUATION) This paper discusses a method for identifying troll accounts on Twitter by using the similarity of keyword signatures and hashtags of tweet content to the keyword signatures and hashtags of known trolls. This work is an excellent example of a poster on preliminary work that deserves further feedback and exploration. I would encourage the authors to consider evaluating their algorithm's success against a random tweet (i.e., non-troll) on a topic rather than a dataset of only known trolls.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2/13/2018,2:48,no
561,152,404,Dxx W,2,"(OVERALL EVALUATION) The poster analyzes the data of archive of tweets related to Brexit. However, from the data analysis point of view, the basic statistic is descriptive and too simple. It does not have much implications for the digital library communities.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/18/2018,5:26,no
562,152,111,Nixxxx Fexx,3,"(OVERALL EVALUATION) The paper discusses the identification of topics produced by troll about Brexit and how they can be targeted at influencing  not only to British politics but also foreign one, e.g. German.

The idea of the poster is nice and interesting as well as the results. However, it lacks any details (or links to code) for using the developed tool and run this kind of analysis again or on other datasets. The paper just says ""We used LDA topic modelling [1] using the Mallet toolkit [4] to cluster the tweet text content"". So either important information has omitted, severely limiting the usefulness of the paper, or an already existing and off-the-shelf script has been run, severely limiting the innovation of the paper.","Overall evaluation: -1
Reviewer's confidence: 3
Recommend for best paper: no",-1,,,,,2/18/2018,22:46,no
563,154,42,Joxx Borxxxx,1,"(OVERALL EVALUATION) This poster describes a proposal for a service that finds and makes available surrogates of academic papers that are only accessible on subscription.

It seems a very interesting work, very relevant for this conference!","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2/13/2018,20:04,no
564,154,348,Joxx Smxx,2,"(OVERALL EVALUATION) The authors take on the challenge of identifying open-access (OA) versions of paywalled research articles, with a set of ""Computer and Electrical Engineering"" papers as their baseline. They name the tool for finding such papers ""Surrogator"" and note that determining what is an acceptable substitute involves a certain amount of subjective determination such as allowing for 'nearly the same content' versus 'exactly the same content'. The distinction also applies to article title and list of authors. A feature of their tool is that it allows the user to upvote or downvote a specific result. They compare their tool with Google's Scholar tool, pointing out that Scholar only finds OA versions if the match of title and author are exact whereas their tool finds near-match results. 

This is a really interesting tool, one that would be helpful to many researchers/users. The presentation is reasonably articulate, with no serious grammar/language errors. Also, the approach is clearly explained and the results worth consideration. The paper would be a good addition to the JCDL poster session.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I wonder how ACM would feel about presenting a method that while potentially ""enriching"" a DL simultaneously potentially ""impoverishes"" its paywalled content.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/15/2018,11:40,no
565,154,404,Dxx W,3,"(OVERALL EVALUATION) The poster presents a lightweight tool called Surrogator to automatically identify open access surrogates of access-restricted scholarly papers present in a digital library. From the evaluation, it is proved that the system is effective. However, the evaluation of the system is too simple. It should set a baseline to compare with other algorithms and more metrics should be used for the evaluation.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/18/2018,5:35,no
567,155,42,Joxx Borxxxx,1,"(OVERALL EVALUATION) The subject addressed by this proposal is confusing... In simple terms, and generalizing what I could perceive from the text, it seems about to understand the worldwide legal frameworks on the reuse of structured information (the authors name it ""metadata"") from third-parties...

If that is the purpose, it is interesting and relevant! But one cannot be sure from what can be read in the proposal...","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/23/2018,10:34,no
568,155,348,Joxx Smxx,2,"(OVERALL EVALUATION) This poster proposes a decision-based flow chart for navigating copyright rules as they apply to metadata maintenance and enhancement. The discussion is straightforward, but does need some improvement. For example, the flow chart in Figure 1 is inside-out, i.e., positive phases should flow directly down and negative flow to the side but the authors have all the positives flowing left and the negatives flowing in sequence downwards. A minor detail that does enable the graphic to fit into the allowed column space. More important, though, is that the flow chart only ends in ""OK to use"" and never in any negative result. This detail should be fixed or at least properly annotated to explain the perfect conclusion. Figure 2 has a spelling error in the top box (creats --> creates) which should be fixed.  I think the JCDL community would find this approach interesting and that the authors would gain valuable feedback. With these 2 items fixed, I would make it a strong accept.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/14/2018,15:41,no
569,155,404,Dxx W,3,"(OVERALL EVALUATION) The poster presents a copyright roadmap for heterogeneous metadata interoperability in integrated information systems. It is probablly a part of a project, therefore, it is hard to understand what are the research questions that paper wants to answer. Some key issues should be presented based on the evidences. How the roadmap comes out? Whether it can be tested? What are the findings of the research? The paper needs to write more clearly.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/18/2018,0:32,no
571,156,348,Joxx Smxx,1,"(OVERALL EVALUATION) The authors look at the differences between professional (i.e., librarians) use of video search tactics against those of regular users (undergraduate students). They found, not surprisingly, that there are several key differences particularly when a search fails to produce the desired/expected results. The authors conclude that there is considerable room for improvement in video search tools and training.

Because of the somewhat thin content, the poster is really not quite ready to be accepted even though the underlying research concept appears sound. The results are predictable, and the sample size is too small and likely too homogeneous (a single university's library). Still, there is some interesting work that can come out of their proposal potentially leading to more useful development. The area of video search and retrieval seems like an area worthy of deeper investigation.

I do have a few suggestions: First, the Research Method section should be expanded with more discussion about their approach. Second, a section on ""Future Work"" or ""Next Steps"" should be added to clearly lay out their plans for future research on the topic. This part is essential for a poster since it lays out how the researchers intend to pursue the next stage of their work. And finally, some discussion about the limitations of the sample size ought to appear in their section on results. There should be plenty of room for these if the authors reformat using the JCDL template.

In summary, the authors shortchanged themselves by failing to include sufficient justification of their approach and by artificially limiting the content length by using the wrong template.

A couple of items that need correction:

1. The authors are not listed on the PDF of this paper. Actually, the submission did not have the correct JCDL publication template. The template used is single-column, larger font, with different spacing and structure which reduces available print area. The authors would have more room for discussion if the proper template is used. 

2. The authors use both written and numeric quantities. Example ""Twenty-four (24) participants "" --> It makes for awkward reading. I suggest using one or the other, not both.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I think I mistakenly clicked ""2"" instead of 1 during my initial review since my hand-written margin notes have ""1 or 0"" on the paper. Whatever the reason, I have revised my score down to reflect the many issues that are present in this poster. Even though, as I've said elsewhere, I am usually more focused on a poster's concept than its content, there are enough issues with this submission that it warrants only a borderline rating.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/20/2018,19:44,no
572,156,404,Dxx W,2,"(OVERALL EVALUATION) The poster studies over twenty academic library users?€? interaction behaviors in two typical video platforms. But I am wondering what are the research goal to compare the searach behaviors between undergraduates and librarians? What is the search task? What are the two video platform? Are they different? What do you mean ""interaction tactics""? How to calculate it? The data analysis is descriptive and simple. I do not see much implications of the study. And the poster is not presented in ACM format as the conference requires.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/18/2018,1:58,no
573,156,111,Nixxxx Fexx,3,"(OVERALL EVALUATION) The paper does not follow the SIGIR template so it is not possible to say whether it complies with the page limit.

The paper conducted a study to see what are the most common interaction with online video contents for students and librarians.

The paper reports just basic statistics about the most chosen options but not particular insights or design suggestions are derived from them, so it is not clear what is the contribution of this paper.","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2/18/2018,22:51,no
574,157,42,Joxx Borxxxx,1,"(OVERALL EVALUATION) Wikidata is nowadays a very relevant source of information.
This poster reports the findings of a project aiming to understand the practices of wikidata curators. 
It looks very interesting, which can provide a very good poster and attract discussion in the conference!","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2/13/2018,20:17,no
576,157,3,Trxxx Aaxxxx,2,"(OVERALL EVALUATION) This poster abstract presents preliminary findings from a study on knowledge curation activities in Wikidata projects. Content analysis of discussions is used to identify actions. The poster topic is of interest to the conference, but contribution in this very vague. The activities that could have been of interest are the ones that are mentioned as ""mapped to the DCC occasional activities of reappraising and dispose"", but this is not further explained. The activities that are further explained are more administrative ones that do not really count as knowledge curation activities. The section on tools is also rather out of the scope indicated by the title. 

The abstract needs to be shortened because it prints as 3 pages.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Given that I find the actual contribution of this very preliminary and vague, I gave it a low score.
It is not a competitive poster as it is presented now, but I assume that an improved presentation of the findings and discussion could have saved it.

I revised my score to borderline rather than weak reject, to be more in line with the other reviews.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/15/2018,11:52,no
578,157,348,Joxx Smxx,3,"(OVERALL EVALUATION) The authors apply Activity Theory to the actions of people involved in the curation of Wikidata WikiProjects. Given that both Activity Theory (AT) and WikiProjects are group-based and action-oriented, using AT to examine such processes seems like a natural approach. However, the explanation of the project left me somewhat confused. Three research questions were identified in the introduction, but the authors' explanation of their methodology was not clear to me. The Findings & Discussion section did attempt to answer each of the research questions but the results were not particularly compelling; I'm not sure new knowledge came out of the experiments. Perhaps the authors could develop an explanatory diagram to help clarify their process and goals. Still, I recommend accepting the poster at JCDL because it will probably help the authors solidify their goals, evaluation processes, and explanation of results. 

Grammatically and linguistically, the paper is fine. A couple of very minor edits:

Section 3.1: ""necessary to re-organizing"" --> necessary to reorganize
Multiple Sections: ""as well as, "" --> trailing comma is not needed

Also, the template used is incorrect (WOODSTOCK '97 ??). This resulted in an odd 3rd page with a header from that template. The authors need to recompile using the JCDL template.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Forgot to mention the weird template issue. Sorry about that.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/15/2018,10:39,no
579,158,92,Mixxxx Doxxxx,1,(OVERALL EVALUATION) An interesting novel approach relevant to the conference.,"Overall evaluation: 3
Reviewer's confidence: 2
Recommend for best paper: no",3,,,,,2/17/2018,14:05,no
580,158,111,Nixxxx Fexx,2,"(OVERALL EVALUATION) The paper describes the application of Signpostings to scholarly repositories to facilitate the navigation of agents into landing pages. It provides four patterns for object identification, author information, bibliographic metadata, and boundary information.

The proposed approach is light-weight and can represent an improvement in accessing resources.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/18/2018,22:57,no
581,160,42,Joxx Borxxxx,1,"(OVERALL EVALUATION) This submission describes a service for the preservation of web resources, on demand, and to be made on a structure of distributed archives.
This looks very interesting and appropriate for the conference!!!

jlb","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2/13/2018,20:28,no
582,160,92,Mixxxx Doxxxx,2,(OVERALL EVALUATION) A useful and informative poster proposal - however the text is very technical which might be prohibitive for those conference participants who are not familiar in detail with web archiving.,"Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/18/2018,19:44,no
583,160,111,Nixxxx Fexx,3,"(OVERALL EVALUATION) The paper describes the ArchiveNow tool to generating WARC files from Web sites and seamlessly storing them into Internet Archive, Archive.is, WebCite, and Perma. The tool is available as CLI, Web-based UI, python module and docker container.

ArchiveNow is an useful tool which can be used in a very flexible way, help lowering the barriers for Web archiving.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/18/2018,23:02,no
584,161,3,Trxxx Aaxxxx,1,"(OVERALL EVALUATION) This poster gives a reasonably good introduction to alternative solutions for implementing banners for web archive display, and presents the specific solution designed and implemented by the authors. The abstract is well structured and easy to read. The actual scientific contribution is somewhat limited due to the lack of some kind of evaluation, but sufficient for a poster.","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/16/2018,14:37,no
585,161,92,Mixxxx Doxxxx,2,"(OVERALL EVALUATION) The topic is relevant to the conference. 

How exactly the values in Table 1 had been obtained?","Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/18/2018,12:02,no
586,162,37,Joxxxx Bxx,1,"(OVERALL EVALUATION) The authors introduce a research paper recommender system for students and integrate the recommender system into the eBook system BookRoll. They evaluate the recommender system and present some statistics about how often students accessed the recommender system. 

Overall, the paper is easy to follow, though the English could use some proofreading and editing. However, the evaluation is focused solely on the one recommender system developed by the authors. It would add a lot of value if the authors compared their system and algorithm against some other baselines or would, for instance, not only use HCF-IDF but A/B test different weighting schemes. 

Minor issues: I know that space in a poster is limited but only two references (one being a self-reference) seems not enough. I suggest describing related work in more detail. For instance, there is interesting related work on recommending university courses to students https://dl.acm.org/citation.cfm?id=1297254 and for the statement ""In the last decades, a lot of works have dealt with research paper recommender systems to facilitate researchers to and information"" I suggest adding a reference to back up this claim. One potential paper to cite is https://link.springer.com/article/10.1007/s00799-015-0156-0 .","Overall evaluation: 0
Reviewer's confidence: 5
Recommend for best paper: no",0,,,,,2/15/2018,9:41,no
587,162,339,Sanxxxxx Sixxx,2,"(OVERALL EVALUATION) This system could be very useful to the students. The authors might also try to augment other materials such as relevant blogs, youtube videos to improve the reading experience. The authors can look into this as well - Agrawal, Rakesh, et al. ""Navigational aid for electronic books and documents."" U.S. Patent No. 9,720,914. 1 Aug. 2017.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/16/2018,10:57,no
588,162,404,Dxx W,3,"(OVERALL EVALUATION) From the title of the poster, I consider the paper should talk about how to design a research paper recommender system based on the E-book system. However, it is not the case. The poster actually presents how university students use the recommender system. I suggest the poster to re-name the title. If the system is introduced in another paper, for the evaluation, I think the paper should do a deeper research on the users' behavior of using the recommender system, rather than some basic statistic. For example, why people use or not use some of the system's fuctions, how people achieve from using the system, what are the effect of the recommendation, etc, which are important research questions.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/18/2018,4:56,no
589,163,3,Trxxx Aaxxxx,1,"(OVERALL EVALUATION) This poster abstract presents the preliminary results from a project using word embeddings for song lyric classification.
The topic is of interest to the conference and results are sufficient and suitable for a poster. The abstract is well structured and presented.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/14/2018,15:03,no
590,163,348,Joxx Smxx,2,"(OVERALL EVALUATION) This paper looks at methods for the semantic classification of song lyrics, comparing the usual TF approach against a data-driven vector approach. The experiments are described well, and for once the figures convey something useful and more effectively than several paragraphs of explanation would do. This would be a nice addition to the JCDL poster session. 

A couple of very minor niggles: 

Sect. 2.2: ""this preliminary experiments"" --> either ""these"" or singular ""experiment""
		""almost as good as"" --> technically, it should be ""almost as well as""

Sect 4: ""the word vectors kept topic information quite a lot"" --> needs rephrasing","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2/14/2018,21:00,no
591,163,92,Mixxxx Doxxxx,3,(OVERALL EVALUATION) This proosal develops further a research topic which had been previously discussed at JCDL. I expect this woudl be developed into an interesting and useful poster.,"Overall evaluation: 1
Reviewer's confidence: 3
Recommend for best paper: no",1,,,,,2/18/2018,11:43,no
592,164,348,Joxx Smxx,1,"(OVERALL EVALUATION) The authors take a new approach to evaluating mobile library user satisfaction, and also compare differences in expectations between users of academic mobile libraries and users of public mobile libraries. Their research focuses on 4 dimensions of service quality (resource, environment, interaction, outcome) to determine user satisfaction. Surveys being what they are, an accurate model for evaluating user satisfaction is almost a digital library's Golden Fleece (or Holy Grail). The paper is reasonably well written, although with the usual confused use of articles (the, a, an) and occasional odd phrase. This would be a good addition to the JCDL poster session.

A few items that should be cleaned up:

Abstract: ""To notify the meaningful factors"" --> to identify the meaningful factors

Pg 1: ""on the one-time evaluation"" --> ?? for its one-time evaluation (unsure exactly what is intended here)

Pg 2: ""strongly related users' satisfaction"" --> strongly related to users' satisfaction

Also, the authors used the wrong LaTeX publication template. It needs to be reformatted with the appropriate JCDL template.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Looking at the template used to generate the PDF, I suspect the correct template will result in a poster that is about 2.5 to 3 pages long. Not sure if the committee cares about that detail.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2/14/2018,20:47,no
593,164,339,Sanxxxxx Sixxx,2,(OVERALL EVALUATION) Overall the paper is easy to follow albeit there are a few typos. The authors perform a thorough study on user satisfaction with mobile library services. I would have preferred a discussion on how the authors intend to use these observations to improve the mobile library services.,"Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/16/2018,11:36,no
594,164,92,Mixxxx Doxxxx,3,(OVERALL EVALUATION) This is an interesting and relevant topic but I found the structure confusing as well as the overall content and expression. Needs to be rewritten in a more clear manner.,"Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/17/2018,13:58,no
596,165,348,Joxx Smxx,1,"(OVERALL EVALUATION) The authors use a combination of TF-IDF, burstiness (applied to terms), term growth rate, and other factors as the basis for a model for determining what areas of research areas of science are ""hot"" (growing and/or being exceptionally productive). The paper would be stronger if not for a few issues. For example, they make a couple of claims without backing them up either in the text or in the references. First, that this model is ""one of the best practices for content-based service in the library area""; and second that their approach overcomes ""disadvantages of the Fuzzy-AHP method.""  The diagrammed model is not especially informative and seems mostly to just box each of the components without really showing an extractive result. Finally, there are only 3 references cited. Others that refer directly to their claims should perhaps be listed.

For these reasons and the many minor grammatical issues, this paper falls short of the mark for acceptance to the posters section of JCDL. Although I believe that the authors would find the feedback at the conference valuable for moving their model forward, it needs to be more clearly defined and justified before it is ready for presentation. I also suggest any future submission be edited ahead of time by someone with a stronger grasp of written English.

A few of the many corrections needed:

Pg. 1: ""what has led to"" --> which has led to
          ""the word frequency statistics"" --> word frequency statistics
          ""among other areas, finance,"" --> among other areas such as finance,

Pg. 2: ""TF-IDF mothod"" --> TF-IDF method

Numerous misuse of singular-plural as well as use of articles ""a"" and ""the"" where standard English grammar doesn't use them. However, the paper is perfectly readable and understandable so these are perhaps not important in this case.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) When it comes to posters, I tend to be somewhat less critical of content and more focused on concept. But after considering the other reviewers' comments, I revised my score down. Their objections are on target I think, and it's not clear that asking the authors to revise will produce a significantly better result at this time.","Overall evaluation: 0
Reviewer's confidence: 4
Recommend for best paper: no",0,,,,,2/20/2018,19:17,no
597,165,339,Sanxxxxx Sixxx,2,(OVERALL EVALUATION) The writing is not always easy to follow and needs to be improved. The methodology proposed in the paper could be useful in understanding trends but I am not sure how it could be used in libraries. A discussion on it could make the proposal better.,"Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/16/2018,12:43,no
598,165,92,Mixxxx Doxxxx,3,"(OVERALL EVALUATION) A proposal which focuses on a theoretical model of popularity of scientific and technological resources but does not offer any proof of concept. 

The language needs serious revision, e.g. phrases like ""heat calculate"", ""A3 Suddenly index heat"" are not grammatically correct.","Overall evaluation: -2
Reviewer's confidence: 3
Recommend for best paper: no",-2,,,,,2/18/2018,11:53,no
599,166,339,Sanxxxxx Sixxx,1,(OVERALL EVALUATION) This is a very nicely written paper. The idea of robust link is also very interesting. I have nothing much to add.,"Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/16/2018,13:09,no
600,166,92,Mixxxx Doxxxx,2,(OVERALL EVALUATION) Well structured and clear proposal which is relevant to the conference audience.,"Overall evaluation: 3
Reviewer's confidence: 3
Recommend for best paper: no",3,,,,,2/17/2018,13:54,no
601,167,398,Nicxxxxx Woxx,1,"(OVERALL EVALUATION) This panel has a rich and diverse composition of panelists. Its central questions have the potential to engage both practitioners as well as iSchool faculty. Of the three proposed panels, this might appeal to audience members more interested in institutions and policies as opposed to the more technically rigorous topics. If venue and timing considerations are not extremely limited, I suggest acceptance of this panel to make for more diverse audience options.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,2/15/2018,20:17,no
602,167,269,Ixx Milxxxx,2,"(OVERALL EVALUATION) This is a very strong panel submitted to JCDL 2018. The proposal has done an excellent job of outlining the field of intelligence analysis, its relevancy both to the library profession and the Joint conference on Digital Libraries, and have outlined the current problem of the lack of services and/or ""clear, concrete and operable action plans."" Accordingly, the convenors have brought together four panelists and two moderators to have this discussion. The questions are well put and get at problems of relevancy, the role of the library, and how we could better develop new graduates and infrastructure to serve this problem.

The panelists are all well suited to this panel and bring a variety of perspectives to bear.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I was very impressed with this panel!","Overall evaluation: 3
Reviewer's confidence: 4",3,,,,,2/16/2018,18:57,no
603,167,160,Jxx Hxx,3,"(OVERALL EVALUATION) This discussion proposal would seem to be relevant to JCDL. The topic is certainly timely regarding information analysis needed to understand the amount of misinformation in the current news-cycles. The proposed discussion consists of factors like education of information professional when considering information services for intelligence analysis.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I would have been more comfortable accepting this discussion if it included someone with public service experience in a research library. I couldn't tell from the bios if any were reference librarians. Given that exclusion, I wonder if the discussion will paint a realistic picture for what types of services could be provided?","Overall evaluation: 0
Reviewer's confidence: 4",0,,,,,2/19/2018,11:42,no
604,167,311,Edxx Rasxxxxx,4,"(OVERALL EVALUATION) This is a well-argued panel proposal and seems solid---the participants have agreed to take part, and seem well qualified to discuss this topic.  They come from a range of institutions and backgrounds.  (Not sure why two moderators are needed, though.)

On the other hand, the topic is quite specific and may not be of interest to a wide segment of conference attendees.  The link to digital libraries is a bit tenuous (though the proposers could have made a stronger case in this regard).  The topic is most likely to be of interest to digital library practitioners, a group which has not been as numerous as conference organizers might have liked---so this could be a good way of providing them with some targeted content.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I have some reservations about the broad appeal of this topic... probably in the ""accept if there is room"" category.","Overall evaluation: 1
Reviewer's confidence: 4",1,,,,,2/24/2018,0:53,no
605,168,3,Trxxx Aaxxxx,1,"(OVERALL EVALUATION) This contribution presents the data model behind the work set model that is implemented for HTRC digital library. The introduction is mainly emphasizing the user needs and information behavior of researchers, and the implementation part is rather generic and technically oriented. I believe that the presentation could benefit from a better connection between the parts eg a more user/usage-oriented description of the RDF ontology. However, the topic is interesting for the conference and should make an acceptable poster.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/16/2018,14:45,no
606,168,92,Mixxxx Doxxxx,2,(OVERALL EVALUATION) A proposal which is definitely within the scope of the conference and would be a useful contribution.,"Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2/17/2018,13:48,no
607,168,404,Dxx W,3,"(OVERALL EVALUATION) The paper presents three key objectives for worksets and the implementations at the HathiTrust Research Center. The poster looks like a work report of the HathiTrust Research Center, rather than a research paepr. It should give evidences for why there are three objectives, how they are implemented, and what are the effects?","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/18/2018,4:42,no
608,169,42,Joxx Borxxxx,1,"(OVERALL EVALUATION) This poster reports the finding of a study on the present offer of Research Data Services among libraries in the USA.

It looks very interesting and appropriate to this conference!","Overall evaluation: 3
Reviewer's confidence: 5
Recommend for best paper: no",3,,,,,2/13/2018,20:41,no
609,169,48,Juxxxx Bruxxxx,2,"(OVERALL EVALUATION) The poster examines the state of three aspects of Research Data Services (RDS); namely, Information Access, Technical Support, and Personalized Consultation using 50 academic library websites of North America. They found that almost all academic libraries have support for all three aspects of RDS in some capacity. However, the level of support for the personalized consultation is underrepresented and often not documented clearly.

These findings are worth getting published. I hope it will encourage the digital libraries community to bring in more diversity in their tool sets and more clarity in the documentation of their services.

Following are a few comments for improvement:

- The alignment of the abstract is justified, but not rest of the paper. Please follow the style guide on this.
- The term ""Fifty"" can be replaced with numeric ""50"" in the abstract as numbers greater than ten are generally written numerically.
- Citation style is different from what is being used in JCDL and some other ACM conferences. For example, ""(Hey, Tansley, & Tolle, 2009)"" should have been cited as ""[4]"". This will save some space as well. Please refer to the style guide about it.
- There is a bare heading where sub-section 2.1 starts immediately after section 2. This is generally not considered a good writing style. Having a couple sentences under section 2 before beginning the sub-section would be great.
- The first used of the abbreviation ""IA"" is not expanded immediately, expanded in the next paragraph instead.
- The term ""coded"" can be misleading for general readers while ""labeled"" is well known. A brief inline description of the term might be helpful.
- Results are described in the text, but tables or charts can be easier to scan quickly. Also, some figures describing the procedure, when necessary, are generally helpful. However, the limited space could be considered as a constraint.
- Adding links to certain services such as ""LibGuide"" and ""DMPTool"" in the footnotes can be helpful too (if the limited space allows that).
- The use of ""very much"" in the conclusion is vague, unnecessary, and does not add any value.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,Sawood,Alam,salam@cs.odu.edu,360,2/13/2018,21:27,no
611,170,348,Joxx Smxx,1,"(OVERALL EVALUATION) The authors present a proof-of-concept for enabling the extraction of linked data using machine learning techniques. The process starts with an archive, applies data-tilling-services to the content, then uses  an RDF extraction method (Brown Dog). The resulting linked data is then available for building or adding to digital collections. Their primary proposal is that local extraction of the linked data is superior to remote services. 

The main problems are insufficient explanation of the methodology; lack of a good graphic to clarify the process, and no real data examples. The paper sounds more like the introductory section to a longer explanation of a (new?) linked data extraction utility; it should instead state the project goal, explain associated process(es), demonstrate experimental methods/support for reaching the goal, then present results-so-far along with future plans.  

Since the submission is only 1 page in length, there is plenty of room for including more data and better examples. Also, the authors did not use the correct JCDL submission format which would have given them even more page space for their discussion. These issues, together with a lacking bibliography, need to be addressed before this poster is ready for acceptance.

The authors have an interesting proposal, especially given the growth of linked data and the various approaches used in collections around the world. They appear to be at the very early stages, though and perhaps should consider doing a demonstration at a symposium or workshop in preparation for developing a more robust paper.

Other corrections: 
-- The LaTeX for the apostrophe in the Abstract (Brown Dog's) needs to be fixed.
-- What's up with page 2?
-- The bibliography is too sparse and is not properly formatted.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Once again I find myself giving more weight to a poster's concept than to its content. I'd like to see the project in action. In any case, the other reviewers' issues are justified, so I have revised my review to include the paper's various shortcomings. On further reflection, this poster might be better presented at a symposium/workshop than at the conference. 

The paper has a subtitle-link to an ""Extended Abstract"". Can't recall ever seeing that before and don't know if this is kosher or needs to be removed. But of course that would probably disappear if they used the correct JCDL template.","Overall evaluation: 0
Reviewer's confidence: 3
Recommend for best paper: no",0,,,,,2/20/2018,22:15,no
612,170,3,Trxxx Aaxxxx,2,"(OVERALL EVALUATION) This poster abstract describes the DRAS-TIC open source digital repository platform and a proof of concept prototype on machine-learning based data extraction. The presentation is in one page, without any references and appears rather unfinished. The text is mainly focused on describing what the the system is. The extraction part, which is the most interesting , is only described from a technology perspective. The topic can potentially be relevant for the conference, but the abstract itself is not suitable for publishing in the proceedings  in its current shape.","Overall evaluation: -2
Reviewer's confidence: 4
Recommend for best paper: no",-2,,,,,2/15/2018,12:10,no
613,170,63,Vitxxxx Casxxxx,3,"(OVERALL EVALUATION) The poster description is very concise, and therefore it is difficult to really understand what 
is the main purpose of the poster. 
Is it to present (and explain) what are ?€?DRAS-TIC collections?€?? 
Is it to present (and explain) how machine learning applications can extract RDF Linked data 
from DRAS-TIC collections? 
Despite the title, the description spends many more words on the first topic than on the second. 
If accepted, I would recommend to better focus the poster on the second one, as it appears to 
be more recent and more interesting for the Digital Libraries community.","Overall evaluation: 1
Reviewer's confidence: 2
Recommend for best paper: no",1,,,,,2/20/2018,21:46,no
614,171,42,Joxx Borxxxx,1,"(OVERALL EVALUATION) This poster describes a solution to classify resources as ""historical events"", to support those searching under these premises...
The perceived proposal does not seem too innovative considering the present state of the art, but the subject is relevant, and maybe the poster can be more clear than the present text... It is, anyway, relevant for this conference!","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/13/2018,20:45,no
615,171,343,Giaxxxxxx Silxxxx,2,"(OVERALL EVALUATION) This is a nice work describing a system for retrieving historical events where the user can search by using keywords, categories and time ranges. 
It's a well-written paper that may be of interest for the JCDL community.","Overall evaluation: 3
Reviewer's confidence: 5
Recommend for best paper: no",3,,,,,2/14/2018,13:38,no
616,171,404,Dxx W,3,"(OVERALL EVALUATION) This paper proposes an interactive online system for retrieving past event descriptions. The topic of the paper is interesting and relevant to the conference. However, some issues in the paper should be stated more clearly. What do you mean category relevance? It should be defined early in the paper. The authors assume a user query is going to represent a named entity in most of the cases. What are the reasons and references? From my personal experience, it is not always the case. Moreover, evaluation on the system should be given to prove the effectiveness of it.","Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/18/2018,3:55,no
617,172,42,Joxx Borxxxx,1,"(OVERALL EVALUATION) This poster describes a project to develop an infrastructure that can support researchers in experiments where data is used and created... 
Subjects like research data management, scientific workflows, metadata for experiments, etc., are still hot topics in our community, making this work very relevant!","Overall evaluation: 2
Reviewer's confidence: 5
Recommend for best paper: no",2,,,,,2/13/2018,20:51,no
618,172,3,Trxxx Aaxxxx,2,"(OVERALL EVALUATION) Well-structured and presented poster abstract about how explicit models of the scientific data can be used in scientific data repositories. Timely topic of interest to the conference. Would have liked to know more about the evaluation of the clustering e.g. the usability and quality of the clusters, but this is a minor objection to the paper.","Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2/15/2018,12:49,no
619,173,48,Juxxxx Bruxxxx,1,"(OVERALL EVALUATION) This paper proposes a textual analysis and comparison between scientific text abstracts and blog postings about the same texts. The authors provide an analysis of a variety of metrics (i.e., predictors) that appear to be related to complexity rather than understanding. It seems like quite a big leap to take the complexity scores calculated in this algorithm and tie it to blog author understanding. Further, the notion that blog post text and abstract text can be compared for understanding may even be too much of a stretch. 

This paper is a preliminary discussion that would benefit from additional input and discussion with the JCDL community, but has questionable initial methods.","Overall evaluation: 1
Reviewer's confidence: 4
Recommend for best paper: no",1,,,,,2/13/2018,2:58,no
620,173,339,Sanxxxxx Sixxx,2,(OVERALL EVALUATION) The paper is nicely written and easy to follow. The problem looked into is also very interesting. The authors generate a comprehensive score which is representative of how semantically similar the text from the blogs is to the abstract. I think a better idea would be to perform a human judgement experiment and classify whether an article has been comprehensively understood or not. Cosine similarity may not be a good indicator.,"Overall evaluation: 2
Reviewer's confidence: 4
Recommend for best paper: no",2,,,,,2/16/2018,14:10,no
621,173,404,Dxx W,3,(OVERALL EVALUATION) This study generates features from scientific text that represent some common text structures and builds five regression models to predict the semantic similarity between the scientific text and the textual content posted by the general about the same scientific text online. The results indicate the existence of a relationship between the scientific text and the public understanding of the text. The models can help readers to understand the scitific articles easier. The idea is novel and interesting. More features should be extracted for the prediction.,"Overall evaluation: 3
Reviewer's confidence: 4
Recommend for best paper: no",3,,,,,2/18/2018,3:31,no
622,174,139,Danxxxx Gxx,1,"(OVERALL EVALUATION) The authors of this poster-paper present a new approach, using graphical model to detect the impact that a document have or have no after its publication. They are also concerned to examine its evolution in time. Moreover, the results are promising, reflecting how the graphical model performs on this task. We are looking forward to seeing their future results.
The structure of this paper is clear and well presented, giving information about the technologies used. 
However, a few comments should be taken into account:
-	The method is shown to have preliminary good results, but there is still much work to be done (describe it briefly).
-	Proofreading is necessary to correct some minor errors (p. 2 ?€?Correlation between between topical prevalence?€?, ?€?can provides?€?).
The paper has a good practical implication.","Overall evaluation: 1
Reviewer's confidence: 5
Recommend for best paper: no",1,,,,,2/14/2018,16:01,no
623,174,339,Sanxxxxx Sixxx,2,"(OVERALL EVALUATION) This paper develops a graphical model to capture the temporal dynamics in the impact of latent topics from a corpus of documents. The paper is interesting but the authors should also look into the rich literature on evolution of topics (some of them are provided below) to improve on their model. Also the ""Results and Discussion"" section lacks details. 

1. Qi He, Bi Chen, Jian Pei, Baojun Qiu, Prasenjit Mitra, and Lee Giles. 2009. Detecting
topic evolution in scientific literature: how can citations help?. In CIKM.
ACM, 957?€?966

2. Yookyung Jo, John E Hopcroft, and Carl Lagoze. 2011. The web of topics: discovering
the topology of topic evolution in a corpus. In Proceedings of the 20th
international conference on World wide web. ACM, 257?€?266

3. Ziyu Lu, Nikos Mamoulis, and David W Cheung. 2014. A collective topic model
for milestone paper discovery. In SIGIR. ACM, 1019?€?1022.","Overall evaluation: 2
Reviewer's confidence: 3
Recommend for best paper: no",2,,,,,2/16/2018,14:54,no
624,175,184,Xixx H,1,"(OVERALL EVALUATION) The topic of this panel is a good fit for the themes of JCDL. The topic is timely as machine learning and artificial intelligence are hot topics in these days. The panelists are experienced on this topic, and hopefully can give insights from different perspectives. 

The proposal includes most required elements: a statement of learning objectives, a topical outline for the panel, identification of the expected audience, and a list of panelists and their bios. However, this proposal does not discuss any engagement techniques that will require specific physical or technical requirements for the local hosts. Assuming the panel does not require special physical or technical arrangement, it is acceptable.","Overall evaluation: 1
Reviewer's confidence: 4",1,,,,,2/18/2018,5:01,no
626,175,290,Pexxx Orgaxxxxxxx,2,"(OVERALL EVALUATION) This is generally an interesting, relevant topic to the JCDL community, in an area that the panelists are qualified for. However, the proposal is underdeveloped and overly broad. I would like to see more evidence of the specific directions that the panelists hope to address, and more detail about why this discussion is needed now in the DL community. Lacking focus, detail and strong justification, it is difficult to assess the appropriateness of this panel.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I'd be willing to defer to recommendations of other reviewers. These authors can do well with the topic and there can be an audience for them, but that's based on background knowledge and  not evident at all in what they submitted.","Overall evaluation: -2
Reviewer's confidence: 3",-2,,,,,2/21/2018,20:10,no
627,175,157,Juxxx Grxxxx,3,"(OVERALL EVALUATION) The topic is relevant to the JCDL community and seems ideally suited for a panel session format that encourages audience participation. It may help to include individuals on the panel who represent different institutional data governance perspectives and user communities. Karim Boughida would also be an excellent addition, to speak about the project cited and how it applies to machine learning in digital libraries. I like that authors address privacy concerns.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,2/21/2018,0:42,no
628,175,120,Edxxxx Fx,4,"(OVERALL EVALUATION) This panel has a good title, which should be appealing.
The three panelists have good records, coming from good places, and cover a range of skills and knowledge and experience.

The proposal is a bit vague and unclear. The writeup could mention some case studies and give examples of where ML has been shown to be helpful in DLs.  For example, there are some research DLs that have extensive analysis services, such as those connecting with Web archives, or in connection with the Hathi Trust. Since space is available, the document should be expanded with a few case studies.  Similarly, while there is mention of recommendation, personalization, and NLP, those are not explained, nor is there any discussion of ML techniques like classification, clustering, topic analysis, . . . There should be additions made about these.

Two of the panelists seem particularly interested in teaching and learning. If that will be a focus of the panel, the title should say that, and the writeup should make that more clear.  However, for the conference it probably is not that important a topic wrt ML and DL, so perhaps the set of panelists should be expanded.  It would not hurt to have someone doing ML research. It would be nice to have a few others reporting on successful DL enhancements through ML.

In short, some improvement is recommended in the writeup and in the panel makeup.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I like having panels but this one needs some help.  Changing the title, or adding some people, or both, would help.","Overall evaluation: 1
Reviewer's confidence: 4",1,,,,,2/21/2018,1:59,no
629,176,42,Joxx Borxxxx,1,"(OVERALL EVALUATION) This submission describes an environment to manage archeological data in an university campus...

However, only a short-text abstract has been submitted, with no PDF submission on the required 2 pages format accompanying it... Therefore it is not possible to provide a proper review for this submission (anyway, the case described in the abstract seems ""regular engineering"", always interesting due to the specific context of the project, but not innovative as a challenger or as a result...)","Overall evaluation: -3
Reviewer's confidence: 5
Recommend for best paper: no",-3,,,,,2/13/2018,20:56,no
630,176,348,Joxx Smxx,2,"(OVERALL EVALUATION) No paper has been attached to this submission. The only content available is that of the submission form's Abstract field. Without more information, it is not possible to properly review this article for acceptance. Since the submission process expects a PDF submission, and none is provided, the only evaluation possible is ""reject.""

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Did this paper get submitted but fail to be properly attached? Happy to revise the review above if the actual paper can be located.","Overall evaluation: -3
Reviewer's confidence: 4
Recommend for best paper: no",-3,,,,,2/14/2018,16:57,no
631,177,380,Giaxxxx Tsaxxxx,1,"(OVERALL EVALUATION) This is a very well written panel proposal that aims to explore the current transformations in digital libraries as platforms for computational works and processes, especially in the area of digital humanities. It establishes well on the critical shift in understanding content and how the material in digital libraries becomes data for processing and analysis. It is interesting of course that the panel is going to give examples from two initiatives and, therefore,it will not be purely theoretical. The panel line up is very interesting and -to my knowledge- most of the panelists are experienced researchers of the field, equipped with good communication skills. There is always the fear of making it work for the advertisement of the platforms, but this is up to the coordinator to make it sure it will not happen, but they will be just examples of a larger picture.","Overall evaluation: 3
Reviewer's confidence: 4",3,,,,,2/16/2018,7:02,no
632,177,366,Maxxx Taxxx,2,"(OVERALL EVALUATION) This panel is on textual and image mining methodologies for applications of large scale digital libraries, especially on HathiTrust.
The goal of the panel is very relevant to the conference and helpful for many researchers around the field of digital libraries. Variety of speakers are able to provide an insight into use cases.
Audiences could learn and discuss about future possibilities and directions from such technologies.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,2/20/2018,5:41,no
633,177,283,Hexxx Nexxxx,3,"(OVERALL EVALUATION) The panel intends to encourage a discussion about the topic ""collections as data and data as collections"" and ""the potential for such data as collections to contribute ... to knowledge and integration across societies, disciplines, and systems."" The panelist have a very good experience regarding ""data collections"" in the context of digital libraries (e.g. AIDA, HathiTrust Research Center and Digital Library etc.). The potential outcomes are very interesting and promising. I am just wondering whether some legal aspects should be discussed as well as due to some other experiences researcher in general are not sure how to use digital collections without having legal problems.","Overall evaluation: 3
Reviewer's confidence: 5",3,,,,,2/20/2018,14:36,no
635,178,69,Boxxx Caxxxx,1,"(OVERALL EVALUATION) The topic of the poster is especially relevant in these days when we hear so much of the filtering of information provided to an individual based on algorithmic assessment of what the user would want to see or would react to in a predictable way.  Any effort to expand the user's range of input is interesting and worth exploring.  This project is appropriate to JCDL, and should be a source of good discussion at the poster session.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) My default on posters is ACCEPT as long as the topic is relevant and the submission is well done.  This one, I would have said accept in any case.","Overall evaluation: 3
Reviewer's confidence: 3",3,,,,,3/25/2018,17:16,no
636,178,340,Sanxxxxx Sixxx,2,(OVERALL EVALUATION) This work reports on the evaluation of topic space recommendation model as an alternative to the personalized algorithms to circumvent the problem of filter bubble. I am not sure about the evaluation. The author needs to verify whether the users preferred the proposed model over the existing recommendation algorithms. The motivation of the paper was also not clear.,"Overall evaluation: -1
Reviewer's confidence: 4",-1,,,,,3/25/2018,13:52,no
637,178,64,Vitxxxx Casxxxx,3,"(OVERALL EVALUATION) The topic is interesting but, based on the
description, it appears difficult to re-use
the experience of this project in another
environment.
Also, probably due to paper length limitations,
there are not enough technical details for a real
appreciation of the system. The sketchy descriptions
of the main functions only give a glimpse of the
whole system.
In conclusion, while the topic is relevant for the
objectives of the conference, this poster risks of
being interesting and useful to a minority of the
attendees.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Entered by Richard Furuta","Overall evaluation: -1
Reviewer's confidence: 1",-1,,,,,3/26/2018,23:14,no
638,178,349,Joxx Smxx,4,"(OVERALL EVALUATION) This paper looks at ways to improve the recommender model so that items related to the researcher's topic will be part of the search results/recommended-reading list. Of course, this is not itself novel but what this author is examining is the problem of overcoming systematic bias, which tends to keep users within a relatively narrow range of readings. The author likens this to a library patron working within a particular area of the book stacks unaware that other pertinent information may exist in another section of the library. The idea is that ""personalization algorithms"" tend to keep one within a narrowly defined set of results whereas the proposed Topic Space navigation approach allows for expanding the list to more-distantly-related but relevant items. 

Unfortunately, the study size is relatively small (18 mobile users) and only a few of the library's many resources were accessed. So it is unrealistic to extrapolate from this experiment to the larger library audience. Still, the concept (escaping confirmation bias, expanding exploration frontiers) is something that is important for digital libraries and needs to be addressed. This project is clearly still in its early stages and needs some maturation in methodology. Even so, I think the project is acceptable as a Poster submission and that both the author and the conference audience will find the idea of topic space navigation worthwhile.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Although this is early-stage (even for a Poster submission), I think the topic and the author's work will be a useful addition to the conference. So I'm giving him a bit more slack than I might otherwise...","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/27/2018,2:45,no
639,179,126,Nuxx Frxxx,1,"(OVERALL EVALUATION) This poster fits very well in the context of JCDL, and presents an interesting approach that will likely gather the interest of many JCDL attendees. The paper is also written with adequate quality for publication.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,4:50,no
640,179,69,Boxxx Caxxxx,2,"(OVERALL EVALUATION) This project is interesting in a number of ways, including the incorporating of the community in the design process.  The multi-disciplinary aspects of the project are appealing.  A small note -- should sections 3.2 through 3.7 be subsections of 3.1?","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,13:46,no
641,179,64,Vitxxxx Casxxxx,3,"(OVERALL EVALUATION) The topic of community-driven digital libraries
is interesting, but the project is described at such
a high level that it is difficult get inspiration
for replication in another context.
It might be better to present it in a more
specific context.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Entered by Richard Furuta","Overall evaluation: -1
Reviewer's confidence: 1",-1,,,,,3/26/2018,23:15,no
642,180,405,Dxx W,1,"(OVERALL EVALUATION) This paper used empirical qualitative research to discuss the development of data infrastructure and digital libraries, aiming to building smart community. This paper provided interesting design thinking for digital library. 
Suggestion:
(1) It will be better to present the structure diagram of this ?€?integrated cyber-physical-social system?€? in the poster. 
(2) in the session 3, there are sentence like ?€?the participants described?€??€??€?according to the participants?€??€??€?there are three main concepts of smart digital library emerging from the interviewees?€?. It is not clear. Are all participants think so? (How many participants think so? ) I think there should be simple quantitative statistics on frequency of participants.
(3) it will be better to present the questions of the interview in the poster.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,3/24/2018,21:47,no
643,180,69,Boxxx Caxxxx,2,(OVERALL EVALUATION) The topic is very appropriate and adds an interesting dimension to the study of digital libraries.  Looking at inputs from many types of sensors makes the project current and adds interesting new areas of consideration.,"Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,13:51,no
644,180,64,Vitxxxx Casxxxx,3,"(OVERALL EVALUATION) The topic is very interesting and up to date, but the
poster should better capitalize on the three concepts
briefly outlined in Section 3, rather than insisting
on the ""buzz words"" Intelligent Infrastructure and
Human-Centered Community.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Entered by Richard Furuta","Overall evaluation: 1
Reviewer's confidence: 1",1,,,,,3/26/2018,23:16,no
645,181,405,Dxx W,1,"(OVERALL EVALUATION) This paper presented an entity relationship model, including main-entity, child entity and secondary entity. Then this paper presented a set of visualization scheme to display the relationship of entities and applied the scheme to the visualization of Chinese herbal medicine prescription dataset and paper dataset. In the use case, the relationship between multi entities, including paper, author, category, could be displayed visually and other information of the selected paper also could be showed in table. This research is practical.
Suggestion:
Some libraries have digital humanities platform in which the entities were extracted from paper resources or digital resources and the relationship of entities was displayed visually. What the difference between your research and those platforms?","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,3/24/2018,21:48,no
646,181,69,Boxxx Caxxxx,2,"(OVERALL EVALUATION) The poster will be very suitable for JCDL, focusing on visualization of relationships among entities in the data collection.  The poster submission has a number of illustrations that show that the final poster will be well done and interesting to the conference attendees.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,13:54,no
647,181,64,Vitxxxx Casxxxx,3,"(OVERALL EVALUATION) The title of the poster promises a Visualization scheme
for Multi-entity relationships, but in the description
it is very hard to identify what are the relationships,
and how they are visualized. The pictures provided are
almost unreadable and there is practically no description
of their content and meaning in the text.
As it is it would be of very limited interest to the
attendees of JCDL.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Entered by Richard Furuta","Overall evaluation: -2
Reviewer's confidence: 1",-2,,,,,3/26/2018,23:17,no
648,181,22,Suxxx Alxxx,4,(OVERALL EVALUATION) This paper introduces an entity relationship model that can visually display relationships of multi-entities.  The paper outlines the relationships used to create the visual query and presents a use case to illustrate this.  The paper presents figures and diagrams to illustrate the project and suggest that this project can be well-presented on a poster.,"Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,3/27/2018,0:28,no
649,182,69,Boxxx Caxxxx,1,(OVERALL EVALUATION) The poster addresses distinguishing entities and reports on both the theoretical approach and an experiment using industrial data.  The topic is appropriate and it is presented well.  It will make a poster of interest to JCDL.,"Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,13:57,no
650,182,340,Sanxxxxx Sixxx,2,"(OVERALL EVALUATION) This paper presents a clustering analysis-based approach for detecting
potentially mixed entities in a knowledge bases. The work touches on an important problem and the proposed method performs well although one needs to explore further. I would recommend to accept this work.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,3/25/2018,14:00,no
651,182,64,Vitxxxx Casxxxx,3,"(OVERALL EVALUATION) The poster fails to clearly describe the problem (Entity
mixture) and the methodology used to solve this problem.
It refers to triples, without describing what they are,
it provides an example of inconsistency without explaining
where the inconsistency is, it refers to clustering without
explaining how the clusters are obtained, it concludes that
two entities named Isaac Newton are mixed, without providing
the data leading to this conclusion.
It concludes that the methodology has been applied ""on two
data sets of industrial applications"", but no details (size,
entities, attributes) are provided for them.
As it is it would be of very limited interest to the
attendees of JCDL.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Entered by Richard Furuta","Overall evaluation: -2
Reviewer's confidence: 1",-2,,,,,3/26/2018,23:18,no
652,182,22,Suxxx Alxxx,4,"(OVERALL EVALUATION) This paper presents a methodology for detecting mixed entities for knowledge base construction and population. The methodology is well-described using equations, explanations and figures.  THe design of the test case, using two datasets of industrial applications is a good approach for proof of concept. This topic is within scope for JCDL.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/27/2018,0:57,no
653,183,405,Dxx W,1,"(OVERALL EVALUATION) This paper used grounded theory principles to analyze 75 articles which published from 2005 to 2016 and summarized seven main challenges to planning, deploying, and maintaining cloud services in libraries. This is a summation research.
Suggestion:
(1) It will be better to provide more detail about data processing (section 2). The coding basis or processing should be presented. 
(2) The publishing date of articles were from 2005 to 2016. I think the duration is too long. Maybe some challenges that proposed few yeas ago have been overcame. Maybe it is more interesting to study which challenge haven?€?t been overcame although it is more difficult.","Overall evaluation: -1
Reviewer's confidence: 3",-1,,,,,3/24/2018,21:49,no
654,183,222,Mxx Kexx,2,"(OVERALL EVALUATION) This poster submissions claims to identify challenges for libraries based on literature and experiences and does exactly that. However, beyond simply a review of the state-of-the-art, there seems to be little contribution of the submission in itself.

There were a few errors including ""Software a Service (Saas)"" missing ""as"" in the abstract and the simultaneous use of both ""e.g."" and ""etc."".

Because of the apparent lack of lack of contribution of the submission, I recommend this poster to not be accepted to JCDL.","Overall evaluation: -2
Reviewer's confidence: 3",-2,,,,,3/25/2018,18:48,no
655,183,43,Joxx Borxxxx,3,"(OVERALL EVALUATION) This poster reports the findings of a literature review concerning the deploying of libraries services in the cloud. This is a very interesting issue, which has had not enough coverage in the literature, indeed. Stating that, we can identify the following weaknesses in this work:
- In rigor, I believe the title should not be ""...Deploying Cloud Services in Libraries..."" but eventually ""...Deploying Library Services in the Cloud...""
- Not all ""services"" are the same... for example, storing only a catalogue in a cloud is not the same as storing the patrons database; contracting remote access to publishers contents might not be the same that storing its own digital contents in a storage cloud, etc. Instead of a careful analysis of this kind, the poster presents a fuzzy concept diagram that exposes the complexity of the issue, but does not contributes to its understanding... 
In conclusion, this seems work in progress that, in case it is accepted by the JCDL, it'd be important the authors could move it forward a few more steps and come to the conference with more substantial results...","Overall evaluation: 1
Reviewer's confidence: 5",1,,,,,3/26/2018,7:37,no
656,184,405,Dxx W,1,"(OVERALL EVALUATION) This paper proposed a novel solution, PCCRF(Publication Content and Citation Representation Fusion), for cross-language citation recommendation. This method was language independent and could be generalized between any other pair of language. This method was tested in a ?€?Chinese-to-English?€? recommendation task and the performance was better than other baseline algorithms. This research is practical and novel.
Suggestion:
(1) It will be more vivid to take an example of how to implement cross-language citation recommendation with PCCRF.  
(2) Pay attention to the tense. Some sentences were in the past tense and some were not.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,3/24/2018,21:49,no
657,184,222,Mxx Kexx,2,"(OVERALL EVALUATION) In this submission the authors propose a method for recommending citations across languages using a novel approach. The methodology appears sound and the contribution, as stated by the authors, is applicable beyond their Chinese-to-English experimental task. I believe this poster to be a good fit for JCDL and may spark interest from the international attendees of the conference.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,18:59,no
658,184,43,Joxx Borxxxx,3,"(OVERALL EVALUATION) This poster reports the findings of a project addressing the problem of cross-language recommendation, based on a semi-supervised random walk algorithm...
It is not clear what is the problem the work reported in this poster intends to address. The authors stress the purpose of ""citation"", but in fact the core of the problem is ""recommendation"", being any concern related with ""citation"" irrelevant (maybe the authors have that in mind in the objectives of the overall project, but that is not relevant for what is reported as results in this paper...).
Assuming authors can refocus this poster as a case of addressing a cross-language recommendation challenge, this can result in a relevant report of work in progress in that sense (I mean, please strip the text of repeated and unnecessary language and use the limited resource that are the two pages to provide more insight about your core work and results).","Overall evaluation: 1
Reviewer's confidence: 4",1,,,,,3/26/2018,7:59,no
659,186,126,Nuxx Frxxx,1,"(OVERALL EVALUATION) The authors present an interesting and relevant work for the digital libraries community, and presents an interesting approach that will likely gather the interest of many JCDL attendees. The paper is also written with adequate quality for publication.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,4:53,no
660,186,222,Mxx Kexx,2,(OVERALL EVALUATION) The authors of this poster submission describe an open source tool they created to extract the 5+1 journalistic questions from articles. The methodology is very clear and well-organized and the breakdown of the corpus into categories of articles is particularly interesting in evaluating their tool. I believe this poster to be a good fit for JCDL and recommend it be accepted.,"Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,19:12,no
662,186,43,Joxx Borxxxx,3,"(OVERALL EVALUATION) This poster describes the architecture of a system to extract events from news articles, considering the classic concepts of ""who"", ""what""; ""when"", ""where""; ""why""; and ""how"".
The poster does not describes all the techniques used in each step of the system, but since the solution is open-source, it makes it reusable and thus very relevant for the community (we expect more detailed documentation might provide details on all the steps).","Overall evaluation: 3
Reviewer's confidence: 4",3,,,,,3/26/2018,8:08,no
663,187,405,Dxx W,1,"(OVERALL EVALUATION) This paper analyzed the relationship between patent and rank of F500 companies. 68 companies were selected for this study and divided into three 50-years temporal buckets. The study found that (i) the patenting volume affects the F500 ranks more sharply in the long run,especially for bucket I (companies founded between 1851?€?1900) and (ii)Companies with better ranks tend to produce higher patents. Besides, this study found that bucket I neither cites nor receives citations from the rest of the buckets and bucket III (companies founded between 1951?€?2000) always cites from bucket II (companies founded between 1901?€?1950). It is interesting. This study can be used for revenue prediction of companies.
Suggestions:
(1) Pay attention to the tense. Some sentences were in the past tense and some were not. 
(2) It is better to explain why this study divided companies into three bucket and adapted 50-year interval. In section 2, this study found that ?€?Bucket I mostly constitutes of the consumer product companies, while bucket III mostly comprises information technology companies?€?. So why did not this study just divided companies according to the type of company? 
(3) Why the correlation of bucket I in 3.1 and 3.2? is the difference between bucket I, II and III have relationship with the types of company or foundation time? It could be better to discuss this.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,3/24/2018,21:50,no
665,187,222,Mxx Kexx,2,"(OVERALL EVALUATION) This work represents a study investigating correlation between publication of research patents and Fortune 500 rank. Claiming the first study of its kind that does not have the fallacies of previous older studies, the authors identify these correlative trends as they have occurred in time.

A few stylistic errors exist in this submission:
* Section 3.2: to'steal' needs a space.
* Throughout the submission there is inconsistent use of smart quote and straight quote. For example, ""current"" in Section 3.1 uses straight-quote characters while ""corner"" in Section 2 uses smart/curly quotes. The mixed use of single and double quotes is also a little stylistically odd.

This work seems off-topic for JCDL and thus I recommend rejection.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Based on Joan's review, I have modified my own to recommend rejection.","Overall evaluation: -1
Reviewer's confidence: 3",-1,,,,,3/27/2018,3:21,no
666,187,43,Joxx Borxxxx,3,"(OVERALL EVALUATION) This poster describes a project intended to correlate companies listed in the Fortune 500 index and their respective production of patents. This is interesting work, but nothing on it (methods, techniques, and results) is relevant for the scope of the JCDL conference...","Overall evaluation: -3
Reviewer's confidence: 3",-3,,,,,3/26/2018,8:20,no
668,187,371,Praxxxx Terxxxxx,4,"(OVERALL EVALUATION) This paper aims to identify the correlation between Fortune 500 companies ranks and their patent portfolios. It introduces the concept of temporal buckets (grouping companies by their founding years). A study of future ranking based on patent grants is provided. 

The set of Fortune 500 companies encompass different areas of industry, the paper does not make it clear of how prevalent patents and investment in research are for these companies (which could explain some of the results of both high and low correlation).

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I am not sure about the applicability of this paper in this forum. It seems to be linked more to econometrics or economics than digital libraries.","Overall evaluation: -1
Reviewer's confidence: 3",-1,,,,,3/27/2018,14:09,no
669,187,349,Joxx Smxx,5,"(OVERALL EVALUATION) The authors examine patent submissions and revenue of Fortune 500 companies and attempt to correlate the two, in the view that patents are indicative of innovation and that successful innovation would be reflected in revenue. They use a standard patent dataset as the source of patent submissions and the standard Fortune 500 published list of companies. They focused on patents filed during 2005-2017 and associated companies, eventually culling the list to 68 corporations.

This is the kind of paper that begs the question, What is a Digital Library? The Fortune 500 magazine could arguably be labelled a digital library, and perhaps the Reed Technology Index (source the authors used as to patents filed) as well. What I fail to see is the relevance of the paper to the JCDL community. The paper is interesting but there is not anything innovative that would bring an ""aha"" moment to digital library research. At best, it would be a topic for an economic or business research conference. In short, the authors seems to be doing standard analysis but just happen to be using a couple of quasi-DLs for their work. 

In addition, I am not convinced by their arguments regarding the companies they chose to analyze. It's one thing to eliminate corner cases; it's another thing to exclude inconvenient data, however ""noisy"" it may be. It seems to me that a contrasting analysis -- early patent cases analyzed against the historical success of those companies -- should be presented in order to strengthen their argument. Finally, revenue alone is only part of the story. Share price can also be a strong indicator of success and is a different type of ""revenue"" (company value). 

The bottom line is that, while the approach and analysis provide and interesting read the paper does not seem to be a good fit for the JCDL community.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) For once, a poster that was submitted using the correct template!","Overall evaluation: -2
Reviewer's confidence: 4",-2,,,,,3/27/2018,2:09,no
670,188,126,Nuxx Frxxx,1,"(OVERALL EVALUATION) The authors present an interesting and relevant work for the digital libraries community, and presents an interesting approach that will likely gather the interest of many JCDL attendees. The research underlying the paper is at a stage where personal discussion with other researcher working in the same area is valuable for the authors and interesting for JCDL attendees.
The paper is written with adequate quality for publication.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,4:51,no
671,188,340,Sanxxxxx Sixxx,2,"(OVERALL EVALUATION) This work presents an approach to explore news archives by automatically
generating semantic aspects for their navigation. More specifically, given a keyword
query as an input the authors utilize semantic annotations present in the
pseudo-relevant set of documents for generating the aspects. The paper is well-written and should be an interesting read a JCDL 2018.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,14:11,no
672,188,222,Mxx Kexx,3,"(OVERALL EVALUATION) In this work, the authors explore using semantic aspects of documents as a means of navigation by generating aspects to information queries. Their approach, as compared to previous work,  investigates modeling semantics of the annotations for the documents as a measure of salience.

The approach at generating the aspects is particularly interesting in this work that I think will be of interest to JCDL attendees in this research area. It's a little unclear of the methods that were used to generate the models (e.g., bag-of-words, bag-of-temporal-expressions) but some further documentation of these methods would give more credence in their results.","Overall evaluation: 1
Reviewer's confidence: 2",1,,,,,3/25/2018,20:30,no
674,189,349,Joxx Smxx,1,"(OVERALL EVALUATION) The author presents his experience using 2 different eye-tracking products as part of a larger study on the success of digital library user tools. He  describes his basis for selecting the two products and provides a comparative summary of his observations. Overall, the paper is reasonably well written with only a few small grammatical issues. There are a few things that could be improved and make the paper more usable by other, future researchers:

1) Break the discussion down into 4 sections: 
    -- the Abstract (essentially just the first paragraph as currently written)
    -- Methodology section (most of the remaining text)
    -- Comparison section (using the Table as the content)
    -- References section
    
2) Rework the Table so that it appears as a single, landscape-mode table in the paper. This will make it more readable. If necessary, abbreviate the content to fit

3) Improve the References section. Eye-tracking experiments have been going on for a long time now. It's hard to believe that this paper cites only a single textbook on the subject. Google alone has published lots of work in this area. At least 2 or 3 other reference works should have figured into this work.

4) Use the proper JCDL publication template. 

The rating of ""weak accept"" is really due to the above-mentioned weaknesses. With those improvedments, it would be rated much higher. The content is mostly there (except for the citations) and is likely to be of significant interest to the JCDL community.

Editorial note:  ""monthly long personal evaluation"" should be ""month-long personal evaluation""

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Sorry: Using vim to write the reviews and it converted window -wraps to EOLs, so I fixed it. Forgot I needed to do that.","Overall evaluation: 1
Reviewer's confidence: 4",1,,,,,3/25/2018,19:24,no
675,189,156,Juxxx Grxxxx,2,"(OVERALL EVALUATION) Eye tracking functionality for evaluating the user experience for digital libraries may be an area for further research in digital library research. However, the author does not adequately explain why comparable price informs the selection of the tools for product evaluation, how they came up with the list of evaluation criteria, or the digital library project that the eye tracking tool would be used to evaluate. The point at which the author is deciding between two tools seems to be too early in the study for presentation. To strengthen the proposal, authors would want to include this information in a paper that describes the larger digital library project and usability study completed.","Overall evaluation: -1
Reviewer's confidence: 2",-1,,,,,3/26/2018,14:01,no
676,189,85,Haxxx Daxx,3,"(OVERALL EVALUATION) Authors reviewed two eye-tracking tools considering their own experience. The text is well written and the comparison table is organized and informative. Eye tracking tools can be useful for assessing digital libraries interfaces. However, the appliance is not direct and the number of revised tools was limited in this paper. Because of this, I have concerns if this paper is relevant to the JCDL audience.","Overall evaluation: -1
Reviewer's confidence: 3",-1,,,,,3/26/2018,17:19,no
677,190,349,Joxx Smxx,1,"(OVERALL EVALUATION) These ""Anonymous Authors"" propose a method for refining sequential search queries from users by looking at the time-sequence history of similar queries in user logs. The paper is well organized moving from introduction through methodology to experiment and conclusion. However, the introduction is a bit weak in justifying the value of this line of research, i.e., as to how time diversification is useful to the user. The authors use VSM to determine semantically related query coverage and log volume-by-time for the temporal dimension.  The choice of AOL query log dataset surprised me, but at the same time I cannot definitively point to a better alternative source. Also, the use of the term ""history-oriented"" seems inaccurate, even in context when they really are referring to ""recently searched"" or ""also viewed recently"". Finally, the authors need to realize that it is important to sue the correct JCDL template when submitting a paper for review. 

The topic is interesting and should result in some valuable discussions about query diversification as an aspect of search refinement. Overall, I recommend the paper be accepted and that the authors consider how to more clearly explain their goals to those at the JCDL conference. 

A few minor edits:
	-- Abstract correction = ""demonstrate that the our proposed"" --> ""demonstrate that our proposed""
	-- Introduction correction = ""queries with multi-faced aspects"" --> ""queries with multifaceted aspects""
	-- Compile the paper using the proper JCDL template with the authors names, emails, etc. included. This will also remove the line numbering which shows on the existing submission.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,21:05,no
678,190,156,Juxxx Grxxxx,2,"(OVERALL EVALUATION) Authors describe a study of a framework for adapting query recommendations based on when a search query is conducted. In terms of replicability/methodology, it is unclear how authors acquired the AOL search log database and how ethics and privacy issues were addressed. I also had trouble understanding the methodology; the meaning of ?€?@10?€? for MRR and DCG is unclear (could be due to the lack of a description of MRR and DCG methods) and the use of brackets for numbers as well as references may have contributed to the confusion. It appears that the study described represents a significant research study, which I believe may not be well suited for a short poster presentation; a literature review section and a longer more detailed description of the methodology should help to address the problems presented. I would also recommend describing in a paper proposal how the new knowledge presented by the study benefits practitioners and researchers in the digital library community.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) I have concerns about data privacy issues with regard to the AOL query log data source. The study described also appears to represent a significant research study, which is not well suited for a short poster presentation. A literature review section and a longer more detailed description of the methodology would make a more thorough review of novelty, methodology, and comparison with prior work possible.","Overall evaluation: -2
Reviewer's confidence: 2",-2,,,,,3/26/2018,15:31,no
679,190,85,Haxxx Daxx,3,"(OVERALL EVALUATION) The author proposes a method for diversifying queries taking the time into account. This poster fits in the conference as digital repositories can take benefit of those information retrieval advances.

Just a minor edit:  put subtitle in Table 2.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/26/2018,16:10,no
680,190,22,Suxxx Alxxx,4,"(OVERALL EVALUATION) This paper addresses a new approach to make query suggestions more appropriate for the user. This topic area is relevant to JCDL attendees.  The methodology appears to be well-designed and is documented with copious equations. The paper would be improved if there were more concise explanations that clearly stated the purpose of each equation. The experiment setup and results outline a careful inquiry approach although it is unclear why the two assessors are appropriate experts to judge relevance of the queries since we do not know the basis of the queries. The paper would be improved with a more in-depth explanation of the results.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This paper needs copy editing for grammatical errors, some of which make it difficult to read.","Overall evaluation: -1
Reviewer's confidence: 3",-1,,,,,3/26/2018,18:33,no
681,191,340,Sanxxxxx Sixxx,1,"(OVERALL EVALUATION) This work proposes a statistical data
extraction method to acquire affiliation information directly from
university websites and solve the name extraction task. The paper looks into an interesting problem and proposes a nice solution. It would be an interesting read at the conference.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,15:08,no
682,191,251,Byxxx Marxxxx,2,"(OVERALL EVALUATION) This submission explores extraction of names from faculty directories resulting in 97% recall ?€? a solid result. To be published as a longer piece, comparison of the technique in operation and effect against other similar approaches would be in order. I would have liked to hear how the authors propose to use the extracted data to annotate or otherwise connect to the public bibliographic databases they mention in the first words in the abstract. It was not clear to me how that would be achieved given the need for disambiguation. However, details such as that will be nice to discuss at the conference.
I suspect the first word of the introduction should be spelled ?€?typical?€?.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/26/2018,0:04,no
683,191,371,Praxxxx Terxxxxx,3,(OVERALL EVALUATION) The authors have addressed a particularly challenging task of extracting author information across web pages.  A formulation of the underlying model is provided along with results for a selected set of 11782 pages.  The authors have addressed several of the review comments.,"Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/26/2018,19:36,no
684,192,349,Joxx Smxx,1,"(OVERALL EVALUATION) These ""Anonymous Authors"" propose using a relationship-classification system to show where new technologies are converging into new industries and opportunities. The authors base their analysis on the US Patent Classification System (USPC). Their analysis points to high-end equipment manufacturing as the one experiencing the most technology convergence. 

The topic is interesting but its applicability to JCDL is not obvious to me. I suppose that the USPC can be considered a digital library, and that therefore mining it for relationships that expose where technologies are converging would be useful for those looking to do some cutting-edge integration and/or leading technological innovation. But what about doing a discrete analysis from a historical perspective? That is, can you look at 20-year-old data and correctly forecast the technologies that actually did emerge? That is, compare the 2005-2015 dataset you did use with a set from, say, 1990-2005. Despite this and numerous grammatical/typographical issues listed below, I think this paper is worth considering as part of this year's JCDL poster session, where the authors may find useful feedback for continuing this line of research.

A few minor edits:
	-- Abstract correction: ""and increase dramatically"" --> have increased dramatically
	
	-- Introduction corrections: 
		""put forward analysis method"" --> put forward various analysis methods
		""and increase dramatically"" --> and have increased dramatically
		""patents are been employed"" --> patents have been employed
		""the paper of Geum Y [2] takes"" --> the paper by Geum et al.[2] which takes
		""ofIT and BT"" --> of IT and BT  {however, best rewritten as: ""Information Technology (IT) and Business Technology (BT)"" to clarify for the reader}
		""ofInternet,AI and robotics"" --> of the Internet, AI, and robotics
	
	-- Datase & Methods corrections: 
		""UC"" --> USPC. {even though you state that you are using ""UC"" as a shorter form of USPC, it seems pointless and requires needless transformation in the mind of the reader. Stick with USPC.}
		""Interpret the converging technology"" --> perhaps the word ""determine"" would be a better fit than the word interpret
		Check spacing between words, punctuation and parentheses. There are numerous places within the document where these run together.
		

And one major edit:	
	-- Compile the paper using the proper JCDL template with the authors names, emails, etc. included.The proper template should always be used when submitting to a conference.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Vacillating between yes-no on relevance, but overall the idea is interesting so giving it an Accept.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,22:32,no
685,192,43,Joxx Borxxxx,2,"(OVERALL EVALUATION) This poster reports the findings of a project intending to detect technology convergence based on the analysis of patents. The objective is very relevant, but we wonder if it fits well for the JCDL audience... What would be of most relevance to the JCDL conference would be the methods and techniques used to analyze the corpus (the details of how the so called ""co-classification"" had been performed, which is not described). That makes the poster only marginally relevant..","Overall evaluation: -1
Reviewer's confidence: 4",-1,,,,,3/26/2018,8:37,no
687,192,85,Haxxx Daxx,3,"(OVERALL EVALUATION) The authors presented a method and identified covering technology by using patents. They have performed an analysis of which industry of the converging technologies came from. If we consider patents as a knowledge repository, this presentation can be suitable for the conference.

However, there is some presentation and typos in the paper which needs to be fixed: 
* The paper is not in the appropriated format: the text needs to be justified. In addition, spaces in some words seem to be missing.
* increase dramatically => increased dramatically 
* In Section 2, in order to inform the number of downloaded patents and classes, they expressed using the phrase ""and the number is XXX""  which is not clear. 
* The text of Figure 3 is not readable","Overall evaluation: 0
Reviewer's confidence: 3",0,,,,,3/26/2018,16:40,no
688,193,85,Haxxx Daxx,1,"(OVERALL EVALUATION) The authors propose a method for diversifying citation contexts recommendation by using semantic similarity and re-ranking algorithms. They have shown that their method was better than the just a citation count metric. In addition, that using a Maximal Marginal Relevance and Explicit Semantic Analysis they could reach the best results. This is a very interesting topic for information retrieval and content organization in digital repositories. Because of that, I think this poster fits well in the conference.

Some suggestions: 
* Authors presented which tools they use to perform the semantic distance calculation. It was not clear to me how it was computed. This tool uses the topics of the citation context in order to measure the similarity? 
* In Table 1, it was not clear to me what was the citation context presented by the CiteSeerX, it was using the citation number? 
* In a future work, it would be interesting to use a bigger dataset.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,3/24/2018,17:06,no
689,193,349,Joxx Smxx,2,"(OVERALL EVALUATION) The paper presents an alternative approach to automated recommended reading lists and automated abstract generation. Their goal is to reduce redundancy in recommendations and improve the precision of the generated abstracts. To do this, they evaluate the content (semantic) similarity of citations, screen for redundancy, then re-rank and re-screen until the result set is sufficiently diverse. The problem of redundancy is familiar territory to digital librarians, and researchers have been working for some years now on methods to improve search results with maximum impact and minimum repetitive content. 

Producing 10 citation contexts from only 15 articles seems out of proportion. For that matter, having a case study of only 15 articles is surprising given the size of the CiteSeerX database. Also, I am a bit surprised that K??????ktun?? et al.'s work  (ACM/2015)in this area was not cited or discussed. Nonetheless, the iterative re-ranking approach is interesting and the the paper is clearly organized with a logical explanation of their methodology, experiment, and results. Nice job on the flow chart, too. Additional work is needed to strengthen their thesis (or to expose areas where their algorithm needs improvement), and I think they may get some of this through interacting with the JCDL conference attendees. Overall, I think the paper is acceptable as a poster submission with the caveat that the edits noted below are addressed prior to publication.

A few content edits:
	-- Note on the Abstract section: 
		= ""Our study helps to develop"" --> this seems a bit strong given the brief experiment. Perhaps better to say ""Our study aims to develop""
	
	-- Methodology section corrections: 
		= ""to outputs the semantic"" --> to output the semantic

	-- Experiments section corrections:
		= ""all the punctuations and stop words"" --> all the punctuation and stop words
		= The last sentence of that section should be reworked into 2 sentences to more clearly state what happened.
		
	-- Results section correction:
		= ""results showed that, our proposed"" --> no comma needed

And one final, major correction:	
	-- Compile the paper using the proper JCDL template.The proper template should always be used when submitting to a conference.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,23:48,no
690,193,251,Byxxx Marxxxx,3,"(OVERALL EVALUATION) The submission looks at a methodology for identifying usefully diverse citation contexts. This is an interesting and promising yet difficult task. Better indication of how this approach compares to other techniques would be important if this were to be expanded, for example, into a journal article. Also, this reader would like to know a bit more about the questions the subjects answered. Finally, I note that the notes in Table 1 use both start or strat  - perhaps that is a typo.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,3/26/2018,0:12,no
691,194,85,Haxxx Daxx,1,"(OVERALL EVALUATION) The authors propose a way to integrate many sources of information in order to enrich scholars profile pages. The work is interesting and useful for organizing the information as well as for information retrieval tasks.  They also have shown how feasible the integration is presenting the number of authors with unique identifiers.  

Some minor edits: 
sate => state
* for readability, in numbers, use: 111,107 instead of 111107

* power researchers: instead of putting a number as a baseline, It would be more interesting the authors provide a graphic of distribution (number of publications vs % of authors with a GND identifier).","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,3/24/2018,17:10,no
692,194,126,Nuxx Frxxx,2,"(OVERALL EVALUATION) This poster fits very well in the context of JCDL. The general problem addressed in the paper is indeed relevant and current. CRIS is gaining importance in digital libraries. 
This type of approach is of interest to many attendees of JCDL. 
The paper is also written with adequate quality for publication.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,4:53,no
693,194,251,Byxxx Marxxxx,3,"(OVERALL EVALUATION) This paper explores the feasibility of generating the data needed to cross-walk authors across multiple open access repositories. While the work adds little theoretically, the practical information and report on a particular approach will certainly be of interest to members of the JCDL community. One lens for conversation on the work would be comparing its result to Google Scholar author pages. Also, it would be interesting to have the authors better quantify or characterize the degree of unique identifier overlap even between selected repositories. Some information on that was provided, more would be welcome.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,3/26/2018,0:22,no
694,195,126,Nuxx Frxxx,1,"(OVERALL EVALUATION) This poster fits very well in the context of JCDL. type of research presented is appropriate for presentation as a poster.
The paper is written with sufficient quality for publication.","Overall evaluation: 2
Reviewer's confidence: 2",2,,,,,3/25/2018,4:54,no
695,195,251,Byxxx Marxxxx,2,"(OVERALL EVALUATION) This work documents the evolution of the term ?€?Smart Libraries?€? as it has appeared in the academic literature. The observation that we have moved from books, through services, on to knowledge would make for interesting conversation.  If this becomes a poster, I hope the authors will provide more information about the method by which the indications were distilled, better document the evidence that supports their conclusions, and share more data found during the analysis process. For example, they mention a particular paper discussing RFIDs but do not say how other papers indicated the theme they call out.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,3/26/2018,0:21,no
696,196,340,Sanxxxxx Sixxx,1,(OVERALL EVALUATION) In this work the authors propose a framework for automatically identifying roles of contributors of Bio medical research papers. Although this is an interesting work I am not sure whether such a system is even required. Also the text explaining the contributions are often structured and simple rule-based framework would be sufficient.,"Overall evaluation: 0
Reviewer's confidence: 3",0,,,,,3/25/2018,15:41,no
697,196,361,Kazxxxxx Sugxxxx,2,"(OVERALL EVALUATION) The authors propose a method for identifying authors' roles in biomedical publications 
by employing naive Bayes classifier. 

The authors need to conduct more experiments by applying another classifiers such as 
support vector machines, maximum entropy, decision tree, and so on. In addition, 
the authors need to compare their proposed approach with some state-of-the-arts 
so that the audience can learn how effective the authors' approach is.","Overall evaluation: -2
Reviewer's confidence: 4",-2,,,,,3/25/2018,21:59,no
698,196,136,Liaxxxxx Gx,3,"(OVERALL EVALUATION) 1.This paper addressed an interesting and important problem: identifying authors' roles in biomedical publications. 

2.The motivation of this work is well-described, clarifying the author's contribution in a scientific publication is a topic worth researching.

3.Based on a contribution section, this paper semi-automatically discovered common roles in the corpus and built an automated role extractor to extract the authors' roles.

4.Generally, this paper is well written and easy to follow, and the preliminary results are promising. 

5.Some minor issues:

1) Authors just tried Nai??ve Bayes classifier in the experiment, I understand a traditional classification model may be suitable in this study, as the dataset is relatively small. However, a better solution may be trying more classical algorithms: SVM, Max entropy, random forest, etc.
2) The dataset is relatively small, authors could perhaps validate their method on a larger dataset.","Overall evaluation: 1
Reviewer's confidence: 4",1,,,,,3/30/2018,16:55,no
699,197,222,Mxx Kexx,1,"(OVERALL EVALUATION) In this submission, the author evaluated the effectiveness of using rough co-citation in addition to origin co-citation compared to only original co-citation for detecting document relationships. Though the work is (admitted by the author) early, the novelty of using the additional method indicates promising results when the new method is explored further.

While the 2-page limit is restrictive, this work would benefit from illustrative examples, while I would anticipate being shown on the poster itself.

This work is highly relevant to JCDL and would be of interest to the conference attendees. Because of this, I would recommend it be accepted as a poster.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,20:14,no
700,197,361,Kazxxxxx Sugxxxx,2,"(OVERALL EVALUATION) The author proposes a co-clustering technique using both of the original co-citation and rough co-citation. 

The improvement in the author's approach compared with baseline (co-citation clustering) is quite slight. 
In addition, while the author compares some approaches by varying tuning parameters, he does not compare 
his approach with some state-of-the-arts. These indicates that the audience cannot learn how effective 
his approach is.","Overall evaluation: -2
Reviewer's confidence: 4",-2,,,,,3/25/2018,22:18,no
701,197,371,Praxxxx Terxxxxx,3,"(OVERALL EVALUATION) This paper explores the use of rough-citation, which utilizes the full text of the citing document, using similarity measure with hierarchical clustering to build weak co-citation relationships. A brief overview of the approach, similarity, clustering and evaluation with MeSH are provided. 

A comparison with previous work by the same author using citation context would be useful to understand the improvements  by using full text.","Overall evaluation: 1
Reviewer's confidence: 4",1,,,,,3/26/2018,23:35,no
702,197,349,Joxx Smxx,4,"(OVERALL EVALUATION) One function of digital libraries is identifying related works so that researchers can identify and navigate a topic space. Various methods are used to identify topic groups, including citations (sets of papers which cite the same other paper) and co-citations (sets of papers which are cited together by other papers). The author's goal in this submission is to improve one of the co-citation clustering technique using a full-text analysis on the co-citation works (""rough co-citation"" analysis). 

Researchers have been working on improving document clustering techniques for decades, and many variations exist that begin with citation analysis on some level. What is not clear to me is in what way this author's research is innovative. Others, including Chaomei Chen for example, have been expanding on citation clustering using text analysis. Previous researchers have shown success in applying multiple analyses to documents to better define topic clusters and closely-related works. But this author cites only a couple of these works (Boyack; Zhao) and ignores, for example, van Eck's work. Worse, 2 of the 4 citations are to the author's own work. 

While the paper's overall development is coherent and the results interesting, I am not convinced that it introduces something really new. If the author had more clearly contrasted her efforts against those already developed, it might have been more convincing.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) There may be genuine value in this author's work but it did not make a strong enough case differentiating it from prior work in this area. If others know this line of research better, they might have more helpful insight. But its overlap with previous work and 50% self-referenced bibliography lead me toward a rejection.","Overall evaluation: -1
Reviewer's confidence: 3",-1,,,,,3/27/2018,14:17,no
703,198,251,Byxxx Marxxxx,1,"(OVERALL EVALUATION) This submission describes and assesses a tool intended to extract focus/aspect/value triples to support semi-automated extraction of data to support comparison of notions in the content of documents. Of course space did not allow for important details or for comparison to existing tools. Still, the framing is interesting and the results promising. The work simultaneously proposes a potentially useful frame and provides some preliminary data on how the framework might be effectively implemented. Hopefully the authors will be able to show some examples of comparable extracted triples at the conference.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,3/26/2018,0:18,no
705,198,93,Mixxxx Doxxxx,2,"(OVERALL EVALUATION) The submission describes work on extracting comparison points from the Wikipedia articles on pairs of entities. The motivation and contextualisation of this task are not entirely convincing - there is a statement that ""The task is challenging as comparison points in a typical pair of articles tend to be sparse."" but beyond that, what is the relevance of the proposed method to digital libraries?","Overall evaluation: 2
Reviewer's confidence: 2",2,,,,,3/26/2018,18:45,no
706,198,22,Suxxx Alxxx,3,"(OVERALL EVALUATION) This paper discusses results on a preliminary test of an approach to extract comparison points of entities from the text articles describing them. This topic is appropriate to the JCDL conference. This proposal is well-written and does a good job of establishing the reason this work is meaningful as well as concisely outlining the methods.  THe results are interesting and would provoke conversation.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) There are some grammatical issues in several places but the proposal is still coherent.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,3/26/2018,18:43,no
707,199,165,Fexxx Haxxxx,1,"(OVERALL EVALUATION) The paper presents an interesting experiment carried out on accepted journal articles with the goal to find features that may be used by future research projects to (automatically) estimate whether an article manuscript is likely to be accepted or rejected by a particular journal. 

Due to the (probably, I'm not an expert in this area) valuable findings, such as influential features as depicted in Table 1, the paper opens further research opportunities. The authors suggest to use the results of the presented research to devise a machine learning algorithm that estimates whether a paper has a chance of being accepted at a specific journal.","Overall evaluation: 2
Reviewer's confidence: 2",2,,,,,3/25/2018,10:53,no
708,199,349,Joxx Smxx,2,"(OVERALL EVALUATION) The authors present an analysis of features common to articles that were rejected by editors of scholarly journals. They cite 5 factors that lead to rejection: scope, quality, novelty, linguistics and formatting. Their goal is to provide an AI approach to help with the pre-culling of manuscripts to ease the editorial burden. Mining information provided by Elsevier, the authors look at how these factors affected the final decision to accept, desk-reject, or review-reject each submissions. It's an interesting study that is well worth presenting at JCDL, despite numerous defects in the linguistics and formatting aspects of the paper (some of which are listed below). 

Overall, I think the paper is acceptable as a poster submission with the caveat that the edits noted below are addressed prior to publication.

A few content edits:
	-- Abstract section corrections: 
		= ""being made to journals now-a-days"" --> being made to journals today
		= ""potential towards the development"" --> potential for development
		= ""authors in taking appropriate"" --> authors by making appropriate
		= ""hence speed up the overall"" --> thus accelerating the overall
	
	-- Introduction section corrections: 
		= ""there exists at least"" --> there exist at least (plural transitive)
		= ""generic"" --> perhaps the word 'general' is better here? Even then, you're really providing 5 specific factors that play a role, so neither word seems quite appropriate. 
		= ""from the editors' desk :"" --> from the editor's desk: (singular editor; remove space between desk and colon)
		= Plagiarism --> Is this the right word to use here? Is Redundancy, Unoriginality, or Innovation perhaps more apt?

	-- Data section corrections:
		= ""set of 5,500 Desk Rejected papers "" --> set of 5,500 desk-rejected papers (be consistent in using this term)
		= ""41737169"" --> 41,737,169 (use commas for readability)
		= ""this we use"" --> these we use
		= ""to extract meta information for"" --> to extract metadata for
		
	-- Other section corrections:
		= There are a lot of small corrections that would greatly improve the readability of this paper. But at that point, it would be nearly completely re-written. Despite this, the paper is fairly readable with only minor stumbling over some awkward phrases and sentences.

However, there is one additional, major correction:	
	-- Compile the paper using the proper JCDL template.The proper template should always be used when submitting to a conference. I am surprised and disappointed that this has to be said here, given that this article discusses reasons that papers are *rejected* for publication and that template mismatch can be a factor. One would think the authors would use their own data points as a checklist prior to submission.","Overall evaluation: 2
Reviewer's confidence: 4",2,,,,,3/26/2018,0:34,no
709,199,22,Suxxx Alxxx,3,"(OVERALL EVALUATION) This is an interesting project that is examining an important part of scholarly publishing, and could be relevant to the conference. The data set for review is well-formed and the approach is well-explained.  The reported findings are interesting descriptors of what is happening now and provide some insights about current reviewer behavior.  I would have concerns about using this for AI since an automated system will be reifying some habits that may be constricting scholarship. For example scope: defining whether an article fits the scope of a journal by constricting the bibliography to domain-specific references can lead to a narrowing of scholarship in the area and a loss of discovery.  Perhaps these findings can be used to explore how AI can be used to support new discovery by helping extend current reviewing behaviors.","Overall evaluation: 1
Reviewer's confidence: 4",1,,,,,3/26/2018,12:12,no
710,200,165,Fexxx Haxxxx,1,"(OVERALL EVALUATION) This paper presents a co-authorship recommender system that uses networks of co-authorships on already published articles to predict / suggest new co-authors for a given publication. While I find the idea quite interesting, I am not sure whether the novelty of the proposed approach is enough, since the approach basically just combines state-of-the-art techniques. Moreover, the authors could have used less citations (currently, 25% of the content of the paper is the reference list) and go into more detail when describing their approach. In its current version (with only a couple of paragraphs stating the method/contribution of the paper) it is difficult to tell what the technical/conceptual contribution of the paper is and whether it would be sufficient. Hence, I am suggesting a 'weak reject', and suggest to go into more detail (and use less references when only 2 pages are to be submitted) for future versions of this interesting research project.","Overall evaluation: -1
Reviewer's confidence: 3",-1,,,,,3/25/2018,11:15,no
711,200,22,Suxxx Alxxx,2,"(OVERALL EVALUATION) Learning what co-authorship networks can reveal is an interesting topic.  The model could be better described; for example there should be more detail about the parameters established for extracting information from Scopus. While you note that one or more co-authors belong to HSE there is no note as to how many years you were reviewing, how this affiliation was established (listed in article or other means) or ultimate size of the dataset. There should be more discussion about the findings.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This abstract needs some copy editing to fix several grammatical issues.","Overall evaluation: 0
Reviewer's confidence: 4",0,,,,,3/26/2018,12:22,no
712,201,22,Suxxx Alxxx,1,"(OVERALL EVALUATION) This paper is a descriptive analysis of authority records.  The paper would be much stronger if it highlighted why this work is important.  Keeping pace with standards and maintaining functionality is essential but it would be useful to add a sentence about why authority records matter and how they affect scholarly communication. This would provide context for the research questions.  

The methods are well-described and appropriate. It would be good to add context in the findings and discussion section. While it is interesting to learn the descriptive results-- what does it mean?

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) This submission is not properly formatted. There are some areas where editing would make it easier to read and understand.","Overall evaluation: 0
Reviewer's confidence: 3",0,,,,,3/26/2018,12:30,no
713,201,371,Praxxxx Terxxxxx,2,"(OVERALL EVALUATION) This work presents results from analyzing MARC authority records over a period of one year. The observations include information on several attributes that have become more represented across this time period.  These attributes could indicate the importance being attached to names and persons and meetings.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) The format of the paper does not appear to be in the ACM template. It would however be a paper that can start discussions in the conference.","Overall evaluation: 0
Reviewer's confidence: 2",0,,,,,3/26/2018,19:40,no
714,202,349,Joxx Smxx,1,"(OVERALL EVALUATION) The authors have an interesting idea that could have been a good addition to JCDL. They are working on a medical ontology that could provide targeted, patient-specific medical treatment by organizing research data using semantic relationships among 6 branches of medical research. Unfortunately, they have provided scant information about the project. At a mere 303 words, the discussion barely constitutes an abstract. What the authors should have done is:

A) Add an Introduction -- Provide a brief introduction, expanding on the shortcomings of existing data sets and why something like this ontology is needed.

B) Add a Methodology section -- Present an example of the proposed ontology, discuss the data sources they use, justify these sources versus others that might be available.

C) Provide a Next Steps or Conclusion or Results section -- Discuss where they currently are at in their research and what they intend to do next. 

D) Add a Bibliography -- Provide references to relevant works. For a Poster submission, this will often be only 4-6 articles or books. These works should be cited/referred to in the paper.

E) Compile and submit the paper **using the correct submission template** in this case, the JCDL template.

Since this paper lacks even elemental discussion of their research methodology and status, I have to recommend rejection. The authors should keep these recommendations in mind if they submit to another conference or to a future JCDL.

(CONFIDENTIAL REMARKS FOR THE PROGRAM COMMITTEE) Just wondering why so many of these posters fail to use the correct JCDL template. Occasionally a submission uses a format similar to the required template, but too often the one selected has such clearly different parameters that it's hard to tell whether the content will fit within the JCDL length limits.","Overall evaluation: -2
Reviewer's confidence: 4",-2,,,,,3/25/2018,2:52,no
716,202,43,Joxx Borxxxx,2,"(OVERALL EVALUATION) Precision medicine is a very important trend in science and technology. This poster describes an initiative to develop an ontology for that purpose, considering patient phenotype, diseases, drugs, cellular and molecular mechanisms, and sequences.
The intention for this objective might be seen as quite naive, considering the complexity of the domain, and also that the proposal does not describes the approach to build the ontology (ontology engineering techniques/method?). However, it is important to expose the JCDL community to challenges of this kind, as they might bring new relevant opportunities for the domain of digital libraries...

This submission is of only 1 page, with no details and bibliography. We believe that is the poster is accepted, the authors might be able to produce a proper 2 pages paper with these details","Overall evaluation: -2
Reviewer's confidence: 5",-2,,,,,3/30/2018,18:55,no
717,202,22,Suxxx Alxxx,3,(OVERALL EVALUATION) The paper that was submitted for review consisted only of an extended abstract which did not provide sufficient information for a full review.  The topic of building a precision medicine ontology is interesting and could fit the conference. Unfortunately not enough can be gleaned from the available submission to accept this paper.,"Overall evaluation: -2
Reviewer's confidence: 4",-2,,,,,3/26/2018,12:33,no
718,203,126,Nuxx Frxxx,1,"(OVERALL EVALUATION) This poster fits very well in the context of JCDL. The general problem addressed in the paper is indeed relevant and current, because of all efforts being done in the institutional repositories of research institutions for evaluation of research outputs, and to support CRIS systems in general.
This results presented in the paper are of interest to many attendees of JCDL. 
The paper is also written with good quality for publication.","Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/25/2018,4:56,no
719,203,340,Sanxxxxx Sixxx,2,"(OVERALL EVALUATION) This work contributes to the problem of Author name disambiguation. As the author points out there are tons of work in the domain, it is essential that the author compares his result with the state-of-art which is missing. The paper is nicely written and easy to follow.","Overall evaluation: 0
Reviewer's confidence: 4",0,,,,,3/25/2018,15:58,no
720,203,22,Suxxx Alxxx,3,(OVERALL EVALUATION) A well-designed project building on existing work that explores an area that has potential for further development. This topic is relevant to the conference.  The results suggest that semantic features can contribute but it would be interesting to know the researcher's thoughts on why there isn't a stronger improvement.,"Overall evaluation: 2
Reviewer's confidence: 3",2,,,,,3/26/2018,12:41,no
